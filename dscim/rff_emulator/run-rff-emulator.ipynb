{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55517aab-03ff-45dc-89af-ff9477c12484",
   "metadata": {},
   "source": [
    "# RFF emulator\n",
    "\n",
    "Method: https://www.overleaf.com/project/618bf194c5e93dd6ff7e1bf4\n",
    "\n",
    "Original script: https://bitbucket.org/ClimateImpactLab/socioeconomics/src/master/emulator/rff_slice.py\n",
    "\n",
    "This was adapted from the bitbucket code on 12/9/2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bce52da0-5e8e-404b-8be4-313c86b7a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## UPDATE ME!!!\n",
    "\n",
    "HEADER = \"\"\"oneline: Emulation parameters for RFF sample\n",
    "\n",
    "version: EMU-RFF.2021-12-09\n",
    "dependencies: GDPPC-RFF.2021-11-05\n",
    "variables:\n",
    "    iso: Country ISO (from RFF file) [str]\n",
    "\n",
    "description: \"Generated by socioeconomics/emulator/rff_slice.py\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef79276-0a0c-4ad4-8bf9-064eafbb9c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rhg_compute_tools.utils as rhgu\n",
    "import fsspec\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf4d04ef-ad32-4c8d-bd39-fd80ed7c7c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engine: SciPy\n",
      "gs://impactlab-data/gcp/integration/integration_raw_data/rff-weights/fivebean/20220225.1054\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "try:\n",
    "    import gurobipy as gp\n",
    "    from gurobipy import GRB\n",
    "    engine = \"Gurobi\"\n",
    "except:\n",
    "    from scipy.optimize import linprog\n",
    "    engine = \"SciPy\"\n",
    "    \n",
    "print(\"Engine: %s\" % engine)\n",
    "\n",
    "recipe = 'fivebean'\n",
    "\n",
    "fs_kwargs = {'token': '/opt/gcsfuse_tokens/impactlab-data.json'}\n",
    "\n",
    "fs = fsspec.filesystem('gs', **fs_kwargs)\n",
    "\n",
    "outdir = os.path.join(\n",
    "    'gs://impactlab-data/gcp/integration/integration_raw_data/rff-weights/',\n",
    "    recipe,\n",
    "    pd.Timestamp.now(tz='US/Pacific').strftime('%Y%m%d.%H%M'),\n",
    ")\n",
    "\n",
    "rffpath = \"gs://impactlab-data/gcp/integration/integration_raw_data/rff/feather\"\n",
    "\n",
    "\n",
    "# os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "assert not fs.isdir(outdir)\n",
    "\n",
    "print(outdir)\n",
    "\n",
    "\n",
    "## https://www.overleaf.com/project/618bf194c5e93dd6ff7e1bf4\n",
    "## Parameters are d_it (d = |y - z|), alpha_is\n",
    "\n",
    "# @rhgu.block_globals(whitelist=['engine','gp'])\n",
    "def cook(recipe, df, wantdf):\n",
    "\n",
    "    output = []\n",
    "\n",
    "    assert recipe in ['delimeat', 'coleslaw', 'fivebean']\n",
    "\n",
    "    if recipe == 'delimeat':\n",
    "        header = ['iso', 'param', 'name', 'value']\n",
    "        groups = pd.unique(df.iso)\n",
    "    elif recipe == 'coleslaw':\n",
    "        header = ['isoyear', 'param', 'name', 'value']\n",
    "        groups = pd.unique(df.isoyear)\n",
    "    elif recipe == 'fivebean':\n",
    "        header = ['year', 'param', 'name', 'value']\n",
    "        groups = pd.unique(df.year)\n",
    "    \n",
    "    for group in groups:\n",
    "\n",
    "        if recipe == 'delimeat':\n",
    "            iso = group\n",
    "            idf = df[df.iso == iso]\n",
    "            wantidf = wantdf[wantdf.iso == iso]\n",
    "        elif recipe == 'coleslaw':\n",
    "            iso = group[:3]\n",
    "            idf = df[df.isoyear == group]\n",
    "            wantidf = wantdf[wantdf.isoyear == group]\n",
    "        elif recipe == 'fivebean':\n",
    "            year = group\n",
    "            idf = df[df.year == group]\n",
    "            wantidf = wantdf[wantdf.year == group]\n",
    "\n",
    "        if wantidf.shape[0] == 0:\n",
    "            continue\n",
    "            \n",
    "        isoyears = pd.unique(idf.isoyear)\n",
    "\n",
    "        ## Create parameters list\n",
    "\n",
    "        if recipe in ['delimeat', 'coleslaw']:\n",
    "            alphaparams = pd.unique(idf.isoscen)\n",
    "        elif recipe == 'fivebean':\n",
    "            alphaparams = pd.unique(idf.yearscen)\n",
    "\n",
    "        # Drop the first entry for each subgroup\n",
    "        alphaparams_butfirst = np.delete(alphaparams, 0)\n",
    "        params = np.concatenate((isoyears, alphaparams_butfirst))\n",
    "\n",
    "        paramindex_alpha0 = len(isoyears)\n",
    "\n",
    "        # Construct objective function\n",
    "\n",
    "        if 'weight' in wantdf.columns:\n",
    "            weights = [wantidf.weight[(wantidf.isoyear == isoyear)].values[0] for isoyear in isoyears]\n",
    "        else:\n",
    "            weights = np.ones(len(isoyears))\n",
    "\n",
    "        if recipe in ['delimeat', 'fivebean']:\n",
    "            objfunc = np.concatenate((weights, np.zeros(len(alphaparams_butfirst))))\n",
    "        elif recipe == 'coleslaw':\n",
    "            # Prefer lower SSPs\n",
    "            objfunc = np.concatenate((weights, 1e-3 * np.arange(1, len(alphaparams_butfirst) + 1)))\n",
    "\n",
    "        # Contruct constraints, all in the form of A x < b\n",
    "        AA_rows = []\n",
    "        AA_cols = []\n",
    "        AA_data = []\n",
    "        bb = []\n",
    "\n",
    "        def add_AA_cell(row, col, value):\n",
    "            AA_rows.append(row)\n",
    "            AA_cols.append(col)\n",
    "            AA_data.append(value)\n",
    "\n",
    "        ## Rows defining absolute values\n",
    "\n",
    "        # d_it > y_it - (sum_s>1 alpha_is y_sit + (1 - sum_s>1 alpha_is) y_1it)\n",
    "        # -d_it - (sum_s>1 alpha_is y_sit - (sum_s>1 alpha_is) y_1it) < -y_it + y_1it\n",
    "\n",
    "        # d_it > -(y_it - (sum_s>1 alpha_is y_sit + (1 - sum_s>1 alpha_is) y_1it))\n",
    "        # -d_it + (sum_s>1 alpha_is y_sit - (sum_s>1 alpha_is) y_1it) < y_it - y_1it\n",
    "\n",
    "        for ii, isoyear in enumerate(isoyears):\n",
    "            subdf = idf[idf.isoyear == isoyear]\n",
    "            if subdf.shape[0] == 0 or not np.any(wantidf.isoyear == isoyear):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                if recipe in ['delimeat', 'coleslaw']:\n",
    "                    y1it = subdf.loginc[subdf.isoscen == alphaparams[0]].values[0]\n",
    "                elif recipe == 'fivebean':\n",
    "                    y1it = subdf.loginc[subdf.yearscen == alphaparams[0]].values[0]\n",
    "            except Exception as ex:\n",
    "                # print(ii,isoyear,ex)\n",
    "                # print(\"Exception! Keep going..\") # KM added\n",
    "                continue\n",
    "\n",
    "            add_AA_cell(len(bb), ii, -1)\n",
    "            add_AA_cell(len(bb) + 1, ii, -1)\n",
    "\n",
    "            for jj, alphaparam in enumerate(alphaparams_butfirst):\n",
    "                if recipe in ['delimeat', 'coleslaw']:\n",
    "                    ysit = subdf.loginc[subdf.isoscen == alphaparam].values[0]\n",
    "                elif recipe == 'fivebean':\n",
    "                    ysit = subdf.loginc[subdf.yearscen == alphaparam].values[0]\n",
    "        \n",
    "                add_AA_cell(len(bb), paramindex_alpha0 + jj, -ysit + y1it)\n",
    "                add_AA_cell(len(bb) + 1, paramindex_alpha0 + jj, ysit - y1it)\n",
    "\n",
    "            bb.append(-wantidf.loginc[(wantidf.isoyear == isoyear)].values[0] + y1it)\n",
    "            bb.append(wantidf.loginc[(wantidf.isoyear == isoyear)].values[0] - y1it)\n",
    "\n",
    "        constrindex_alphag10 = len(bb)\n",
    "    \n",
    "        # sum alpha_is < 1\n",
    "        for jj in range(paramindex_alpha0, len(params)):\n",
    "            add_AA_cell(constrindex_alphag10, jj, 1)\n",
    "        bb.append(1)\n",
    "\n",
    "        constrindex_end = constrindex_alphag10 + 1\n",
    "\n",
    "        AA = coo_matrix((AA_data, (AA_rows, AA_cols)), shape=(constrindex_end, len(params)))\n",
    "\n",
    "        ## Also constrained so that d_it > 0 and alpha_is > 0\n",
    "\n",
    "        if engine == 'SciPy':\n",
    "            res = linprog(objfunc, A_ub=AA, b_ub=bb)\n",
    "            errors = np.around(res.x[:paramindex_alpha0], 6)\n",
    "            alphas = np.around(res.x[paramindex_alpha0:], 6)\n",
    "        elif engine == 'Gurobi':\n",
    "            mod = gp.Model(\"gourmet\")\n",
    "            xx = mod.addMVar(shape=len(objfunc), vtype=GRB.CONTINUOUS, name=\"xx\")\n",
    "            mod.setObjective(objfunc @ xx, GRB.MINIMIZE)\n",
    "            bb = np.array(bb)\n",
    "            mod.addConstr(AA @ xx <= bb)\n",
    "            mod.optimize()\n",
    "            errors = np.around(xx.X[:paramindex_alpha0], 6)\n",
    "            alphas = np.around(xx.X[paramindex_alpha0:], 6)\n",
    "\n",
    "\n",
    "        for ii, error in enumerate(errors):\n",
    "            output.append([group, 'error', isoyears[ii], error])\n",
    "\n",
    "        output.append([group, 'alpha', alphaparams[0], max(0, 1 - sum(alphas))])\n",
    "        for ii, alpha in enumerate(alphas):\n",
    "            output.append([group, 'alpha', alphaparams_butfirst[ii], alpha])\n",
    "\n",
    "    out_df = pd.DataFrame(output, columns=header)\n",
    "\n",
    "    return out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f68ae296-93ad-4af1-b00c-6afa9ff53291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @rhgu.block_globals(whitelist=['HEADER','rffpath'])\n",
    "def process_rff_sample(i, df, outdir, recipe, **storage_options):\n",
    "    fs = fsspec.filesystem('gs', **storage_options)\n",
    "    read_fp = (\n",
    "        \"gs://impactlab-data/gcp/integration/integration_raw_data/rff/gdppc/gdppc-nohier-%d.csv\"\n",
    "        % i\n",
    "    )\n",
    "\n",
    "    with fs.open(read_fp) as f:\n",
    "        wantdf = pd.read_csv(f, skiprows=11)\n",
    "\n",
    "    wantdf['loginc'] = np.log(wantdf.value)\n",
    "    wantdf['isoyear'] = wantdf.apply(lambda row: \"%s:%d\" % (row.iso, row.year), axis=1)\n",
    "\n",
    "    read_feather = (\n",
    "        os.path.join(rffpath, \"run_%d.feather\" % i)\n",
    "    )\n",
    "    with fs.open(read_feather) as f:\n",
    "        weights = pd.read_feather(f)\n",
    "\n",
    "    weights.rename(columns={'Year': 'year', 'Country': 'iso'}, inplace=True)\n",
    "    wantdf = pd.merge(wantdf, weights, on=['year', 'iso'], how='left')\n",
    "    wantdf.rename(columns={'GDP': 'weight'}, inplace=True)\n",
    "    print(wantdf.iso[np.isnan(wantdf.weight)])\n",
    "    wantdf.weight[np.isnan(wantdf.weight)] = np.exp(np.nanmean(np.log(wantdf.weight))) # not dominated by large values\n",
    "\n",
    "    out_df = cook(recipe, df, wantdf)\n",
    "\n",
    "    write_file = os.path.join(outdir, \"emulate-%s-%d.csv\") % (recipe, i)\n",
    "\n",
    "    protocol = write_file.split('://')[0] if '://' in write_file else ''\n",
    "    write_options = storage_options if protocol != '' else {}\n",
    "    fs = fsspec.filesystem(protocol, **write_options)\n",
    "\n",
    "    with fs.open(write_file, 'w') as outf:\n",
    "        outf.write(HEADER.strip() + '\\n')\n",
    "        out_df.to_csv(outf, index=False)\n",
    "        print(\"writing\",write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "407ccaed-bcb5-42cc-a6e1-840d00599819",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssp_fp = \"gs://impactlab-data/gcp/integration/integration_raw_data/gdppc-merged-nohier.csv\"\n",
    "with fs.open(ssp_fp) as f:\n",
    "    df = pd.read_csv(f, skiprows=11)\n",
    "\n",
    "df = df[(df.scenario != 'SSP1') & (df.scenario != 'SSP5')]\n",
    "df = df[df.year >= 2010] # low only starts in 2010\n",
    "df['loginc'] = np.log(df.value)\n",
    "df['isoyear'] = df.apply(lambda row: \"%s:%d\" % (row.iso, row.year), axis=1)\n",
    "df['isoscen'] = df.apply(lambda row: \"%s:%s/%s\" % (row.iso, row.model, row.scenario), axis=1)\n",
    "df['yearscen'] = df.apply(lambda row: \"%d:%s/%s\" % (row.year, row.model, row.scenario), axis=1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1603ad-c5b8-4b2d-b235-b35e7b0b0b5a",
   "metadata": {},
   "source": [
    "### Test a couple RFF-SPs\n",
    "ran rff_sp=1 and rff_sp=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "068877d9-8c07-4211-b76e-930f209e03f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://impactlab-data/gcp/integration/integration_raw_data/rff-weights/fivebean/20220224.1242\n",
      "0        ABW\n",
      "1        ABW\n",
      "2        ABW\n",
      "60       AFG\n",
      "61       AFG\n",
      "        ... \n",
      "11097    ZMB\n",
      "11155    ZWE\n",
      "11156    ZWE\n",
      "11157    ZWE\n",
      "11158    ZWE\n",
      "Name: iso, Length: 728, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_851/2309972590.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wantdf.weight[np.isnan(wantdf.weight)] = np.exp(np.nanmean(np.log(wantdf.weight))) # not dominated by large values\n",
      "/tmp/ipykernel_851/2209766583.py:142: OptimizeWarning: Sparse constraint matrix detected; setting 'sparse':True.\n",
      "  res = linprog(objfunc, A_ub=AA, b_ub=bb)\n",
      "/tmp/ipykernel_851/2209766583.py:142: OptimizeWarning: Solving system with option 'sym_pos':True failed. It is normal for this to happen occasionally, especially as the solution is approached. However, if you see this frequently, consider setting option 'sym_pos' to False.\n",
      "  res = linprog(objfunc, A_ub=AA, b_ub=bb)\n",
      "/tmp/ipykernel_851/2209766583.py:142: OptimizeWarning: Solving system with option 'sym_pos':False failed. This may happen occasionally, especially as the solution is approached. However, if you see this frequently, your problem may be numerically challenging. If you cannot improve the formulation, consider setting 'lstsq' to True. Consider also setting `presolve` to True, if it is not already.\n",
      "  res = linprog(objfunc, A_ub=AA, b_ub=bb)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing gs://impactlab-data/gcp/integration/integration_raw_data/rff-weights/fivebean/20220224.1242/emulate-fivebean-1.csv\n"
     ]
    }
   ],
   "source": [
    "print(outdir)\n",
    "process_rff_sample(\n",
    "    1,\n",
    "    df=df,\n",
    "    outdir=outdir,\n",
    "    recipe=recipe,\n",
    "    token=\"/opt/gcsfuse_tokens/impactlab-data.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f90c6a0-b6c6-4f6d-85a2-b79a6e0df901",
   "metadata": {},
   "source": [
    "### TODO: compare output to original fivebean to make sure this produces the same outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589a9279-0a2e-4faf-9529-0815a63eec94",
   "metadata": {},
   "source": [
    "### Map all RFF-SPs: Below has not yet been tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c08d898-6cd1-4a40-9c2f-af752fde12fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rhg_compute_tools.kubernetes as rhgk\n",
    "import dask.distributed as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328c97e3-8f98-4f0d-aa58-3da01140e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client, cluster = rhgk.get_micro_cluster()\n",
    "cluster.scale(20)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb176a5-70d4-4fec-848c-6920eadd03ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = client.map(\n",
    "    process_rff_sample,\n",
    "    list(range(1, 10001)),\n",
    "    df=df,\n",
    "    outdir=outdir,\n",
    "    recipe=recipe,\n",
    "    token='/opt/gcsfuse_tokens/impactlab-data.json',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54d4882-930c-4436-bf93-13c9fff91e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.progress(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959b2834-bc0d-467f-bf23-1e2fc8222742",
   "metadata": {},
   "source": [
    "# The following lines clean up / shut down your cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f54a32-7405-40d7-97b7-53dad3d8dc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will block until futures finish, so you can just run this notebook straight through.\n",
    "# if you're trying to kill the job, run the next cell without running this one.\n",
    "dd.wait(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe1d4c6-c89c-4268-aa61-ac4040a72ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()\n",
    "cluster.scale(0)\n",
    "client.close()\n",
    "cluster.close()\n",
    "del client\n",
    "del cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bf2f04-267b-4806-a2e1-eb3b40df68d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
