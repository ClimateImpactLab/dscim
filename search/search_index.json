{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#dscim-the-data-driven-spatial-climate-impact-model","title":"DSCIM: The Data-driven Spatial Climate Impact Model","text":"<p>This Python library enables the calculation of sector-specific partial social cost of greenhouse gases (SC-GHG) and SCGHGs that are combined across sectors using a variety of valuation methods and assumptions. The main purpose of this library is to parse the monetized spatial damages from different sectors and integrate them using different options (\"menu options\") that encompass different decisions, such as discount levels, discount strategies, and different considerations related to economic and climate uncertainty. </p>"},{"location":"#installation","title":"Installation","text":"<p>Install with <code>pip</code> using: <pre><code>pip install dscim\n</code></pre></p> <p>Install the unreleased bleeding-edge version of the package with: <pre><code>pip install git+https://github.com/climateimpactlab/dscim\n</code></pre></p>"},{"location":"#dependencies","title":"Dependencies","text":"<p><code>dscim</code> requires Python &gt; 3.8. Additional compiled packages are required so we recommend installing <code>dscim</code> into a <code>conda</code> environment along with its dependencies.</p> <ul> <li>numpy</li> <li>pandas</li> <li>xarray</li> <li>matplotlib</li> <li>dask</li> <li>distributed</li> <li>requests</li> <li>statsmodels</li> <li>zarr</li> <li>netcdf4</li> <li>h5netcdf</li> <li>impactlab-tools</li> <li>p_tqdm</li> </ul>"},{"location":"#support","title":"Support","text":"<p>Source code is available online at https://github.com/climateimpactlab/dscim. Please file bugs in the bug tracker.</p> <p>This software is Open Source and available under the Apache License, Version 2.0.</p>"},{"location":"#structure-and-logic","title":"Structure and logic","text":"<p>The library is split into several components that implement the hierarchy defined by the menu options. These are the main elements of the library and serve as the main classes to call different menu options.</p> <pre><code>graph TD\nSubGraph1Flow(Storage and I/O)\n  subgraph \"Storage utilities\"\n  SubGraph1Flow --&gt; A[Stacked_damages]\n  SubGraph1Flow -- Climate Data --&gt; Climate\n  SubGraph1Flow -- Economic Data --&gt; EconData\n  end\n\n  subgraph \"Recipe Book\"\n  A[StackedDamages] --&gt; B[MainMenu]\n  B[MainMenu] --&gt; C[AddingUpRecipe];\n  B[MainMenu] --&gt; D[RiskAversionRecipe];\n  B[MainMenu] --&gt; E[EquityRecipe]\n  end</code></pre> <p><code>StackedDamages</code> takes care of parsing all monetized damage data from several sectors and read the data using a <code>dask.distributed.Client</code>. At the same time, this class takes care of ingesting FaIR GMST and GMSL data needed to draw damage functions and calculate FaIR marginal damages to an additional emission of carbon. The data can be read using the following components: </p> Class Function <code>Climate</code> Wrapper class to read all things climate, including GMST and GMSL. You  can pass a <code>fair_path</code> with a NetCDF with FaIR control and pulse simulations and median FaIR runs. You can use <code>gmst_path</code> to input a  CSV file with model and year anomaly data, for fitting the damage functions. <code>EconVars</code> Class to ingest sector path related data, this includes GDP and population data. Some intermediate variables are also included in this class, check the documentation for more details <code>StackedDamages</code> Damages wrapper class. This class contains all the elements above and  additionally reads all the computed monetized damages. A single path is needed to read all damages, and sectors must be separated by folders.  If necessary, the class will save data in <code>.zarr</code> format to make chunking operations more efficient. Check documentation of the class for more details. <p>and these elements can be used for the menu options:   - <code>AddingUpRecipe</code>: Adding up all damages and collapse them to calculate a general SCC without valuing uncertainty.  - <code>RiskAversionRecipe</code>: Add risk aversion certainty equivalent to consumption calculations - Value uncertainty over econometric and climate draws.  - <code>EquityRecipe</code>: Add risk aversion and equity to the consumption calculations. Equity includes taking a certainty equivalent over spatial impact regions.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"changelog/#070-2025-08-15","title":"0.7.0 - 2025-08-15","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Documentation pages added using mkdocs (PR #254, @JMGilbert)</li> <li>Added discounting option <code>constant_gwr</code>, which applies discounting across SSPs (PR #405, @JMGilbert).</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>The function signature for <code>calculate_labor_batch_damages()</code> in <code>src/dscim/preprocessing/input_damages.py</code> was updated to include additional args with default values that allow the labor SCC application to run without modifying <code>dscim</code> code in the future. This is backwards compatible. (PR #415, @JMGilbert).</li> <li>Python version for running automated tests in CI upgraded from Python 3.10 to 3.12 (PR #270, @brews).</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Fixed how quantile regression SCCs (<code>quantreg</code>) are calculated by allowing for the full cloud of damage points in the damage function fit stage (previously the <code>batch</code> dimension was incorrectly reduced before damage function fit even if <code>quantreg=True</code>) (PR #405, @JMGilbert). </li> <li>Minor code cleanup. Switch old %-string formatting to use f-strings (PR #351, @brews).</li> <li>Pin <code>numcodecs</code> package to 0.15.1 to fix automated tests in CI. This works with <code>zarr &lt; 3</code>. (PR #406, @JMGilbert).</li> <li>Pin <code>statsmodels</code> to 0.14.5 to fix automated tests in CI. (PR #429, @C1587S). </li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li>Removed <code>preprocessing/climate</code> and <code>preprocessing/misc</code> subpackages. (PR #249, @JMGilbert)<ul> <li>The modules in these subpackages referenced hard coded filepaths and were used for old versions of climate files and inputs that are now properly formatted by default</li> </ul> </li> <li>Removed <code>utils/generate_yaml</code> and <code>utils/plotting_utils</code> modules. (PR #249, @JMGilbert)<ul> <li><code>generate_yaml</code> seems to have been designed for an old version of <code>dscim-epa</code> and that functionality has now been transferred to the <code>scripts/directory_setup.py</code> script in the <code>dscim-epa</code> and <code>dscim-facts-epa</code> repositories</li> <li><code>plotting utils</code> was only in use for a single diagnostic and was transferred to the script that generated that diagnostic</li> </ul> </li> <li>Removed <code>midprocessing</code> <code>update_damage_function_library</code> and <code>utils</code> <code>constant_equivalent_discount_rate</code>, <code>calculate_constant_equivalent_discount_rate</code>, and <code>get_model_weights</code> functions. (PR #249, @JMGilbert)<ul> <li><code>update_damage_function_library</code> was previously used to move files prior to the functionality that directly saved files into the appropriate location</li> <li><code>constant_equivalent_discount_rate</code> and <code>calculate_constant_equivalent_discount_rate</code> are used for integration paper tables, and have been transferred to the appropriate scripts</li> <li><code>get_model_weights</code> is used for a few diagnostics and has been transferred to the appropriate scripts</li> </ul> </li> </ul>"},{"location":"changelog/#060-2024-04-24","title":"0.6.0 - 2024-04-24","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Add an option for producing SCC ranges that account for only statistical uncertainty. (PR #143, @davidrzhdu)</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Fix concatenate_energy_damages netcdf saving functionality which was not clearing data encoding causing some coordinates to be truncated. (PR #229, @JMGilbert)</li> <li>Fix tests broken by sorting update in pandas v2.2.1 (PR #216, @JMGilbert)</li> </ul>"},{"location":"changelog/#050-2023-11-17","title":"0.5.0 - 2023-11-17","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Add naive list of package dependencies to pyproject.toml.(PR #123, @brews)</li> <li>CI, coverage, DOI badges on README. (PR #134, @brews)</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Dropped optional/unused dependencies <code>click</code>, <code>dask-jobqueue</code>, <code>geopandas</code>, <code>gurobipy</code>, <code>ipywidgets</code>, <code>seaborn</code>. (PR #99, @brews)</li> <li>Switch build system from <code>setuptools</code> to <code>hatchling</code>. (PR #128, @brews)</li> <li>Clean up unit test for <code>dscim.utils.utils.c_equivalence</code>. (PR #135, @brews)</li> <li>Reformat gmst/gmsl pulse files by removing unnecessary dimensions and indices. (PR #169, @JMGilbert)</li> </ul>"},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Fix DeprecationWarning on import. (PR #128, @brews)</li> <li>Fix write-to-copy warning in <code>process_rff_sample()</code>. (PR #116, @brews)</li> <li>Fix exception from indexing with dask-backed boolean array and input climate Dataset attrs collision with xarray &gt;= v2023.3.0. (PR #129, @brews)</li> <li>Fix bad release header links in CHANGELOG.md. (PR #105, @brews)</li> <li>Fixed broken code quality checks in CI. Now using <code>ruff</code> instead of <code>flake8</code>. (PR #107, @brews)</li> <li>Minor code style cleanup. (PR #133, @brews)</li> </ul>"},{"location":"changelog/#040-2023-07-06","title":"0.4.0 - 2023-07-06","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Functions to concatenate input damages across batches. (PR #83, @davidrzhdu)</li> <li>New unit tests for dscim/utils/input_damages.py. (PR #68, @davidrzhdu)</li> <li>New unit tests for dscim/utils/rff.py. (PR #73, @JMGilbert)</li> <li>New unit tests for dscim/dscim/preprocessing.py. (PR #67, @JMGilbert)</li> <li>Functions used for producing RFF weights. (PR #66, @davidrzhdu)</li> </ul>"},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Re-enable equity menu option tests. (PR #84, @JMGilbert)</li> <li>Changed <code>coastal_inputs</code> function to work with new version of coastal outputs. (PR #75, @davidrzhdu)</li> <li>Changed <code>prep_mortality_damages</code> function to work with new format mortality outputs. (PR #74 and PR #68, @JMGilbert)</li> <li>Included US territories in damages and economic variable subsetting. (PR #78, @JMGilbert)</li> <li>Changed format of <code>eta_rhos</code> to allow for multiple values of <code>rho</code> for the same <code>eta</code>. (PR #65, @JMGilbert)</li> <li>Removed incomplete \"time_trend\" extrapolation option from <code>dscim.utils.utils.model_outputs()</code>, along with unused function arguments. This is a breaking change. (PR #53, @brews)</li> </ul>"},{"location":"changelog/#removed_1","title":"Removed","text":"<ul> <li>Removed <code>clip_damage</code> function in <code>dscim/preprocessing/preprocessing.py</code>. (PR #67, @JMGilbert)</li> <li>Removed climate reformatting functions and files -- to be added back with climate file generation. (PR #67, @JMGilbert)</li> <li>Remove diagnostics module. (PR #60, @JMGilbert)</li> <li>Remove old/unnecessary files. (PR #57, @JMGilbert)</li> <li>Remove unused \u201csave_path\u201d and \u201cec_cls\u201d from <code>read_energy_files_parallel()</code>. (PR #56, @davidrzhdu)</li> </ul>"},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>Make all input damages output files with correct chunksizes. (PR #83, @JMGilbert)</li> <li>Add <code>.load()</code> to every loading of population data from EconVars. (PR #82, @davidrzhdu)</li> <li>Make <code>compute_ag_damages</code> function correctly save outputs in float32. (PR #72 and PR #82, @davidrzhdu)</li> <li>Make rff damage functions read in and save out in the proper filepath structure. (PR #79, @JMGilbert)</li> <li>Enter the proper functional form of isoelastic utility when <code>eta = 1</code>. (PR #65, @JMGilbert)</li> <li>Pin numpy version to stop tests failing. (PR #60, @JMGilbert)</li> </ul>"},{"location":"changelog/#030-2022-09-29","title":"0.3.0 - 2022-09-29","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>New unit tests. (PR #50, PR #52, @brews)</li> </ul>"},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>Removed unused \u201cpulseyrs\u201d and \u201cglobal_cons\u201d from <code>convert_old_to_newformat_AR()</code> and <code>run_rff()</code>. Note this is a breaking change. (PR #51, @davidrzhdu, @kemccusker)</li> <li>Updated README with additional technical details. (PR #49, @brews)</li> </ul>"},{"location":"changelog/#fixed_4","title":"Fixed","text":"<ul> <li>Fix xarray <code>.drop()</code> deprecation. (PR #54, @brews)</li> <li>Fix pathlib.Path/str <code>TypeError</code> in <code>preprocessing.clip_damages()</code>. (PR #55, @brews)</li> <li>Minor fixes to docstrs. (PR #50, PR #52, @brews)</li> </ul>"},{"location":"changelog/#021-2022-09-22","title":"0.2.1 - 2022-09-22","text":""},{"location":"changelog/#fixed_5","title":"Fixed","text":"<ul> <li>Fix issue #45 by allowing for <code>emission_scenario</code> to be <code>None</code>. (PR #46, PR #47, @JMGilbert)</li> </ul>"},{"location":"changelog/#020-2022-09-16","title":"0.2.0 - 2022-09-16","text":""},{"location":"changelog/#changed_4","title":"Changed","text":"<ul> <li>Remove mutable argument defaults to avoid gotchas. (PR #44, @brews)</li> <li>Quiet unused(?), common, logging messages to terminal. (PR #14, @brews)</li> </ul>"},{"location":"changelog/#fixed_6","title":"Fixed","text":"<ul> <li>Add missing <code>self</code> arg to <code>global_consumption_calculation</code> abstract method. (PR #43, @brews)</li> </ul>"},{"location":"changelog/#010-2022-08-30","title":"0.1.0 - 2022-08-30","text":"<ul> <li>Initial release.</li> </ul>"},{"location":"menu/baseline/","title":"baseline","text":"<p>Classes:</p> Name Description <code>Baseline</code> <p>Adding up option</p>"},{"location":"menu/baseline/#dscim.menu.baseline.Baseline","title":"dscim.menu.baseline.Baseline","text":"<p>               Bases: <code>MainRecipe</code></p> <p>Adding up option</p> <p>Methods:</p> Name Description <code>global_consumption_calculation</code> <p>Calculate global consumption</p> <code>global_damages_calculation</code> <p>Call global damages</p> Source code in <code>src/dscim/menu/baseline.py</code> <pre><code>class Baseline(MainRecipe):\n    \"\"\"Adding up option\"\"\"\n\n    NAME = \"adding_up\"\n    __doc__ = MainRecipe.__doc__\n\n    def ce_cc(self):\n        pass\n\n    def ce_no_cc(self):\n        pass\n\n    def global_damages_calculation(self):\n        \"\"\"Call global damages\"\"\"\n        return self.adding_up_damages.to_dataframe(\"damages\").reset_index()\n\n    def calculated_damages(self):\n        pass\n\n    def ce_cc_calculation(self):\n        pass\n\n    def ce_test(self):\n        pass\n\n    def ce_no_cc_calculation(self):\n        pass\n\n    def global_consumption_calculation(self, disc_type):\n        \"\"\"Calculate global consumption\"\"\"\n\n        if (disc_type == \"constant\") or (\"ramsey\" in disc_type):\n            global_cons_no_cc = self.gdp.sum(dim=[\"region\"])\n\n        elif disc_type == \"constant_model_collapsed\":\n            global_cons_no_cc = self.gdp.sum(dim=[\"region\"]).mean(dim=[\"model\"])\n\n        elif \"gwr\" in disc_type:\n            global_cons_no_cc = self.gdp.sum(dim=[\"region\"]).mean(dim=[\"model\", \"ssp\"])\n\n        global_cons_no_cc.name = f\"global_cons_{disc_type}\"\n\n        return global_cons_no_cc\n</code></pre>"},{"location":"menu/baseline/#dscim.menu.baseline.Baseline.global_consumption_calculation","title":"dscim.menu.baseline.Baseline.global_consumption_calculation","text":"<pre><code>global_consumption_calculation(disc_type)\n</code></pre> <p>Calculate global consumption</p> Source code in <code>src/dscim/menu/baseline.py</code> <pre><code>def global_consumption_calculation(self, disc_type):\n    \"\"\"Calculate global consumption\"\"\"\n\n    if (disc_type == \"constant\") or (\"ramsey\" in disc_type):\n        global_cons_no_cc = self.gdp.sum(dim=[\"region\"])\n\n    elif disc_type == \"constant_model_collapsed\":\n        global_cons_no_cc = self.gdp.sum(dim=[\"region\"]).mean(dim=[\"model\"])\n\n    elif \"gwr\" in disc_type:\n        global_cons_no_cc = self.gdp.sum(dim=[\"region\"]).mean(dim=[\"model\", \"ssp\"])\n\n    global_cons_no_cc.name = f\"global_cons_{disc_type}\"\n\n    return global_cons_no_cc\n</code></pre>"},{"location":"menu/baseline/#dscim.menu.baseline.Baseline.global_damages_calculation","title":"dscim.menu.baseline.Baseline.global_damages_calculation","text":"<pre><code>global_damages_calculation()\n</code></pre> <p>Call global damages</p> Source code in <code>src/dscim/menu/baseline.py</code> <pre><code>def global_damages_calculation(self):\n    \"\"\"Call global damages\"\"\"\n    return self.adding_up_damages.to_dataframe(\"damages\").reset_index()\n</code></pre>"},{"location":"menu/decorators/","title":"decorators","text":"<p>Functions:</p> Name Description <code>save</code> <p>Decorator for saving output to NetCDF or CSV format</p>"},{"location":"menu/decorators/#dscim.menu.decorators.save","title":"dscim.menu.decorators.save","text":"<pre><code>save(name)\n</code></pre> <p>Decorator for saving output to NetCDF or CSV format</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>Name of file. The file will be modified by class elements</p> required <p>Returns:</p> Type Description <code>    None</code> Source code in <code>src/dscim/menu/decorators.py</code> <pre><code>def save(name):\n    \"\"\"Decorator for saving output to NetCDF or CSV format\n\n    Parameters\n    ----------\n    name str\n        Name of file. The file will be modified by class elements\n\n    Returns\n    -------\n        None\n    \"\"\"\n\n    def decorator_save(func):\n        @functools.wraps(func)\n        def save_wrap(self, *args, **kwargs):\n            out = func(self, *args, **kwargs)\n            save = out\n\n            if (self.save_path is not None) and (name in self.save_files):\n                if not os.path.exists(self.save_path):\n                    os.makedirs(self.save_path)\n\n                filename = f\"{self.NAME}_{self.discounting_type}_eta{self.eta}_rho{self.rho}_{name}{self.filename_suffix}\"\n                filename_path = os.path.join(self.save_path, filename)\n\n                if isinstance(save, xr.DataArray):\n                    save = save.rename(name).to_dataset()\n                    save.attrs = self.output_attrs\n\n                    # change `None` object to str(None)\n                    for att in save.attrs:\n                        if save.attrs[att] is None:\n                            save.attrs.update({att: \"None\"})\n\n                    self.logger.info(f\"Saving {filename_path}.nc4\")\n                    save.to_netcdf(f\"{filename_path}.nc4\")\n\n                elif isinstance(save, xr.Dataset):\n                    save.attrs = self.output_attrs\n\n                    # change `None` object to str(None)\n                    for att in save.attrs:\n                        if save.attrs[att] is None:\n                            save.attrs.update({att: \"None\"})\n\n                    self.logger.info(f\"Saving {filename_path}.nc4\")\n                    save.to_netcdf(f\"{filename_path}.nc4\")\n\n                elif isinstance(save, pd.DataFrame):\n                    self.logger.info(f\"Saving {filename_path}.csv\")\n                    save.to_csv(f\"{filename_path}.csv\", index=False)\n\n            return out\n\n        return save_wrap\n\n    return decorator_save\n</code></pre>"},{"location":"menu/equity/","title":"equity","text":"<p>Classes:</p> Name Description <code>EquityRecipe</code> <p>Equity option</p>"},{"location":"menu/equity/#dscim.menu.equity.EquityRecipe","title":"dscim.menu.equity.EquityRecipe","text":"<p>               Bases: <code>MainRecipe</code></p> <p>Equity option</p> <p>Methods:</p> Name Description <code>risk_aversion_growth_rates</code> <p>Calculate risk aversion global consumption growth rates</p> Source code in <code>src/dscim/menu/equity.py</code> <pre><code>class EquityRecipe(MainRecipe):\n    \"\"\"Equity option\"\"\"\n\n    NAME = \"equity\"\n    __doc__ = MainRecipe.__doc__\n\n    def ce_cc_calculation(self) -&gt; xr.DataArray:\n        if \"gwr\" in self.discounting_type:\n            dims = [\"ssp\", \"model\", \"region\"]\n        else:\n            dims = [\"region\"]\n\n        ce_cc = self.risk_aversion_damages(\"cc\").cc\n\n        # reindex to make sure all regions are being calculated\n        if len(ce_cc.region.values) &lt; len(self.gdppc.region.values):\n            ce_cc = ce_cc.reindex({\"region\": self.gdppc.region.values})\n            # assign zero damages to places with missing damages\n            ce_cc = xr.where(np.isnan(ce_cc), self.gdppc, ce_cc)\n\n        ce_array = c_equivalence(\n            ce_cc,\n            dims=dims,\n            weights=self.pop,\n            eta=self.eta,\n        )\n\n        return ce_array.rename(\"cc\")\n\n    def ce_no_cc_calculation(self) -&gt; xr.DataArray:\n        if \"gwr\" in self.discounting_type:\n            dims = [\"ssp\", \"model\", \"region\"]\n        else:\n            dims = [\"region\"]\n\n        ce_no_cc = self.risk_aversion_damages(\"no_cc\").no_cc\n        # reindex to make sure all regions are being calculated\n        if len(ce_no_cc.region.values) &lt; len(self.gdppc.region.values):\n            ce_no_cc = ce_no_cc.reindex({\"region\": self.gdppc.region.values})\n            # assign zero damages to places with missing damages\n            ce_no_cc = xr.where(np.isnan(ce_no_cc), self.gdppc, ce_no_cc)\n\n        ce_no_cc_array = c_equivalence(\n            ce_no_cc,\n            dims=dims,\n            weights=self.pop,\n            eta=self.eta,\n        )\n\n        return ce_no_cc_array.rename(\"no_cc\")\n\n    @property\n    def calculated_damages(self) -&gt; xr.DataArray:\n        return self.ce_no_cc - self.ce_cc\n\n    def global_damages_calculation(self) -&gt; pd.DataFrame:\n        dams_collapse = self.calculated_damages * self.collapsed_pop.sum(dim=\"region\")\n        df = dams_collapse.to_dataframe(\"damages\").reset_index()\n\n        if \"gwr\" in self.discounting_type:\n            df = df.assign(\n                ssp=str(list(self.gdp.ssp.values)),\n                model=str(list(self.gdp.model.values)),\n            )\n\n        return df\n\n    def global_consumption_calculation(self, disc_type):\n        # get global consumption certainty equivalent across regions\n        ce_cons = c_equivalence(\n            self.gdppc,\n            dims=\"region\",\n            weights=self.pop,\n            eta=self.eta,\n        )\n\n        if disc_type == \"constant_model_collapsed\":\n            global_cons_no_cc = (ce_cons * self.pop.sum(\"region\")).mean(\"model\")\n        elif (disc_type == \"constant\") | (\"ramsey\" in disc_type):\n            global_cons_no_cc = ce_cons * self.pop.sum(\"region\")\n        elif \"gwr\" in disc_type:\n            global_cons_no_cc = self.ce(\n                ce_cons, dims=[\"ssp\", \"model\"]\n            ) * self.collapsed_pop.sum(\"region\")\n\n        # Convert to array in case xarray became temperamental\n        # @TODO: remove this line\n        # if isinstance(global_cons_no_cc, xr.Dataset):\n        #     global_cons_no_cc = global_cons_no_cc.to_array()\n\n        global_cons_no_cc.name = f\"global_cons_{disc_type}\"\n\n        return global_cons_no_cc\n\n    def risk_aversion_growth_rates(self):\n        \"\"\"Calculate risk aversion global consumption growth rates\n\n        This function calculates the risk aversion version of global\n        consumption per capita growth rates, in order to cap growth\n        the equity recipe to growth in the risk aversion recipe.\n\n        Returns\n        -------\n            xr.DataArray\n        \"\"\"\n        if (self.discounting_type == \"constant\") or (\"ramsey\" in self.discounting_type):\n            global_cons_no_cc = self.gdp.sum(dim=[\"region\"])\n\n        elif self.discounting_type == \"constant_model_collapsed\":\n            global_cons_no_cc = self.gdp.sum(dim=[\"region\"]).mean(dim=[\"model\"])\n\n        elif \"gwr\" in self.discounting_type:\n            ce_cons = self.ce(self.gdppc, dims=[\"ssp\", \"model\"])\n            global_cons_no_cc = (ce_cons * self.collapsed_pop).sum(dim=[\"region\"])\n\n        # Calculate global consumption per capita\n        globalc_pc = global_cons_no_cc / self.collapsed_pop.sum(\"region\")\n\n        # calculate growth rates\n        growth_rates = globalc_pc.diff(\"year\") / globalc_pc.shift(year=1)\n\n        return growth_rates\n</code></pre>"},{"location":"menu/equity/#dscim.menu.equity.EquityRecipe.risk_aversion_growth_rates","title":"dscim.menu.equity.EquityRecipe.risk_aversion_growth_rates","text":"<pre><code>risk_aversion_growth_rates()\n</code></pre> <p>Calculate risk aversion global consumption growth rates</p> <p>This function calculates the risk aversion version of global consumption per capita growth rates, in order to cap growth the equity recipe to growth in the risk aversion recipe.</p> <p>Returns:</p> Type Description <code>    xr.DataArray</code> Source code in <code>src/dscim/menu/equity.py</code> <pre><code>def risk_aversion_growth_rates(self):\n    \"\"\"Calculate risk aversion global consumption growth rates\n\n    This function calculates the risk aversion version of global\n    consumption per capita growth rates, in order to cap growth\n    the equity recipe to growth in the risk aversion recipe.\n\n    Returns\n    -------\n        xr.DataArray\n    \"\"\"\n    if (self.discounting_type == \"constant\") or (\"ramsey\" in self.discounting_type):\n        global_cons_no_cc = self.gdp.sum(dim=[\"region\"])\n\n    elif self.discounting_type == \"constant_model_collapsed\":\n        global_cons_no_cc = self.gdp.sum(dim=[\"region\"]).mean(dim=[\"model\"])\n\n    elif \"gwr\" in self.discounting_type:\n        ce_cons = self.ce(self.gdppc, dims=[\"ssp\", \"model\"])\n        global_cons_no_cc = (ce_cons * self.collapsed_pop).sum(dim=[\"region\"])\n\n    # Calculate global consumption per capita\n    globalc_pc = global_cons_no_cc / self.collapsed_pop.sum(\"region\")\n\n    # calculate growth rates\n    growth_rates = globalc_pc.diff(\"year\") / globalc_pc.shift(year=1)\n\n    return growth_rates\n</code></pre>"},{"location":"menu/main_recipe/","title":"main_recipe","text":"<p>Classes:</p> Name Description <code>MainRecipe</code> <p>Main class for DSCIM execution.</p>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe","title":"dscim.menu.main_recipe.MainRecipe","text":"<p>               Bases: <code>StackedDamages</code>, <code>ABC</code></p> <p>Main class for DSCIM execution.</p> <p>Parameters:</p> Name Type Description Default <code>discounting_type</code> <code>str</code> <p>Choice of discounting: <code>euler_gwr</code>, <code>euler_ramsey</code>, <code>constant</code>, <code>constant_gwr</code>, <code>naive_ramsey</code>, <code>naive_gwr</code>, <code>gwr_gwr</code>.</p> <code>None</code> <code>discrete_discounting</code> <p>Discounting is discrete if <code>True</code>, else continuous (default is <code>False</code>).</p> <code>False</code> <code>fit_type</code> <code>str</code> <p>Type of damage function estimation: <code>'ols'</code>, <code>'quantreg'</code></p> <code>'ols'</code> <code>weitzman_parameter</code> <p>If &lt;= 1: The share of global consumption below which bottom coding is implemented. If &gt; 1: Absolute dollar value of global consumption below which bottom. Default is [0.1, 0.5]. coding is implemented.</p> <code>None</code> <code>fair_aggregation</code> <code>list of str or None</code> <p>How to value climate uncertainty from FAIR: <code>median</code>, <code>mean</code>, <code>ce</code>, <code>median_params</code>. Default is [\"ce\", \"mean\", \"gwr_mean\", \"median\", \"median_params\"].</p> <code>None</code> <code>rho</code> <code>float</code> <p>Pure rate of time preference parameter</p> <code>0.00461878399</code> <code>fair_dims</code> <code>list of str or None</code> <p>List of dimensions over which the FAIR CE/mean/median options should be collapsed. Default value is [\"simulation\"], but lists such as [\"simulation\", \"rcp\", \"ssp\"] can be passed. Note: If dimensions other than 'simulation' are passed, 'median_params' fair aggregation cannot be passed.</p> <code>None</code> <p>Methods:</p> Name Description <code>calculate_discount_factors</code> <p>Calculates the stream of discount factors based on the Euler equation that defines an optimal</p> <code>calculate_scc</code> <p>Calculate range of FAIR-aggregated SCCs</p> <code>calculate_stream_discount_factors</code> <p>Stream of discount factors</p> <code>calculated_damages</code> <p>Calculate damages (difference between CEs) for collapsing</p> <code>ce</code> <p>Rechunk data appropriately and apply the certainty equivalence</p> <code>ce_cc_calculation</code> <p>Calculate CE damages depending on discount type</p> <code>ce_no_cc_calculation</code> <p>Calculate GDP CE depending on discount type.</p> <code>collapsed_pop</code> <p>Collapse population according to discount type.</p> <code>damage_function</code> <p>Calls damage function calculation method.</p> <code>damage_function_calculation</code> <p>The damage function model fit may be : (1) ssp specific, (2) ssp-model specific, (3) unique across ssp-model.</p> <code>damage_function_points</code> <p>Global damages by RCP/GCM or SLR</p> <code>discounted_damages</code> <p>Discount marginal damages. Distinguishes between constant discount rates method and non-constant discount rates.</p> <code>full_uncertainty_iqr</code> <p>Calculate the distribution of quantile-weighted SCCs produced from</p> <code>global_consumption</code> <p>Global consumption without climate change</p> <code>global_consumption_calculation</code> <p>Calculation of global consumption without climate change</p> <code>global_consumption_no_pulse</code> <p>Global consumption under FAIR control scenario.</p> <code>global_consumption_per_capita</code> <p>Global consumption per capita</p> <code>global_consumption_pulse</code> <p>Global consumption under FAIR pulse scenario.</p> <code>global_damages_calculation</code> <p>Calculate global collapsed damages for a desired discount type</p> <code>marginal_damages</code> <p>Marginal damages due to additional pulse</p> <code>order_plate</code> <p>Execute menu option section and save results</p> <code>order_scc</code> <p>Execute menu option section and save results</p> <code>stat_uncertainty_iqr</code> <p>Calculate the distribution of quantile-weighted SCCs produced from</p> <code>uncollapsed_sccs</code> <p>Calculate full distribution of SCCs without FAIR aggregation</p> <code>weitzman_min</code> <p>Implements bottom coding that fixes marginal utility below a threshold</p> <p>Attributes:</p> Name Type Description <code>ce_cc</code> <p>Certainty equivalent of consumption with climate change damages</p> <code>ce_fair_no_pulse</code> <p>Certainty equivalent of global consumption under FAIR control scenario</p> <code>ce_fair_pulse</code> <p>Certainty equivalent of global consumption under FAIR pulse scenario</p> <code>ce_no_cc</code> <p>Certainty equivalent of consumption without climate change damages</p> <code>damage_function_coefficients</code> <code>Dataset</code> <p>Load damage function coefficients if the coefficients are provided by the user.</p> <code>damage_function_fit</code> <code>Dataset</code> <p>Load fitted damage function if the fit is provided by the user.</p> <code>gmsl_max</code> <p>This function finds the GMSL value at which the damage function</p> <code>median_params_marginal_damages</code> <p>Calculate marginal damages due to a pulse using a FAIR simulation</p> <code>output_attrs</code> <p>Return dict with class attributes for output metadata</p> Source code in <code>src/dscim/menu/main_recipe.py</code> <pre><code>class MainRecipe(StackedDamages, ABC):\n    \"\"\"Main class for DSCIM execution.\n\n    Parameters\n    ----------\n    discounting_type : str\n        Choice of discounting: ``euler_gwr``, ``euler_ramsey``, ``constant``, ``constant_gwr``, ``naive_ramsey``,\n        ``naive_gwr``, ``gwr_gwr``.\n    discrete_discounting: boolean\n        Discounting is discrete if ``True``, else continuous (default is ``False``).\n    fit_type : str\n        Type of damage function estimation: ``'ols'``, ``'quantreg'``\n    weitzman_parameter: list of float or None, optional\n        If &lt;= 1: The share of global consumption below which bottom coding is implemented.\n        If &gt; 1: Absolute dollar value of global consumption below which bottom.\n        Default is [0.1, 0.5].\n        coding is implemented.\n    fair_aggregation : list of str or None, optional\n        How to value climate uncertainty from FAIR: ``median``, ``mean``,\n        ``ce``, ``median_params``. Default is [\"ce\", \"mean\", \"gwr_mean\",\n        \"median\", \"median_params\"].\n    rho : float\n        Pure rate of time preference parameter\n    fair_dims : list of str or None, optional\n        List of dimensions over which the FAIR CE/mean/median options should be collapsed. Default value is [\"simulation\"], but lists such as [\"simulation\", \"rcp\", \"ssp\"] can be passed. Note: If dimensions other than 'simulation' are passed, 'median_params' fair aggregation cannot be passed.\n    \"\"\"\n\n    NAME = \"\"\n    CONST_DISC_RATES = [0.01, 0.015, 0.02, 0.025, 0.03, 0.05]\n    DISCOUNT_TYPES = [\n        \"constant\",\n        \"constant_model_collapsed\",\n        \"constant_gwr\",\n        \"naive_ramsey\",\n        \"euler_ramsey\",\n        \"naive_gwr\",\n        \"gwr_gwr\",\n        \"euler_gwr\",\n    ]\n    FORMULAS = [\n        \"damages ~ -1 + np.power(anomaly, 2)\",\n        \"damages ~ gmsl + np.power(gmsl, 2)\",\n        \"damages ~ -1 + gmsl + np.power(gmsl, 2)\",\n        \"damages ~ -1 + gmsl\",\n        \"damages ~ anomaly + np.power(anomaly, 2)\",\n        \"damages ~ -1 + anomaly + np.power(anomaly, 2)\",\n        \"damages ~ -1 + gmsl + anomaly + np.power(anomaly, 2)\",\n        \"damages ~ -1 + anomaly + np.power(anomaly, 2) + gmsl + np.power(gmsl, 2)\",\n        \"damages ~ -1 + anomaly * gmsl + anomaly * np.power(gmsl, 2) + gmsl * np.power(anomaly, 2) + np.power(anomaly, 2) * np.power(gmsl, 2)\",\n        \"damages ~ anomaly + np.power(anomaly, 2) + gmsl + np.power(gmsl, 2)\",\n        \"damages ~ -1 + anomaly:gmsl + anomaly:np.power(gmsl, 2) + gmsl:np.power(anomaly, 2) + np.power(anomaly, 2):np.power(gmsl, 2)\",\n        \"damages ~ -1 + gmsl:anomaly + gmsl:np.power(anomaly, 2)\",\n    ]\n\n    def __init__(\n        self,\n        econ_vars,\n        climate_vars,\n        sector,\n        formula,\n        sector_path=None,\n        save_path=None,\n        rho=0.00461878399,\n        eta=1.421158116,\n        fit_type=\"ols\",\n        discounting_type=None,\n        ext_method=\"global_c_ratio\",\n        ext_subset_start_year=2085,\n        ext_subset_end_year=2099,\n        ext_end_year=2300,\n        subset_dict=None,\n        ce_path=None,\n        damage_function_path=None,\n        clip_gmsl=False,\n        gdppc_bottom_code=39.39265060424805,\n        scc_quantiles=None,\n        scenario_dimensions=None,\n        weitzman_parameter=None,\n        fair_aggregation=None,\n        filename_suffix=\"\",\n        discrete_discounting=False,\n        quantreg_quantiles=None,\n        quantreg_weights=None,\n        full_uncertainty_quantiles=None,\n        extrap_formula=None,\n        fair_dims=None,\n        save_files=None,\n        **kwargs,\n    ):\n        if scc_quantiles is None:\n            scc_quantiles = [0.05, 0.17, 0.25, 0.5, 0.75, 0.83, 0.95]\n\n        if weitzman_parameter is None:\n            weitzman_parameter = [0.1, 0.5]\n\n        if fair_aggregation is None:\n            fair_aggregation = [\"ce\", \"mean\", \"gwr_mean\", \"median\", \"median_params\"]\n\n        if quantreg_quantiles is None:\n            quantreg_quantiles = [\n                0.05,\n                0.1,\n                0.15,\n                0.2,\n                0.25,\n                0.3,\n                0.35,\n                0.4,\n                0.45,\n                0.5,\n                0.55,\n                0.6,\n                0.65,\n                0.7,\n                0.75,\n                0.8,\n                0.85,\n                0.9,\n                0.95,\n            ]\n\n        if quantreg_weights is None:\n            quantreg_weights = [\n                0.075,\n                0.05,\n                0.05,\n                0.05,\n                0.05,\n                0.05,\n                0.05,\n                0.05,\n                0.05,\n                0.05,\n                0.05,\n                0.05,\n                0.05,\n                0.05,\n                0.05,\n                0.05,\n                0.05,\n                0.05,\n                0.075,\n            ]\n\n        if full_uncertainty_quantiles is None:\n            full_uncertainty_quantiles = [\n                0.01,\n                0.05,\n                0.17,\n                0.25,\n                0.5,\n                0.75,\n                0.83,\n                0.95,\n                0.99,\n            ]\n\n        if fair_dims is None:\n            fair_dims = [\"simulation\"]\n\n        if save_files is None:\n            save_files = [\n                \"damage_function_points\",\n                \"damage_function_coefficients\",\n                \"damage_function_fit\",\n                \"marginal_damages\",\n                \"discount_factors\",\n                \"uncollapsed_sccs\",\n                \"scc\",\n                \"uncollapsed_discount_factors\",\n                \"uncollapsed_marginal_damages\",\n                \"global_consumption\",\n                \"global_consumption_no_pulse\",\n            ]\n\n        super().__init__(\n            sector_path=sector_path,\n            save_path=save_path,\n            econ_vars=econ_vars,\n            climate_vars=climate_vars,\n            gdppc_bottom_code=gdppc_bottom_code,\n            eta=eta,\n            subset_dict=subset_dict,\n            ce_path=ce_path,\n        )\n\n        self.rho = rho\n        self.eta = eta\n        self.fit_type = fit_type\n        self.fair_aggregation = fair_aggregation\n        self.filename_suffix = filename_suffix\n        self.weitzman_parameter = weitzman_parameter\n        self.discrete_discounting = discrete_discounting\n        self.discounting_type = discounting_type\n        self.sector = sector\n        self.save_path = save_path\n        self.damage_function_path = damage_function_path\n        self.ext_subset_start_year = ext_subset_start_year\n        self.ext_subset_end_year = ext_subset_end_year\n        self.ext_end_year = ext_end_year\n        self.ext_method = ext_method\n        self.clip_gmsl = clip_gmsl\n        self.scenario_dimensions = scenario_dimensions\n        self.scc_quantiles = scc_quantiles\n        self.quantreg_quantiles = quantreg_quantiles\n        self.quantreg_weights = quantreg_weights\n        self.full_uncertainty_quantiles = full_uncertainty_quantiles\n        self.formula = formula\n        self.ce_path = ce_path\n        self.extrap_formula = extrap_formula\n        self.fair_dims = fair_dims\n        self.save_files = save_files\n        self.__dict__.update(**kwargs)\n        self.kwargs = kwargs\n\n        self.logger = logging.getLogger(__name__)\n\n        if self.quantreg_quantiles is not None:\n            assert len(self.quantreg_quantiles) == len(\n                self.quantreg_weights\n            ), \"Length of quantreg quantiles does not match length of weights.\"\n\n        assert (\n            self.discounting_type in self.DISCOUNT_TYPES\n        ), f\"Discount type not implemented. Try one of {self.DISCOUNT_TYPES}.\"\n\n        assert (\n            self.formula in self.FORMULAS\n        ), f\"Formula not implemented. Try one of {self.FORMULAS}.\"\n\n        # Set stream of discounts to None if discounting_type is 'constant'\n        # 'constant_model_collapsed' should be here except that we allow\n        # for a collapsed-model Ramsey rate to be calculated (for labour\n        # and energy purposes)\n        if self.discounting_type in [\n            \"constant\",\n            \"constant_model_collapsed\",\n            \"constant_gwr\",\n        ]:\n            self.stream_discount_factors = None\n\n        # assert formulas for which clip_gmsl is implemented\n        if self.clip_gmsl:\n            assert self.formula in [\n                \"damages ~ -1 + anomaly + np.power(anomaly, 2) + gmsl + np.power(gmsl, 2)\",\n                \"damages ~ -1 + gmsl + np.power(gmsl, 2)\",\n            ]\n\n    def __repr__(self):\n        return f\"\"\"\n        Running {self.NAME}\n        sector: {self.sector}\n        discounting: {self.discounting_type}\n        eta: {self.eta}\n        rho: {self.rho}\n        \"\"\"\n\n    def order_plate(self, course):\n        \"\"\"\n        Execute menu option section and save results\n\n        This method is a entry point to the class and allows the user to\n        calculate different elements of a specific menu option. These elements\n        will automatically be saved in the path defined in `save_path`.\n\n        Parameters\n        ----------\n        course str\n            Output to be calculated. Options are:\n                - `damage_function`: Return and save all damage function\n                  elements including damage function points, coefficients, and\n                  fitted values.\n                - `scc`: Return Social Cost of Carbon calculation. All elements\n                from `damage_function` are saved and returned.\n\n        Returns\n        -------\n        None. Saved all elements to `save_path`\n\n        \"\"\"\n\n        self.logger.info(f\"\\n Executing {self.__repr__()}\")\n\n        def damage_function():\n            self.logger.info(\"Processing damage functions ...\")\n            if self.damage_function_path is None:\n                self.logger.info(\n                    \"Existing damage functions not found. Damage points will be loaded.\"\n                )\n                self.damage_function_points\n            self.damage_function_coefficients\n            try:\n                self.damage_function_fit\n            except FileNotFoundError:\n                pass\n\n        def scc():\n            damage_function()\n            self.global_consumption\n            self.global_consumption_no_pulse\n            self.logger.info(\"Processing SCC calculation ...\")\n            if self.fit_type == \"quantreg\":\n                self.full_uncertainty_iqr\n                # stat_uncertainty_iqr function expects collapsed SCCs, so a fair aggregation is required\n                if len(self.fair_aggregation) &gt; 0:\n                    self.calculate_scc\n                    self.stat_uncertainty_iqr\n            else:\n                if len(self.fair_aggregation) &gt; 0:\n                    self.stream_discount_factors\n                    self.calculate_scc\n                self.uncollapsed_sccs\n                self.uncollapsed_marginal_damages\n                self.uncollapsed_discount_factors\n\n        course_dict = {\"damage_function\": damage_function, \"scc\": scc}\n\n        try:\n            course_dict[course]()\n            self.logger.info(f\"Results available: {self.save_path}\")\n        except KeyError as e:\n            self.logger.error(f\"{course} is not a valid option: {e}\")\n            raise e\n        except Exception as e:\n            self.logger.error(\"Error detected.\")\n            raise e\n\n        return None\n\n    def order_scc(self):\n        \"\"\"\n        Execute menu option section and save results\n\n        This method is a wrapper to `order_plate` that calls the \"scc\" course,\n        which is the Social Cost of Carbon calculation. Elements involved in the calculation\n        (`fair` and `damage_function`) will automatically be saved in the path\n        defined in `save_path`.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        xr.Dataset of SCCs\n\n        \"\"\"\n\n        self.logger.info(f\"\\n Executing {self.__repr__()}\")\n\n        try:\n            sccds = self.calculate_scc\n            self.logger.info(f\"Results available: {self.save_path}\")\n        except Exception as e:\n            self.logger.error(\"Error detected.\")\n            raise e\n\n        if (\"rcp45\" in sccds.rcp) or (\"rcp85\" in sccds.rcp):\n            # leave the dataset alone if there are already rcp scenario names\n            pass\n        else:\n            # rename the CMIP6 scenario names that start with \"ssp*\"\n            sccds = sccds.sortby(sccds.rcp)\n\n            rcpdt = {\n                \"ssp126\": \"RCP2.6\",\n                \"ssp245\": \"RCP4.5\",\n                \"ssp370\": \"RCP7.0\",\n                \"ssp460\": \"RCP6.0\",\n                \"ssp585\": \"RCP8.5\",\n            }\n            rlst = []\n            for rcp in sccds.rcp.values:\n                rlst.append(rcpdt[rcp])\n            sccds.coords[\"rcp\"] = rlst\n            sccds = sccds.sortby(sccds.rcp)\n\n        return sccds.squeeze(drop=True)\n\n    @property\n    def output_attrs(self):\n        \"\"\"Return dict with class attributes for output metadata\n\n        Returns\n        ------\n            A dict Class metadata\n        \"\"\"\n\n        # find machine name\n        machine_name = os.getenv(\"HOSTNAME\")\n        if machine_name is None:\n            try:\n                machine_name = os.uname()[1]\n            except AttributeError:\n                machine_name = \"unknown\"\n\n        # find git commit hash\n        try:\n            label = subprocess.check_output([\"git\", \"describe\", \"--always\"]).strip()\n        except CalledProcessError:\n            label = \"unknown\"\n\n        meta = {}\n        for attr_dict in [\n            vars(self),\n            vars(vars(self)[\"climate\"]),\n            vars(vars(self)[\"econ_vars\"]),\n        ]:\n            meta.update(\n                {\n                    k: v\n                    for k, v in attr_dict.items()\n                    if (type(v) not in [xr.DataArray, xr.Dataset, pd.DataFrame])\n                    and k not in [\"damage_function\", \"logger\"]\n                }\n            )\n\n        # update with git hash and machine name\n        meta.update(dict(machine=machine_name, commit=label))\n\n        # convert to strs\n        meta = {k: v if type(v) in [int, float] else str(v) for k, v in meta.items()}\n\n        return meta\n\n    @cachedproperty\n    def collapsed_pop(self):\n        \"\"\"Collapse population according to discount type.\"\"\"\n        if (self.discounting_type == \"constant\") or (\"ramsey\" in self.discounting_type):\n            pop = self.pop\n        elif self.discounting_type == \"constant_model_collapsed\":\n            pop = self.pop.mean(\"model\")\n        elif \"gwr\" in self.discounting_type:\n            pop = self.pop.mean([\"model\", \"ssp\"])\n        return pop\n\n    @abstractmethod\n    def ce_cc_calculation(self):\n        \"\"\"Calculate CE damages depending on discount type\"\"\"\n\n    @abstractmethod\n    def ce_no_cc_calculation(self):\n        \"\"\"Calculate GDP CE depending on discount type.\"\"\"\n\n    @abstractmethod\n    def calculated_damages(self):\n        \"\"\"Calculate damages (difference between CEs) for collapsing\"\"\"\n\n    @abstractmethod\n    def global_damages_calculation(self):\n        \"\"\"Calculate global collapsed damages for a desired discount type\"\"\"\n\n    @property\n    def ce_cc(self):\n        \"\"\"Certainty equivalent of consumption with climate change damages\"\"\"\n        return self.ce_cc_calculation()\n\n    @property\n    def ce_no_cc(self):\n        \"\"\"Certainty equivalent of consumption without climate change damages\"\"\"\n        return self.ce_no_cc_calculation()\n\n    @cachedproperty\n    @save(name=\"damage_function_points\")\n    def damage_function_points(self) -&gt; pd.DataFrame:\n        \"\"\"Global damages by RCP/GCM or SLR\n\n        Returns\n        --------\n            pd.DataFrame\n        \"\"\"\n        df = self.global_damages_calculation()\n\n        if \"slr\" in df.columns:\n            df = df.merge(self.climate.gmsl, on=[\"year\", \"slr\"])\n        if \"gcm\" in df.columns:\n            df = df.merge(self.climate.gmst, on=[\"year\", \"gcm\", \"rcp\"])\n\n        # removing illegal combinations from estimation\n        if any([i in df.ssp.unique() for i in [\"SSP1\", \"SSP5\"]]):\n            self.logger.info(\"Dropping illegal model combinations.\")\n            for var in [i for i in df.columns if i in [\"anomaly\", \"gmsl\"]]:\n                df.loc[\n                    ((df.ssp == \"SSP1\") &amp; (df.rcp == \"rcp85\"))\n                    | ((df.ssp == \"SSP5\") &amp; (df.rcp == \"rcp45\")),\n                    var,\n                ] = np.nan\n\n        # agriculture lacks ACCESS0-1/rcp85 combo\n        if \"agriculture\" in self.sector:\n            self.logger.info(\"Dropping illegal model combinations for agriculture.\")\n            df.loc[(df.gcm == \"ACCESS1-0\") &amp; (df.rcp == \"rcp85\"), \"anomaly\"] = np.nan\n\n        return df\n\n    def damage_function_calculation(self, damage_function_points, global_consumption):\n        \"\"\"The damage function model fit may be : (1) ssp specific, (2) ssp-model specific, (3) unique across ssp-model.\n        This depends on the type of discounting. In each case the input data passed to the fitting functions and the formatting of the returned\n        output is different because dimensions are different. This function handles this and returns the model fit.\n\n        Returns\n        ------\n        dict with two xr.Datasets, 'params' (model fit) and 'preds' (predictions from model fit), with dimensions depending\n        on self.discounting_type.\n        \"\"\"\n\n        yrs = range(self.climate.pulse_year, self.ext_subset_end_year + 1)\n\n        params_list, preds_list = [], []\n\n        if self.discounting_type == \"constant_model_collapsed\":\n            for ssp in damage_function_points[\"ssp\"].unique():\n                # Subset dataframe to specific SSP\n                fit_subset = damage_function_points[\n                    damage_function_points[\"ssp\"] == ssp\n                ]\n\n                global_c_subset = global_consumption.sel({\"ssp\": ssp})\n                # Fit damage function curves using the data subset\n                damage_function = model_outputs(\n                    damage_function=fit_subset,\n                    formula=self.formula,\n                    type_estimation=self.fit_type,\n                    global_c=global_c_subset,\n                    extrapolation_type=self.ext_method,\n                    quantiles=self.quantreg_quantiles,\n                    year_range=yrs,\n                    year_start_pred=self.ext_subset_end_year + 1,\n                )\n\n                # Add variables\n                params = damage_function[\"parameters\"].expand_dims(\n                    dict(\n                        discount_type=[self.discounting_type],\n                        ssp=[ssp],\n                        model=[str(list(self.gdp.model.values))],\n                    )\n                )\n\n                preds = damage_function[\"preds\"].expand_dims(\n                    dict(\n                        discount_type=[self.discounting_type],\n                        ssp=[ssp],\n                        model=[str(list(self.gdp.model.values))],\n                    )\n                )\n\n                params_list.append(params)\n                preds_list.append(preds)\n\n        elif (self.discounting_type == \"constant\") or (\n            \"ramsey\" in self.discounting_type\n        ):\n            for ssp, model in list(\n                product(\n                    damage_function_points.ssp.unique(),\n                    damage_function_points.model.unique(),\n                )\n            ):\n                # Subset dataframe to specific SSP-IAM combination.\n                fit_subset = damage_function_points[\n                    (damage_function_points[\"ssp\"] == ssp)\n                    &amp; (damage_function_points[\"model\"] == model)\n                ]\n\n                global_c_subset = global_consumption.sel({\"ssp\": ssp, \"model\": model})\n\n                # Fit damage function curves using the data subset\n                damage_function = model_outputs(\n                    damage_function=fit_subset,\n                    formula=self.formula,\n                    type_estimation=self.fit_type,\n                    global_c=global_c_subset,\n                    extrapolation_type=self.ext_method,\n                    quantiles=self.quantreg_quantiles,\n                    year_range=yrs,\n                    year_start_pred=self.ext_subset_end_year + 1,\n                )\n\n                # Add variables\n                params = damage_function[\"parameters\"].expand_dims(\n                    dict(\n                        discount_type=[self.discounting_type], ssp=[ssp], model=[model]\n                    )\n                )\n\n                preds = damage_function[\"preds\"].expand_dims(\n                    dict(\n                        discount_type=[self.discounting_type], ssp=[ssp], model=[model]\n                    )\n                )\n\n                params_list.append(params)\n                preds_list.append(preds)\n\n        elif \"gwr\" in self.discounting_type:\n            # Fit damage function across all SSP-IAM combinations, as expected\n            # from the Weitzman-Ramsey discounting\n            fit_subset = damage_function_points\n\n            # Fit damage function curves using the data subset\n            damage_function = model_outputs(\n                damage_function=fit_subset,\n                type_estimation=self.fit_type,\n                formula=self.formula,\n                global_c=global_consumption,\n                extrapolation_type=self.ext_method,\n                quantiles=self.quantreg_quantiles,\n                year_range=yrs,\n                year_start_pred=self.ext_subset_end_year + 1,\n            )\n\n            # Add variables\n            params = damage_function[\"parameters\"].expand_dims(\n                dict(\n                    discount_type=[self.discounting_type],\n                    ssp=[str(list(self.gdp.ssp.values))],\n                    model=[str(list(self.gdp.model.values))],\n                )\n            )\n\n            preds = damage_function[\"preds\"].expand_dims(\n                dict(\n                    discount_type=[self.discounting_type],\n                    ssp=[str(list(self.gdp.ssp.values))],\n                    model=[str(list(self.gdp.model.values))],\n                )\n            )\n\n            params_list.append(params)\n            preds_list.append(preds)\n\n        return dict(\n            params=xr.combine_by_coords(params_list),\n            preds=xr.combine_by_coords(preds_list),\n        )\n\n    @cachedproperty\n    def damage_function(self):\n        \"\"\"Calls damage function calculation method.\n\n        This function calls the damage function calculation in\n        model_outputs(). It calculates a damage function for each\n        passed `scenario_dimension` based on subsets of\n        self.damage_function_points and extrapolates this function\n        using the specified method for all years post-end_ext_subset_year.\n\n        Returns\n        -------\n        dict\n            dict['params'] is a dataframe of betas for each year\n            dict['preds'] is a dataframe of predicted y hat for each\n                year and anomaly\n        \"\"\"\n        if self.scenario_dimensions is None:\n            # this only occurs for global discounting\n            # with a single scenario passed\n            damage_function = self.damage_function_calculation(\n                damage_function_points=self.damage_function_points,\n                global_consumption=self.global_consumption,\n            )\n\n        else:\n            # cycle through the different combinations of the scenario dims\n            subset = self.damage_function_points.groupby(self.scenario_dimensions)\n            damage_function, dict_list = {}, []\n\n            for name, dt in subset:\n                # turn single-dim into a tuple to make indexing easier later\n                if len(self.scenario_dimensions) == 1:\n                    name = tuple([name])\n\n                df = self.damage_function_calculation(\n                    damage_function_points=dt,\n                    global_consumption=self.global_consumption,\n                )\n                # assigning dimensions to each dataArray in the dictionary\n                for key in df.keys():\n                    df[key] = df[key].expand_dims(\n                        {\n                            var: [val]\n                            for var, val in zip(self.scenario_dimensions, list(name))\n                        }\n                    )\n                dict_list.append(df)\n\n            # concatenate different scenarios into one big dataArray\n            damage_function[\"params\"] = xr.combine_by_coords(\n                [x[\"params\"] for x in dict_list]\n            )\n\n            damage_function[\"preds\"] = xr.combine_by_coords(\n                [x[\"preds\"] for x in dict_list]\n            )\n\n        return damage_function\n\n    @property\n    @save(name=\"damage_function_coefficients\")\n    def damage_function_coefficients(self) -&gt; xr.Dataset:\n        \"\"\"\n        Load damage function coefficients if the coefficients are provided by the user.\n        Otherwise, compute them.\n        \"\"\"\n        if self.damage_function_path is not None:\n            return xr.open_dataset(\n                f\"{self.damage_function_path}/{self.NAME}_{self.discounting_type}_eta{self.eta}_rho{self.rho}_damage_function_coefficients.nc4\"\n            )\n        else:\n            return self.damage_function[\"params\"]\n\n    @property\n    @save(name=\"damage_function_fit\")\n    def damage_function_fit(self) -&gt; xr.Dataset:\n        \"\"\"\n        Load fitted damage function if the fit is provided by the user.\n        Otherwise, compute them.\n        \"\"\"\n        if self.damage_function_path is not None:\n            return xr.open_dataset(\n                f\"{self.damage_function_path}/{self.NAME}_{self.discounting_type}_eta{self.eta}_rho{self.rho}_damage_function_fit.nc4\"\n            )\n        else:\n            return self.damage_function[\"preds\"]\n\n    @property\n    def median_params_marginal_damages(self):\n        \"\"\"Calculate marginal damages due to a pulse using a FAIR simulation\n        calculated with the median climate parameters.\n        \"\"\"\n\n        fair_control = self.climate.fair_median_params_control\n        fair_pulse = self.climate.fair_median_params_pulse\n\n        if self.clip_gmsl:\n            fair_control[\"gmsl\"] = np.minimum(fair_control[\"gmsl\"], self.gmsl_max)\n            fair_pulse[\"gmsl\"] = np.minimum(fair_pulse[\"gmsl\"], self.gmsl_max)\n\n        damages_pulse = compute_damages(\n            fair_pulse,\n            betas=self.damage_function_coefficients,\n            formula=self.formula,\n        )\n\n        damages_no_pulse = compute_damages(\n            fair_control,\n            betas=self.damage_function_coefficients,\n            formula=self.formula,\n        )\n\n        median_params_marginal_damages = damages_pulse - damages_no_pulse\n\n        # collapse further if further collapsing dimensions are provided\n        if len(self.fair_dims) &gt; 1:\n            median_params_marginal_damages = median_params_marginal_damages.mean(\n                [i for i in self.fair_dims if i in median_params_marginal_damages.dims]\n            )\n\n        # add a Weitzman parameter dimension so this dataset can be concatenated\n        # with the other FAIR aggregation results\n        median_params_marginal_damages = median_params_marginal_damages.expand_dims(\n            {\"weitzman_parameter\": [str(i) for i in self.weitzman_parameter]}\n        )\n\n        return median_params_marginal_damages\n\n    @abstractmethod\n    def global_consumption_calculation(self, disc_type):\n        \"\"\"Calculation of global consumption without climate change\n\n        Returns\n        -------\n            xr.DataArray\n        \"\"\"\n\n    def global_consumption_per_capita(self, disc_type):\n        \"\"\"Global consumption per capita\n\n        Returns\n        -------\n            xr.DataArray\n        \"\"\"\n\n        # Calculate global consumption per capita\n        array_pc = self.global_consumption_calculation(\n            disc_type\n        ) / self.collapsed_pop.sum(\"region\")\n\n        if self.NAME == \"equity\":\n            # equity recipe's growth is capped to\n            # risk aversion recipe's growth rates\n            extrapolated = extrapolate(\n                xr_array=array_pc,\n                start_year=self.ext_subset_start_year,\n                end_year=self.ext_subset_end_year,\n                interp_year=self.ext_end_year,\n                method=\"growth_constant\",\n                cap=self.risk_aversion_growth_rates(),\n            )\n\n        else:\n            extrapolated = extrapolate(\n                xr_array=array_pc,\n                start_year=self.ext_subset_start_year,\n                end_year=self.ext_subset_end_year,\n                interp_year=self.ext_end_year,\n                method=\"growth_constant\",\n            )\n\n        complete_array = xr.concat([array_pc, extrapolated], dim=\"year\")\n\n        return complete_array\n\n    @cachedproperty\n    @save(name=\"global_consumption\")\n    def global_consumption(self):\n        \"\"\"Global consumption without climate change\"\"\"\n\n        # rff simulation means that GDP already exists out to 2300\n        if 2300 in self.gdp.year:\n            self.logger.debug(\"Global consumption found up to 2300.\")\n            global_cons = self.gdp.sum(\"region\").rename(\"global_consumption\")\n        else:\n            self.logger.info(\"Extrapolating global consumption.\")\n\n            # holding population constant\n            # from 2100 to 2300 with 2099 values\n            pop = self.collapsed_pop.sum(\"region\")\n            pop = pop.reindex(\n                year=range(pop.year.min().values, self.ext_end_year + 1),\n                method=\"ffill\",\n            )\n\n            # Calculate global consumption back by\n            global_cons = (\n                self.global_consumption_per_capita(self.discounting_type) * pop\n            )\n\n        # Add dimension\n        # @TODO: remove this line altogether\n        global_cons = global_cons.expand_dims(\n            {\"discount_type\": [self.discounting_type]}\n        )\n\n        return global_cons\n\n    def weitzman_min(self, no_cc_consumption, cc_consumption, parameter):\n        \"\"\"\n        Implements bottom coding that fixes marginal utility below a threshold\n        to the marginal utility at that threshold. The threshold is the Weitzman\n        parameter.\n\n        Parameters\n        ----------\n        no_cc_consumption: xr.DataArray\n            Consumption array of which the share will be used to calculate\n            the absolute Weitzman parameter, only if parameter &lt;= 1.\n        consumption: xr.DataArray\n            Consumption array to be bottom-coded.\n        parameter: float\n            A positive number representing the Weitzman parameter, below which marginal utility will be\n            top coded; ie., 0.01 implies marginal utility is top coded to the\n            value of marginal utility at 1% of no-climate change global consumption.\n            If parameter &gt; 1, it is assumed to be an absolute value.\n            If parameter &lt;= 1, it is assumed to be a share of future global consumption\n            (without climate change).\n\n        Returns\n        -------\n            xr.Dataset\n\n        \"\"\"\n        # if parameter is share of consumption,\n        # multiply by no-climate-change consumption\n        if parameter &lt;= 1:\n            parameter = parameter * no_cc_consumption\n\n        if self.eta == 1:\n            w_utility = np.log(parameter)\n            bottom_utility = np.power(parameter, -1) * (parameter - cc_consumption)\n            bottom_coded_cons = np.exp(w_utility - bottom_utility)\n\n            clipped_cons = xr.where(\n                cc_consumption &gt; parameter, cc_consumption, bottom_coded_cons\n            )\n        else:\n            w_utility = np.power(parameter, (1 - self.eta)) / (1 - self.eta)\n            bottom_utility = np.power(parameter, -self.eta) * (\n                parameter - cc_consumption\n            )\n            bottom_coded_cons = power(\n                ((1 - self.eta) * (w_utility - bottom_utility)), (1 / (1 - self.eta))\n            )\n\n            clipped_cons = xr.where(\n                cc_consumption &gt; parameter, cc_consumption, bottom_coded_cons\n            )\n\n        return clipped_cons\n\n    @property\n    def gmsl_max(self):\n        \"\"\"\n        This function finds the GMSL value at which the damage function\n        reaches its local maximum along the GMSL dimension.\n\n        Returns\n        -------\n        xr.DataArray\n            the array of GMSL values at which the local maximum is located, for all\n            years, ssps, models, and if applicable, values of GMST\n        \"\"\"\n\n        if self.formula in [\n            \"damages ~ -1 + anomaly + np.power(anomaly, 2) + gmsl + np.power(gmsl, 2)\",\n            \"damages ~ -1 + gmsl + np.power(gmsl, 2)\",\n        ]:\n            gmsl_max = -self.damage_function_coefficients[\"gmsl\"] / (\n                2 * self.damage_function_coefficients[\"np.power(gmsl, 2)\"]\n            )\n\n            # if the damage function curves up, there is no local max.\n            # Thus there will be no linearization.\n            gmsl_max = xr.where(\n                self.damage_function_coefficients[\"np.power(gmsl, 2)\"] &gt; 0,\n                np.inf,\n                gmsl_max,\n            )\n\n            # confirm that all max values are positive, which is expected\n            # for our damage functions\n            assert len(gmsl_max.where(gmsl_max &lt; 0, drop=True)) == 0\n\n        else:\n            raise NotImplementedError(\n                f\"The first derivative w.r.t. gmsl for {self.formula} has not been implemented in the menu.\"\n            )\n\n        return gmsl_max\n\n    @cachedproperty\n    @save(name=\"global_consumption_no_pulse\")\n    def global_consumption_no_pulse(self):\n        \"\"\"Global consumption under FAIR control scenario.\"\"\"\n\n        fair_control = self.climate.fair_control\n\n        if self.clip_gmsl:\n            fair_control[\"gmsl\"] = np.minimum(fair_control[\"gmsl\"], self.gmsl_max)\n\n        damages = compute_damages(\n            fair_control,\n            betas=self.damage_function_coefficients,\n            formula=self.formula,\n        )\n\n        cc_cons = self.global_consumption - damages\n\n        gc_no_pulse = []\n        for wp in self.weitzman_parameter:\n            gc = self.weitzman_min(\n                no_cc_consumption=self.global_consumption,\n                cc_consumption=cc_cons,\n                parameter=wp,\n            )\n            gc = gc.assign_coords({\"weitzman_parameter\": str(wp)})\n\n            gc_no_pulse.append(gc)\n\n        return xr.concat(gc_no_pulse, dim=\"weitzman_parameter\")\n\n    @cachedproperty\n    @save(name=\"global_consumption_pulse\")\n    def global_consumption_pulse(self):\n        \"\"\"Global consumption under FAIR pulse scenario.\"\"\"\n\n        fair_pulse = self.climate.fair_pulse\n\n        if self.clip_gmsl:\n            fair_pulse[\"gmsl\"] = np.minimum(fair_pulse[\"gmsl\"], self.gmsl_max)\n\n        damages = compute_damages(\n            fair_pulse,\n            betas=self.damage_function_coefficients,\n            formula=self.formula,\n        )\n\n        cc_cons = self.global_consumption - damages\n        gc_no_pulse = []\n        for wp in self.weitzman_parameter:\n            gc = self.weitzman_min(\n                no_cc_consumption=self.global_consumption,\n                cc_consumption=cc_cons,\n                parameter=wp,\n            )\n            gc = gc.assign_coords({\"weitzman_parameter\": str(wp)})\n            gc_no_pulse.append(gc)\n        return xr.concat(gc_no_pulse, dim=\"weitzman_parameter\")\n\n    @property\n    @save(name=\"ce_fair_pulse\")\n    def ce_fair_pulse(self):\n        \"\"\"Certainty equivalent of global consumption under FAIR pulse scenario\"\"\"\n        ce_array = self.ce(self.global_consumption_pulse, dims=self.fair_dims)\n\n        return ce_array.rename(\"ce_fair_pulse\")\n\n    @property\n    @save(name=\"ce_fair_no_pulse\")\n    def ce_fair_no_pulse(self):\n        \"\"\"Certainty equivalent of global consumption under FAIR control scenario\"\"\"\n\n        ce_array = self.ce(self.global_consumption_no_pulse, dims=self.fair_dims)\n\n        return ce_array.rename(\"ce_fair_no_pulse\")\n\n    @cachedproperty\n    @save(name=\"marginal_damages\")\n    def marginal_damages(self):\n        \"\"\"Marginal damages due to additional pulse\"\"\"\n\n        marginal_damages = []\n\n        for agg in [i for i in self.fair_aggregation if i != \"median\"]:\n            if agg == \"ce\":\n                md = self.ce_fair_no_pulse - self.ce_fair_pulse\n            elif agg in [\"mean\", \"gwr_mean\"]:\n                md = self.global_consumption_no_pulse.mean(\n                    self.fair_dims\n                ) - self.global_consumption_pulse.mean(self.fair_dims)\n            elif agg == \"median_params\":\n                md = self.median_params_marginal_damages\n            else:\n                raise NotImplementedError(\n                    f\"{agg} is not available. Enter list including\"\n                    '[\"ce\", \"fair\", \"median\", \"median_params\"]'\n                )\n\n            md = md.assign_coords({\"fair_aggregation\": agg}).expand_dims(\n                \"fair_aggregation\"\n            )\n\n            # convert to the marginal damages from a single tonne\n            md = md * self.climate.conversion\n            marginal_damages.append(md)\n\n        return xr.concat(marginal_damages, dim=\"fair_aggregation\")\n\n    @cachedproperty\n    @save(name=\"scc\")\n    def calculate_scc(self):\n        \"\"\"Calculate range of FAIR-aggregated SCCs\"\"\"\n\n        discounted_damages = self.discounted_damages(\n            damages=self.marginal_damages, discrate=self.discounting_type\n        )\n        discounted_damages = discounted_damages.sum(dim=\"year\").rename(\"scc\")\n\n        if \"median\" in self.fair_aggregation:\n            median = self.uncollapsed_sccs.median(self.fair_dims)\n            median[\"fair_aggregation\"] = [\"median\"]\n            discounted_damages = xr.merge([median.rename(\"scc\"), discounted_damages])\n\n        return discounted_damages\n\n    def discounted_damages(self, damages, discrate):\n        \"\"\"Discount marginal damages. Distinguishes between constant discount rates method and non-constant discount rates.\n\n        Parameters\n        ----------\n        damages : xr.DataArray or xr.Dataset\n            Array of damages with a`discount_type` dimension to subset the damages.\n        discrate : str\n             Discounting type. Be aware that the constant rates are class-wide defined. If this str is either 'constant' or 'constant_model_collapsed', the predetermined constant discount rates are used, otherwise, the stream of (non-constant) discount factors from self.stream_discount_factors() is used.\n\n        Returns\n        -------\n            xr.Dataset\n        \"\"\"\n\n        if discrate in [\"constant\", \"constant_model_collapsed\", \"constant_gwr\"]:\n            if self.discrete_discounting:\n                discrate_damages = [\n                    damages * (1 / (1 + r)) ** (damages.year - self.climate.pulse_year)\n                    for r in self.CONST_DISC_RATES\n                ]\n            else:\n                discrate_damages = [\n                    damages * np.exp(-r * (damages.year - self.climate.pulse_year))\n                    for r in self.CONST_DISC_RATES\n                ]\n\n            pvd_damages = xr.concat(\n                discrate_damages,\n                dim=pd.Index(self.CONST_DISC_RATES, name=\"discrate\"),\n            )\n        else:\n            factors = self.calculate_stream_discount_factors(\n                discounting_type=self.discounting_type,\n                fair_aggregation=damages.fair_aggregation,\n            )\n            pvd_damages = factors * damages\n\n        return pvd_damages\n\n    @cachedproperty\n    @save(name=\"uncollapsed_sccs\")\n    def uncollapsed_sccs(self):\n        \"\"\"Calculate full distribution of SCCs without FAIR aggregation\"\"\"\n\n        md = (\n            self.global_consumption_no_pulse - self.global_consumption_pulse\n        )  # this is for full uncertainty\n\n        # convert to the marginal damages from a single pulse\n        md = md * self.climate.conversion\n\n        md = md.expand_dims({\"fair_aggregation\": [\"uncollapsed\"]})\n\n        sccs = self.discounted_damages(\n            damages=md,\n            discrate=self.discounting_type,\n        ).sum(dim=\"year\")\n\n        return sccs\n\n    @cachedproperty\n    @save(name=\"stat_uncertainty_iqr\")\n    def stat_uncertainty_iqr(self):\n        \"\"\"Calculate the distribution of quantile-weighted SCCs produced from\n        quantile regressions that have already been collapsed across other dimensions to give statistical-only uncertainty.\n        \"\"\"\n        return quantile_weight_quantilereg(\n            self.calculate_scc,\n            fair_dims=self.fair_dims,\n            quantiles=self.full_uncertainty_quantiles,\n        )\n\n    @cachedproperty\n    @save(name=\"full_uncertainty_iqr\")\n    def full_uncertainty_iqr(self):\n        \"\"\"Calculate the distribution of quantile-weighted SCCs produced from\n        quantile regressions.\n        \"\"\"\n        return quantile_weight_quantilereg(\n            self.uncollapsed_sccs,\n            fair_dims=self.fair_dims,\n            quantiles=self.full_uncertainty_quantiles,\n        )\n\n    def calculate_discount_factors(self, cons_pc):\n        \"\"\"Calculates the stream of discount factors based on the Euler equation that defines an optimal\n        intertemporal consumption allocation. Rearranging that equation shows that an outcome that will occur at the\n        period t will be converted into today's, period 0 value, the following way :\n\n        Discrete discounting: discount_factor_t = [ 1/(1+rho)^t ] * [ U'(C(t)) / U'(C(0)) ]\n\n        where rho is the pure rate of time preference, U() is the utility function, U'() the first derivative,\n        C(0) and C(t) today's and the future consumption respectively. The higher rho, the higher the importance\n        of the present relative to the future so the lower the discount factor, and, if the utility function is concave,\n        the higher the growth of consumption, the lower the importance of the future consumption relative to today, and\n        therefore again the lower the discount factor.\n\n        Using a CRRA utility function and plugging the first derivative :\n\n        discount_factor_t = [ 1/(1+rho)^t ] * [ C(0)^eta / C(t)^eta ]\n\n        eta represents the degree of concavity of the utility function.\n\n        With continuous discounting,\n        discount_factor_t = Product_1^t [e^-(rho + eta * g),\n            where g = ln(C(t)/C(t-1))\n\n        rearranging yields rho_continuous = e^rho_discrete - 1\n\n        Parameters\n        ----------\n        cons_pc : array\n            Array of per capita consumption from pulse year to end of time period.\n\n        Returns\n        -------\n        `xarray.DataArray` with discount factors computed following the last equation in the above description.\n        \"\"\"\n\n        # subset to pulse year onward\n        cons_pc = cons_pc.sel(year=slice(self.climate.pulse_year, self.ext_end_year))\n\n        # calculate the time preference component of the discount factors for each period.\n        if self.discrete_discounting:\n            # plug the unique rho in an array\n            rhos = xr.DataArray(self.rho, coords=[cons_pc.year])\n        else:\n            # plug the unique rho in an array, and compute e^rho - 1\n            rhos = np.expm1(xr.DataArray(self.rho, coords=[cons_pc.year]))\n\n        stream_rhos = np.divide(\n            1, np.multiply.accumulate((rhos.values + 1), rhos.dims.index(\"year\"))\n        )\n\n        # calculate the marginal utility component of the discount factors for each period.\n        ratio = cons_pc.sel(year=self.climate.pulse_year) ** (self.eta) / cons_pc ** (\n            self.eta\n        )\n\n        # the discount factor is the product of the two components.\n        factors = stream_rhos * ratio\n\n        return factors\n\n    def calculate_stream_discount_factors(self, discounting_type, fair_aggregation):\n        \"\"\"Stream of discount factors\n        Returns specified Ramsey or Weitzman-Ramsey discount factors.\n\n        Parameters\n        ----------\n\n        discounting_type : str\n            Type of discounting to implement. Typically,\n            self.discounting_type is passed. However, for local Euler rates,\n            this changes depending on the option.\n\n        Returns\n        -------\n        `xarray.DataArray`\n            Discount rates indexed by year and (if Ramsey) SSP/model\n        \"\"\"\n\n        # holding population constant\n        # from 2100 to 2300 with 2099 values\n        full_pop = self.collapsed_pop.sum(\"region\")\n        full_pop = full_pop.reindex(\n            year=range(full_pop.year.min().values, self.ext_end_year + 1),\n            method=\"ffill\",\n        )\n\n        # for aggregations other than uncollapsed,\n        # we need to collapse over pop dimensions\n        if len(self.fair_dims) &gt; 1:\n            pop = full_pop.mean([i for i in self.fair_dims if i in full_pop.dims])\n        else:\n            pop = full_pop\n\n        # for gwr_gwr, we need to calculate regular naive_ramsey rates, get\n        # discount factors, and then average them at the end\n        if discounting_type == \"gwr_gwr\":\n            array = self.global_consumption_per_capita(\"naive_ramsey\")\n\n            # average discount factors and expand dims to match Euler\n            discount_factors = (\n                self.calculate_discount_factors(array)\n                .mean(dim=[\"ssp\", \"model\"])\n                .expand_dims({\"fair_aggregation\": fair_aggregation})\n            )\n\n        # naive options are calculated from the no-climate-change consumption growth rate\n        elif \"naive\" in discounting_type:\n            array = self.global_consumption_per_capita(self.discounting_type)\n\n            # expand dims to match Euler\n            discount_factors = self.calculate_discount_factors(array).expand_dims(\n                {\"fair_aggregation\": fair_aggregation}\n            )\n\n        elif \"euler\" in discounting_type:\n            discount_factors = []\n            for agg in [i for i in fair_aggregation if i != \"median\"]:\n                if agg == \"ce\":\n                    factors = self.calculate_discount_factors(\n                        self.ce_fair_no_pulse / pop\n                    )\n                elif agg == \"mean\":\n                    factors = self.calculate_discount_factors(\n                        self.global_consumption_no_pulse.mean(self.fair_dims) / pop\n                    )\n                elif agg == \"gwr_mean\":\n                    factors = self.calculate_discount_factors(\n                        self.global_consumption_no_pulse / full_pop\n                    ).mean(self.fair_dims)\n                elif agg == \"median_params\":\n                    median_params_damages = compute_damages(\n                        self.climate.fair_median_params_control,\n                        betas=self.damage_function_coefficients,\n                        formula=self.formula,\n                    )\n\n                    median_params_consumption = (\n                        self.global_consumption - median_params_damages\n                    ).expand_dims(\n                        {\n                            \"weitzman_parameter\": [\n                                str(i) for i in self.weitzman_parameter\n                            ]\n                        }\n                    )\n\n                    if len(self.fair_dims) &gt; 1:\n                        median_params_consumption = median_params_consumption.mean(\n                            [\n                                i\n                                for i in self.fair_dims\n                                if i in median_params_consumption.dims\n                            ]\n                        )\n\n                    factors = self.calculate_discount_factors(\n                        median_params_consumption / pop\n                    )\n                elif agg == \"uncollapsed\":\n                    factors = self.calculate_discount_factors(\n                        self.global_consumption_no_pulse / full_pop\n                    )\n\n                factors = factors.assign_coords({\"fair_aggregation\": agg})\n                discount_factors.append(factors)\n\n            discount_factors = xr.concat(discount_factors, dim=\"fair_aggregation\")\n\n        return discount_factors\n\n    @cachedproperty\n    @save(\"discount_factors\")\n    def stream_discount_factors(self):\n        return self.calculate_stream_discount_factors(\n            discounting_type=self.discounting_type,\n            fair_aggregation=self.fair_aggregation,\n        )\n\n    @cachedproperty\n    @save(\"uncollapsed_discount_factors\")\n    def uncollapsed_discount_factors(self):\n        pop = self.collapsed_pop.sum(\"region\")\n        pop = pop.reindex(\n            year=range(pop.year.min().values, self.ext_end_year + 1),\n            method=\"ffill\",\n        )\n        f = self.calculate_discount_factors(\n            self.global_consumption_no_pulse / pop\n        ).to_dataset(name=\"discount_factor\")\n        for var in f.variables:\n            f[var].encoding.clear()\n\n        return f\n\n    @cachedproperty\n    @save(\"uncollapsed_marginal_damages\")\n    def uncollapsed_marginal_damages(self):\n        md = (\n            (\n                (self.global_consumption_no_pulse - self.global_consumption_pulse)\n                * self.climate.conversion\n            )\n            .rename(\"marginal_damages\")\n            .to_dataset()\n        )\n\n        for var in md.variables:\n            md[var].encoding.clear()\n\n        return md\n\n    def ce(self, obj, dims):\n        \"\"\"Rechunk data appropriately and apply the certainty equivalence\n        calculation. This is done in a loop to avoid memory crashes.\n        Not that data MUST be chunked, otherwise Dask will take a CE over each\n        chunk and sum the result.\n\n        *** IMPORTANT NOTE ***\n        This wrapper function CANNOT execute with weights as it uses a map_blocks\n        function which is unable to determine how to match weight dimensions with\n        its chunk. If you must weight, `c_equivalence` function must be used directly\n        on the data.\n        \"\"\"\n        for dim in dims:\n            obj = obj.chunk({dim: len(obj[dim])})\n            obj = obj.map_blocks(c_equivalence, kwargs=dict(dims=dim, eta=self.eta))\n        return obj\n</code></pre>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.ce_cc","title":"dscim.menu.main_recipe.MainRecipe.ce_cc  <code>property</code>","text":"<pre><code>ce_cc\n</code></pre> <p>Certainty equivalent of consumption with climate change damages</p>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.ce_fair_no_pulse","title":"dscim.menu.main_recipe.MainRecipe.ce_fair_no_pulse  <code>property</code>","text":"<pre><code>ce_fair_no_pulse\n</code></pre> <p>Certainty equivalent of global consumption under FAIR control scenario</p>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.ce_fair_pulse","title":"dscim.menu.main_recipe.MainRecipe.ce_fair_pulse  <code>property</code>","text":"<pre><code>ce_fair_pulse\n</code></pre> <p>Certainty equivalent of global consumption under FAIR pulse scenario</p>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.ce_no_cc","title":"dscim.menu.main_recipe.MainRecipe.ce_no_cc  <code>property</code>","text":"<pre><code>ce_no_cc\n</code></pre> <p>Certainty equivalent of consumption without climate change damages</p>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.damage_function_coefficients","title":"dscim.menu.main_recipe.MainRecipe.damage_function_coefficients  <code>property</code>","text":"<pre><code>damage_function_coefficients\n</code></pre> <p>Load damage function coefficients if the coefficients are provided by the user. Otherwise, compute them.</p>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.damage_function_fit","title":"dscim.menu.main_recipe.MainRecipe.damage_function_fit  <code>property</code>","text":"<pre><code>damage_function_fit\n</code></pre> <p>Load fitted damage function if the fit is provided by the user. Otherwise, compute them.</p>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.gmsl_max","title":"dscim.menu.main_recipe.MainRecipe.gmsl_max  <code>property</code>","text":"<pre><code>gmsl_max\n</code></pre> <p>This function finds the GMSL value at which the damage function reaches its local maximum along the GMSL dimension.</p> <p>Returns:</p> Type Description <code>DataArray</code> <p>the array of GMSL values at which the local maximum is located, for all years, ssps, models, and if applicable, values of GMST</p>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.median_params_marginal_damages","title":"dscim.menu.main_recipe.MainRecipe.median_params_marginal_damages  <code>property</code>","text":"<pre><code>median_params_marginal_damages\n</code></pre> <p>Calculate marginal damages due to a pulse using a FAIR simulation calculated with the median climate parameters.</p>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.output_attrs","title":"dscim.menu.main_recipe.MainRecipe.output_attrs  <code>property</code>","text":"<pre><code>output_attrs\n</code></pre> <p>Return dict with class attributes for output metadata</p> <p>Returns:</p> Type Description <code>    A dict Class metadata</code>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.calculate_discount_factors","title":"dscim.menu.main_recipe.MainRecipe.calculate_discount_factors","text":"<pre><code>calculate_discount_factors(cons_pc)\n</code></pre> <p>Calculates the stream of discount factors based on the Euler equation that defines an optimal intertemporal consumption allocation. Rearranging that equation shows that an outcome that will occur at the period t will be converted into today's, period 0 value, the following way :</p> <p>Discrete discounting: discount_factor_t = [ 1/(1+rho)^t ] * [ U'(C(t)) / U'(C(0)) ]</p> <p>where rho is the pure rate of time preference, U() is the utility function, U'() the first derivative, C(0) and C(t) today's and the future consumption respectively. The higher rho, the higher the importance of the present relative to the future so the lower the discount factor, and, if the utility function is concave, the higher the growth of consumption, the lower the importance of the future consumption relative to today, and therefore again the lower the discount factor.</p> <p>Using a CRRA utility function and plugging the first derivative :</p> <p>discount_factor_t = [ 1/(1+rho)^t ] * [ C(0)^eta / C(t)^eta ]</p> <p>eta represents the degree of concavity of the utility function.</p> <p>With continuous discounting, discount_factor_t = Product_1^t [e^-(rho + eta * g),     where g = ln(C(t)/C(t-1))</p> <p>rearranging yields rho_continuous = e^rho_discrete - 1</p> <p>Parameters:</p> Name Type Description Default <code>cons_pc</code> <code>array</code> <p>Array of per capita consumption from pulse year to end of time period.</p> required <p>Returns:</p> Type Description <code>`xarray.DataArray` with discount factors computed following the last equation in the above description.</code> Source code in <code>src/dscim/menu/main_recipe.py</code> <pre><code>def calculate_discount_factors(self, cons_pc):\n    \"\"\"Calculates the stream of discount factors based on the Euler equation that defines an optimal\n    intertemporal consumption allocation. Rearranging that equation shows that an outcome that will occur at the\n    period t will be converted into today's, period 0 value, the following way :\n\n    Discrete discounting: discount_factor_t = [ 1/(1+rho)^t ] * [ U'(C(t)) / U'(C(0)) ]\n\n    where rho is the pure rate of time preference, U() is the utility function, U'() the first derivative,\n    C(0) and C(t) today's and the future consumption respectively. The higher rho, the higher the importance\n    of the present relative to the future so the lower the discount factor, and, if the utility function is concave,\n    the higher the growth of consumption, the lower the importance of the future consumption relative to today, and\n    therefore again the lower the discount factor.\n\n    Using a CRRA utility function and plugging the first derivative :\n\n    discount_factor_t = [ 1/(1+rho)^t ] * [ C(0)^eta / C(t)^eta ]\n\n    eta represents the degree of concavity of the utility function.\n\n    With continuous discounting,\n    discount_factor_t = Product_1^t [e^-(rho + eta * g),\n        where g = ln(C(t)/C(t-1))\n\n    rearranging yields rho_continuous = e^rho_discrete - 1\n\n    Parameters\n    ----------\n    cons_pc : array\n        Array of per capita consumption from pulse year to end of time period.\n\n    Returns\n    -------\n    `xarray.DataArray` with discount factors computed following the last equation in the above description.\n    \"\"\"\n\n    # subset to pulse year onward\n    cons_pc = cons_pc.sel(year=slice(self.climate.pulse_year, self.ext_end_year))\n\n    # calculate the time preference component of the discount factors for each period.\n    if self.discrete_discounting:\n        # plug the unique rho in an array\n        rhos = xr.DataArray(self.rho, coords=[cons_pc.year])\n    else:\n        # plug the unique rho in an array, and compute e^rho - 1\n        rhos = np.expm1(xr.DataArray(self.rho, coords=[cons_pc.year]))\n\n    stream_rhos = np.divide(\n        1, np.multiply.accumulate((rhos.values + 1), rhos.dims.index(\"year\"))\n    )\n\n    # calculate the marginal utility component of the discount factors for each period.\n    ratio = cons_pc.sel(year=self.climate.pulse_year) ** (self.eta) / cons_pc ** (\n        self.eta\n    )\n\n    # the discount factor is the product of the two components.\n    factors = stream_rhos * ratio\n\n    return factors\n</code></pre>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.calculate_scc","title":"dscim.menu.main_recipe.MainRecipe.calculate_scc","text":"<pre><code>calculate_scc()\n</code></pre> <p>Calculate range of FAIR-aggregated SCCs</p> Source code in <code>src/dscim/menu/main_recipe.py</code> <pre><code>@cachedproperty\n@save(name=\"scc\")\ndef calculate_scc(self):\n    \"\"\"Calculate range of FAIR-aggregated SCCs\"\"\"\n\n    discounted_damages = self.discounted_damages(\n        damages=self.marginal_damages, discrate=self.discounting_type\n    )\n    discounted_damages = discounted_damages.sum(dim=\"year\").rename(\"scc\")\n\n    if \"median\" in self.fair_aggregation:\n        median = self.uncollapsed_sccs.median(self.fair_dims)\n        median[\"fair_aggregation\"] = [\"median\"]\n        discounted_damages = xr.merge([median.rename(\"scc\"), discounted_damages])\n\n    return discounted_damages\n</code></pre>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.calculate_stream_discount_factors","title":"dscim.menu.main_recipe.MainRecipe.calculate_stream_discount_factors","text":"<pre><code>calculate_stream_discount_factors(discounting_type, fair_aggregation)\n</code></pre> <p>Stream of discount factors Returns specified Ramsey or Weitzman-Ramsey discount factors.</p> <p>Parameters:</p> Name Type Description Default <code>discounting_type</code> <code>str</code> <p>Type of discounting to implement. Typically, self.discounting_type is passed. However, for local Euler rates, this changes depending on the option.</p> required <p>Returns:</p> Type Description <code>`xarray.DataArray`</code> <p>Discount rates indexed by year and (if Ramsey) SSP/model</p> Source code in <code>src/dscim/menu/main_recipe.py</code> <pre><code>def calculate_stream_discount_factors(self, discounting_type, fair_aggregation):\n    \"\"\"Stream of discount factors\n    Returns specified Ramsey or Weitzman-Ramsey discount factors.\n\n    Parameters\n    ----------\n\n    discounting_type : str\n        Type of discounting to implement. Typically,\n        self.discounting_type is passed. However, for local Euler rates,\n        this changes depending on the option.\n\n    Returns\n    -------\n    `xarray.DataArray`\n        Discount rates indexed by year and (if Ramsey) SSP/model\n    \"\"\"\n\n    # holding population constant\n    # from 2100 to 2300 with 2099 values\n    full_pop = self.collapsed_pop.sum(\"region\")\n    full_pop = full_pop.reindex(\n        year=range(full_pop.year.min().values, self.ext_end_year + 1),\n        method=\"ffill\",\n    )\n\n    # for aggregations other than uncollapsed,\n    # we need to collapse over pop dimensions\n    if len(self.fair_dims) &gt; 1:\n        pop = full_pop.mean([i for i in self.fair_dims if i in full_pop.dims])\n    else:\n        pop = full_pop\n\n    # for gwr_gwr, we need to calculate regular naive_ramsey rates, get\n    # discount factors, and then average them at the end\n    if discounting_type == \"gwr_gwr\":\n        array = self.global_consumption_per_capita(\"naive_ramsey\")\n\n        # average discount factors and expand dims to match Euler\n        discount_factors = (\n            self.calculate_discount_factors(array)\n            .mean(dim=[\"ssp\", \"model\"])\n            .expand_dims({\"fair_aggregation\": fair_aggregation})\n        )\n\n    # naive options are calculated from the no-climate-change consumption growth rate\n    elif \"naive\" in discounting_type:\n        array = self.global_consumption_per_capita(self.discounting_type)\n\n        # expand dims to match Euler\n        discount_factors = self.calculate_discount_factors(array).expand_dims(\n            {\"fair_aggregation\": fair_aggregation}\n        )\n\n    elif \"euler\" in discounting_type:\n        discount_factors = []\n        for agg in [i for i in fair_aggregation if i != \"median\"]:\n            if agg == \"ce\":\n                factors = self.calculate_discount_factors(\n                    self.ce_fair_no_pulse / pop\n                )\n            elif agg == \"mean\":\n                factors = self.calculate_discount_factors(\n                    self.global_consumption_no_pulse.mean(self.fair_dims) / pop\n                )\n            elif agg == \"gwr_mean\":\n                factors = self.calculate_discount_factors(\n                    self.global_consumption_no_pulse / full_pop\n                ).mean(self.fair_dims)\n            elif agg == \"median_params\":\n                median_params_damages = compute_damages(\n                    self.climate.fair_median_params_control,\n                    betas=self.damage_function_coefficients,\n                    formula=self.formula,\n                )\n\n                median_params_consumption = (\n                    self.global_consumption - median_params_damages\n                ).expand_dims(\n                    {\n                        \"weitzman_parameter\": [\n                            str(i) for i in self.weitzman_parameter\n                        ]\n                    }\n                )\n\n                if len(self.fair_dims) &gt; 1:\n                    median_params_consumption = median_params_consumption.mean(\n                        [\n                            i\n                            for i in self.fair_dims\n                            if i in median_params_consumption.dims\n                        ]\n                    )\n\n                factors = self.calculate_discount_factors(\n                    median_params_consumption / pop\n                )\n            elif agg == \"uncollapsed\":\n                factors = self.calculate_discount_factors(\n                    self.global_consumption_no_pulse / full_pop\n                )\n\n            factors = factors.assign_coords({\"fair_aggregation\": agg})\n            discount_factors.append(factors)\n\n        discount_factors = xr.concat(discount_factors, dim=\"fair_aggregation\")\n\n    return discount_factors\n</code></pre>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.calculated_damages","title":"dscim.menu.main_recipe.MainRecipe.calculated_damages  <code>abstractmethod</code>","text":"<pre><code>calculated_damages()\n</code></pre> <p>Calculate damages (difference between CEs) for collapsing</p> Source code in <code>src/dscim/menu/main_recipe.py</code> <pre><code>@abstractmethod\ndef calculated_damages(self):\n    \"\"\"Calculate damages (difference between CEs) for collapsing\"\"\"\n</code></pre>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.ce","title":"dscim.menu.main_recipe.MainRecipe.ce","text":"<pre><code>ce(obj, dims)\n</code></pre> <p>Rechunk data appropriately and apply the certainty equivalence calculation. This is done in a loop to avoid memory crashes. Not that data MUST be chunked, otherwise Dask will take a CE over each chunk and sum the result.</p> <p>*** IMPORTANT NOTE *** This wrapper function CANNOT execute with weights as it uses a map_blocks function which is unable to determine how to match weight dimensions with its chunk. If you must weight, <code>c_equivalence</code> function must be used directly on the data.</p> Source code in <code>src/dscim/menu/main_recipe.py</code> <pre><code>def ce(self, obj, dims):\n    \"\"\"Rechunk data appropriately and apply the certainty equivalence\n    calculation. This is done in a loop to avoid memory crashes.\n    Not that data MUST be chunked, otherwise Dask will take a CE over each\n    chunk and sum the result.\n\n    *** IMPORTANT NOTE ***\n    This wrapper function CANNOT execute with weights as it uses a map_blocks\n    function which is unable to determine how to match weight dimensions with\n    its chunk. If you must weight, `c_equivalence` function must be used directly\n    on the data.\n    \"\"\"\n    for dim in dims:\n        obj = obj.chunk({dim: len(obj[dim])})\n        obj = obj.map_blocks(c_equivalence, kwargs=dict(dims=dim, eta=self.eta))\n    return obj\n</code></pre>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.ce_cc_calculation","title":"dscim.menu.main_recipe.MainRecipe.ce_cc_calculation  <code>abstractmethod</code>","text":"<pre><code>ce_cc_calculation()\n</code></pre> <p>Calculate CE damages depending on discount type</p> Source code in <code>src/dscim/menu/main_recipe.py</code> <pre><code>@abstractmethod\ndef ce_cc_calculation(self):\n    \"\"\"Calculate CE damages depending on discount type\"\"\"\n</code></pre>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.ce_no_cc_calculation","title":"dscim.menu.main_recipe.MainRecipe.ce_no_cc_calculation  <code>abstractmethod</code>","text":"<pre><code>ce_no_cc_calculation()\n</code></pre> <p>Calculate GDP CE depending on discount type.</p> Source code in <code>src/dscim/menu/main_recipe.py</code> <pre><code>@abstractmethod\ndef ce_no_cc_calculation(self):\n    \"\"\"Calculate GDP CE depending on discount type.\"\"\"\n</code></pre>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.collapsed_pop","title":"dscim.menu.main_recipe.MainRecipe.collapsed_pop","text":"<pre><code>collapsed_pop()\n</code></pre> <p>Collapse population according to discount type.</p> Source code in <code>src/dscim/menu/main_recipe.py</code> <pre><code>@cachedproperty\ndef collapsed_pop(self):\n    \"\"\"Collapse population according to discount type.\"\"\"\n    if (self.discounting_type == \"constant\") or (\"ramsey\" in self.discounting_type):\n        pop = self.pop\n    elif self.discounting_type == \"constant_model_collapsed\":\n        pop = self.pop.mean(\"model\")\n    elif \"gwr\" in self.discounting_type:\n        pop = self.pop.mean([\"model\", \"ssp\"])\n    return pop\n</code></pre>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.damage_function","title":"dscim.menu.main_recipe.MainRecipe.damage_function","text":"<pre><code>damage_function()\n</code></pre> <p>Calls damage function calculation method.</p> <p>This function calls the damage function calculation in model_outputs(). It calculates a damage function for each passed <code>scenario_dimension</code> based on subsets of self.damage_function_points and extrapolates this function using the specified method for all years post-end_ext_subset_year.</p> <p>Returns:</p> Type Description <code>dict</code> <p>dict['params'] is a dataframe of betas for each year dict['preds'] is a dataframe of predicted y hat for each     year and anomaly</p> Source code in <code>src/dscim/menu/main_recipe.py</code> <pre><code>@cachedproperty\ndef damage_function(self):\n    \"\"\"Calls damage function calculation method.\n\n    This function calls the damage function calculation in\n    model_outputs(). It calculates a damage function for each\n    passed `scenario_dimension` based on subsets of\n    self.damage_function_points and extrapolates this function\n    using the specified method for all years post-end_ext_subset_year.\n\n    Returns\n    -------\n    dict\n        dict['params'] is a dataframe of betas for each year\n        dict['preds'] is a dataframe of predicted y hat for each\n            year and anomaly\n    \"\"\"\n    if self.scenario_dimensions is None:\n        # this only occurs for global discounting\n        # with a single scenario passed\n        damage_function = self.damage_function_calculation(\n            damage_function_points=self.damage_function_points,\n            global_consumption=self.global_consumption,\n        )\n\n    else:\n        # cycle through the different combinations of the scenario dims\n        subset = self.damage_function_points.groupby(self.scenario_dimensions)\n        damage_function, dict_list = {}, []\n\n        for name, dt in subset:\n            # turn single-dim into a tuple to make indexing easier later\n            if len(self.scenario_dimensions) == 1:\n                name = tuple([name])\n\n            df = self.damage_function_calculation(\n                damage_function_points=dt,\n                global_consumption=self.global_consumption,\n            )\n            # assigning dimensions to each dataArray in the dictionary\n            for key in df.keys():\n                df[key] = df[key].expand_dims(\n                    {\n                        var: [val]\n                        for var, val in zip(self.scenario_dimensions, list(name))\n                    }\n                )\n            dict_list.append(df)\n\n        # concatenate different scenarios into one big dataArray\n        damage_function[\"params\"] = xr.combine_by_coords(\n            [x[\"params\"] for x in dict_list]\n        )\n\n        damage_function[\"preds\"] = xr.combine_by_coords(\n            [x[\"preds\"] for x in dict_list]\n        )\n\n    return damage_function\n</code></pre>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.damage_function_calculation","title":"dscim.menu.main_recipe.MainRecipe.damage_function_calculation","text":"<pre><code>damage_function_calculation(damage_function_points, global_consumption)\n</code></pre> <p>The damage function model fit may be : (1) ssp specific, (2) ssp-model specific, (3) unique across ssp-model. This depends on the type of discounting. In each case the input data passed to the fitting functions and the formatting of the returned output is different because dimensions are different. This function handles this and returns the model fit.</p> <p>Returns:</p> Type Description <code>dict with two xr.Datasets, 'params' (model fit) and 'preds' (predictions from model fit), with dimensions depending</code> <code>on self.discounting_type.</code> Source code in <code>src/dscim/menu/main_recipe.py</code> <pre><code>def damage_function_calculation(self, damage_function_points, global_consumption):\n    \"\"\"The damage function model fit may be : (1) ssp specific, (2) ssp-model specific, (3) unique across ssp-model.\n    This depends on the type of discounting. In each case the input data passed to the fitting functions and the formatting of the returned\n    output is different because dimensions are different. This function handles this and returns the model fit.\n\n    Returns\n    ------\n    dict with two xr.Datasets, 'params' (model fit) and 'preds' (predictions from model fit), with dimensions depending\n    on self.discounting_type.\n    \"\"\"\n\n    yrs = range(self.climate.pulse_year, self.ext_subset_end_year + 1)\n\n    params_list, preds_list = [], []\n\n    if self.discounting_type == \"constant_model_collapsed\":\n        for ssp in damage_function_points[\"ssp\"].unique():\n            # Subset dataframe to specific SSP\n            fit_subset = damage_function_points[\n                damage_function_points[\"ssp\"] == ssp\n            ]\n\n            global_c_subset = global_consumption.sel({\"ssp\": ssp})\n            # Fit damage function curves using the data subset\n            damage_function = model_outputs(\n                damage_function=fit_subset,\n                formula=self.formula,\n                type_estimation=self.fit_type,\n                global_c=global_c_subset,\n                extrapolation_type=self.ext_method,\n                quantiles=self.quantreg_quantiles,\n                year_range=yrs,\n                year_start_pred=self.ext_subset_end_year + 1,\n            )\n\n            # Add variables\n            params = damage_function[\"parameters\"].expand_dims(\n                dict(\n                    discount_type=[self.discounting_type],\n                    ssp=[ssp],\n                    model=[str(list(self.gdp.model.values))],\n                )\n            )\n\n            preds = damage_function[\"preds\"].expand_dims(\n                dict(\n                    discount_type=[self.discounting_type],\n                    ssp=[ssp],\n                    model=[str(list(self.gdp.model.values))],\n                )\n            )\n\n            params_list.append(params)\n            preds_list.append(preds)\n\n    elif (self.discounting_type == \"constant\") or (\n        \"ramsey\" in self.discounting_type\n    ):\n        for ssp, model in list(\n            product(\n                damage_function_points.ssp.unique(),\n                damage_function_points.model.unique(),\n            )\n        ):\n            # Subset dataframe to specific SSP-IAM combination.\n            fit_subset = damage_function_points[\n                (damage_function_points[\"ssp\"] == ssp)\n                &amp; (damage_function_points[\"model\"] == model)\n            ]\n\n            global_c_subset = global_consumption.sel({\"ssp\": ssp, \"model\": model})\n\n            # Fit damage function curves using the data subset\n            damage_function = model_outputs(\n                damage_function=fit_subset,\n                formula=self.formula,\n                type_estimation=self.fit_type,\n                global_c=global_c_subset,\n                extrapolation_type=self.ext_method,\n                quantiles=self.quantreg_quantiles,\n                year_range=yrs,\n                year_start_pred=self.ext_subset_end_year + 1,\n            )\n\n            # Add variables\n            params = damage_function[\"parameters\"].expand_dims(\n                dict(\n                    discount_type=[self.discounting_type], ssp=[ssp], model=[model]\n                )\n            )\n\n            preds = damage_function[\"preds\"].expand_dims(\n                dict(\n                    discount_type=[self.discounting_type], ssp=[ssp], model=[model]\n                )\n            )\n\n            params_list.append(params)\n            preds_list.append(preds)\n\n    elif \"gwr\" in self.discounting_type:\n        # Fit damage function across all SSP-IAM combinations, as expected\n        # from the Weitzman-Ramsey discounting\n        fit_subset = damage_function_points\n\n        # Fit damage function curves using the data subset\n        damage_function = model_outputs(\n            damage_function=fit_subset,\n            type_estimation=self.fit_type,\n            formula=self.formula,\n            global_c=global_consumption,\n            extrapolation_type=self.ext_method,\n            quantiles=self.quantreg_quantiles,\n            year_range=yrs,\n            year_start_pred=self.ext_subset_end_year + 1,\n        )\n\n        # Add variables\n        params = damage_function[\"parameters\"].expand_dims(\n            dict(\n                discount_type=[self.discounting_type],\n                ssp=[str(list(self.gdp.ssp.values))],\n                model=[str(list(self.gdp.model.values))],\n            )\n        )\n\n        preds = damage_function[\"preds\"].expand_dims(\n            dict(\n                discount_type=[self.discounting_type],\n                ssp=[str(list(self.gdp.ssp.values))],\n                model=[str(list(self.gdp.model.values))],\n            )\n        )\n\n        params_list.append(params)\n        preds_list.append(preds)\n\n    return dict(\n        params=xr.combine_by_coords(params_list),\n        preds=xr.combine_by_coords(preds_list),\n    )\n</code></pre>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.damage_function_points","title":"dscim.menu.main_recipe.MainRecipe.damage_function_points","text":"<pre><code>damage_function_points()\n</code></pre> <p>Global damages by RCP/GCM or SLR</p> <p>Returns:</p> Type Description <code>    pd.DataFrame</code> Source code in <code>src/dscim/menu/main_recipe.py</code> <pre><code>@cachedproperty\n@save(name=\"damage_function_points\")\ndef damage_function_points(self) -&gt; pd.DataFrame:\n    \"\"\"Global damages by RCP/GCM or SLR\n\n    Returns\n    --------\n        pd.DataFrame\n    \"\"\"\n    df = self.global_damages_calculation()\n\n    if \"slr\" in df.columns:\n        df = df.merge(self.climate.gmsl, on=[\"year\", \"slr\"])\n    if \"gcm\" in df.columns:\n        df = df.merge(self.climate.gmst, on=[\"year\", \"gcm\", \"rcp\"])\n\n    # removing illegal combinations from estimation\n    if any([i in df.ssp.unique() for i in [\"SSP1\", \"SSP5\"]]):\n        self.logger.info(\"Dropping illegal model combinations.\")\n        for var in [i for i in df.columns if i in [\"anomaly\", \"gmsl\"]]:\n            df.loc[\n                ((df.ssp == \"SSP1\") &amp; (df.rcp == \"rcp85\"))\n                | ((df.ssp == \"SSP5\") &amp; (df.rcp == \"rcp45\")),\n                var,\n            ] = np.nan\n\n    # agriculture lacks ACCESS0-1/rcp85 combo\n    if \"agriculture\" in self.sector:\n        self.logger.info(\"Dropping illegal model combinations for agriculture.\")\n        df.loc[(df.gcm == \"ACCESS1-0\") &amp; (df.rcp == \"rcp85\"), \"anomaly\"] = np.nan\n\n    return df\n</code></pre>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.discounted_damages","title":"dscim.menu.main_recipe.MainRecipe.discounted_damages","text":"<pre><code>discounted_damages(damages, discrate)\n</code></pre> <p>Discount marginal damages. Distinguishes between constant discount rates method and non-constant discount rates.</p> <p>Parameters:</p> Name Type Description Default <code>damages</code> <code>DataArray or Dataset</code> <p>Array of damages with a<code>discount_type</code> dimension to subset the damages.</p> required <code>discrate</code> <code>str</code> <p>Discounting type. Be aware that the constant rates are class-wide defined. If this str is either 'constant' or 'constant_model_collapsed', the predetermined constant discount rates are used, otherwise, the stream of (non-constant) discount factors from self.stream_discount_factors() is used.</p> required <p>Returns:</p> Type Description <code>    xr.Dataset</code> Source code in <code>src/dscim/menu/main_recipe.py</code> <pre><code>def discounted_damages(self, damages, discrate):\n    \"\"\"Discount marginal damages. Distinguishes between constant discount rates method and non-constant discount rates.\n\n    Parameters\n    ----------\n    damages : xr.DataArray or xr.Dataset\n        Array of damages with a`discount_type` dimension to subset the damages.\n    discrate : str\n         Discounting type. Be aware that the constant rates are class-wide defined. If this str is either 'constant' or 'constant_model_collapsed', the predetermined constant discount rates are used, otherwise, the stream of (non-constant) discount factors from self.stream_discount_factors() is used.\n\n    Returns\n    -------\n        xr.Dataset\n    \"\"\"\n\n    if discrate in [\"constant\", \"constant_model_collapsed\", \"constant_gwr\"]:\n        if self.discrete_discounting:\n            discrate_damages = [\n                damages * (1 / (1 + r)) ** (damages.year - self.climate.pulse_year)\n                for r in self.CONST_DISC_RATES\n            ]\n        else:\n            discrate_damages = [\n                damages * np.exp(-r * (damages.year - self.climate.pulse_year))\n                for r in self.CONST_DISC_RATES\n            ]\n\n        pvd_damages = xr.concat(\n            discrate_damages,\n            dim=pd.Index(self.CONST_DISC_RATES, name=\"discrate\"),\n        )\n    else:\n        factors = self.calculate_stream_discount_factors(\n            discounting_type=self.discounting_type,\n            fair_aggregation=damages.fair_aggregation,\n        )\n        pvd_damages = factors * damages\n\n    return pvd_damages\n</code></pre>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.full_uncertainty_iqr","title":"dscim.menu.main_recipe.MainRecipe.full_uncertainty_iqr","text":"<pre><code>full_uncertainty_iqr()\n</code></pre> <p>Calculate the distribution of quantile-weighted SCCs produced from quantile regressions.</p> Source code in <code>src/dscim/menu/main_recipe.py</code> <pre><code>@cachedproperty\n@save(name=\"full_uncertainty_iqr\")\ndef full_uncertainty_iqr(self):\n    \"\"\"Calculate the distribution of quantile-weighted SCCs produced from\n    quantile regressions.\n    \"\"\"\n    return quantile_weight_quantilereg(\n        self.uncollapsed_sccs,\n        fair_dims=self.fair_dims,\n        quantiles=self.full_uncertainty_quantiles,\n    )\n</code></pre>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.global_consumption","title":"dscim.menu.main_recipe.MainRecipe.global_consumption","text":"<pre><code>global_consumption()\n</code></pre> <p>Global consumption without climate change</p> Source code in <code>src/dscim/menu/main_recipe.py</code> <pre><code>@cachedproperty\n@save(name=\"global_consumption\")\ndef global_consumption(self):\n    \"\"\"Global consumption without climate change\"\"\"\n\n    # rff simulation means that GDP already exists out to 2300\n    if 2300 in self.gdp.year:\n        self.logger.debug(\"Global consumption found up to 2300.\")\n        global_cons = self.gdp.sum(\"region\").rename(\"global_consumption\")\n    else:\n        self.logger.info(\"Extrapolating global consumption.\")\n\n        # holding population constant\n        # from 2100 to 2300 with 2099 values\n        pop = self.collapsed_pop.sum(\"region\")\n        pop = pop.reindex(\n            year=range(pop.year.min().values, self.ext_end_year + 1),\n            method=\"ffill\",\n        )\n\n        # Calculate global consumption back by\n        global_cons = (\n            self.global_consumption_per_capita(self.discounting_type) * pop\n        )\n\n    # Add dimension\n    # @TODO: remove this line altogether\n    global_cons = global_cons.expand_dims(\n        {\"discount_type\": [self.discounting_type]}\n    )\n\n    return global_cons\n</code></pre>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.global_consumption_calculation","title":"dscim.menu.main_recipe.MainRecipe.global_consumption_calculation  <code>abstractmethod</code>","text":"<pre><code>global_consumption_calculation(disc_type)\n</code></pre> <p>Calculation of global consumption without climate change</p> <p>Returns:</p> Type Description <code>    xr.DataArray</code> Source code in <code>src/dscim/menu/main_recipe.py</code> <pre><code>@abstractmethod\ndef global_consumption_calculation(self, disc_type):\n    \"\"\"Calculation of global consumption without climate change\n\n    Returns\n    -------\n        xr.DataArray\n    \"\"\"\n</code></pre>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.global_consumption_no_pulse","title":"dscim.menu.main_recipe.MainRecipe.global_consumption_no_pulse","text":"<pre><code>global_consumption_no_pulse()\n</code></pre> <p>Global consumption under FAIR control scenario.</p> Source code in <code>src/dscim/menu/main_recipe.py</code> <pre><code>@cachedproperty\n@save(name=\"global_consumption_no_pulse\")\ndef global_consumption_no_pulse(self):\n    \"\"\"Global consumption under FAIR control scenario.\"\"\"\n\n    fair_control = self.climate.fair_control\n\n    if self.clip_gmsl:\n        fair_control[\"gmsl\"] = np.minimum(fair_control[\"gmsl\"], self.gmsl_max)\n\n    damages = compute_damages(\n        fair_control,\n        betas=self.damage_function_coefficients,\n        formula=self.formula,\n    )\n\n    cc_cons = self.global_consumption - damages\n\n    gc_no_pulse = []\n    for wp in self.weitzman_parameter:\n        gc = self.weitzman_min(\n            no_cc_consumption=self.global_consumption,\n            cc_consumption=cc_cons,\n            parameter=wp,\n        )\n        gc = gc.assign_coords({\"weitzman_parameter\": str(wp)})\n\n        gc_no_pulse.append(gc)\n\n    return xr.concat(gc_no_pulse, dim=\"weitzman_parameter\")\n</code></pre>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.global_consumption_per_capita","title":"dscim.menu.main_recipe.MainRecipe.global_consumption_per_capita","text":"<pre><code>global_consumption_per_capita(disc_type)\n</code></pre> <p>Global consumption per capita</p> <p>Returns:</p> Type Description <code>    xr.DataArray</code> Source code in <code>src/dscim/menu/main_recipe.py</code> <pre><code>def global_consumption_per_capita(self, disc_type):\n    \"\"\"Global consumption per capita\n\n    Returns\n    -------\n        xr.DataArray\n    \"\"\"\n\n    # Calculate global consumption per capita\n    array_pc = self.global_consumption_calculation(\n        disc_type\n    ) / self.collapsed_pop.sum(\"region\")\n\n    if self.NAME == \"equity\":\n        # equity recipe's growth is capped to\n        # risk aversion recipe's growth rates\n        extrapolated = extrapolate(\n            xr_array=array_pc,\n            start_year=self.ext_subset_start_year,\n            end_year=self.ext_subset_end_year,\n            interp_year=self.ext_end_year,\n            method=\"growth_constant\",\n            cap=self.risk_aversion_growth_rates(),\n        )\n\n    else:\n        extrapolated = extrapolate(\n            xr_array=array_pc,\n            start_year=self.ext_subset_start_year,\n            end_year=self.ext_subset_end_year,\n            interp_year=self.ext_end_year,\n            method=\"growth_constant\",\n        )\n\n    complete_array = xr.concat([array_pc, extrapolated], dim=\"year\")\n\n    return complete_array\n</code></pre>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.global_consumption_pulse","title":"dscim.menu.main_recipe.MainRecipe.global_consumption_pulse","text":"<pre><code>global_consumption_pulse()\n</code></pre> <p>Global consumption under FAIR pulse scenario.</p> Source code in <code>src/dscim/menu/main_recipe.py</code> <pre><code>@cachedproperty\n@save(name=\"global_consumption_pulse\")\ndef global_consumption_pulse(self):\n    \"\"\"Global consumption under FAIR pulse scenario.\"\"\"\n\n    fair_pulse = self.climate.fair_pulse\n\n    if self.clip_gmsl:\n        fair_pulse[\"gmsl\"] = np.minimum(fair_pulse[\"gmsl\"], self.gmsl_max)\n\n    damages = compute_damages(\n        fair_pulse,\n        betas=self.damage_function_coefficients,\n        formula=self.formula,\n    )\n\n    cc_cons = self.global_consumption - damages\n    gc_no_pulse = []\n    for wp in self.weitzman_parameter:\n        gc = self.weitzman_min(\n            no_cc_consumption=self.global_consumption,\n            cc_consumption=cc_cons,\n            parameter=wp,\n        )\n        gc = gc.assign_coords({\"weitzman_parameter\": str(wp)})\n        gc_no_pulse.append(gc)\n    return xr.concat(gc_no_pulse, dim=\"weitzman_parameter\")\n</code></pre>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.global_damages_calculation","title":"dscim.menu.main_recipe.MainRecipe.global_damages_calculation  <code>abstractmethod</code>","text":"<pre><code>global_damages_calculation()\n</code></pre> <p>Calculate global collapsed damages for a desired discount type</p> Source code in <code>src/dscim/menu/main_recipe.py</code> <pre><code>@abstractmethod\ndef global_damages_calculation(self):\n    \"\"\"Calculate global collapsed damages for a desired discount type\"\"\"\n</code></pre>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.marginal_damages","title":"dscim.menu.main_recipe.MainRecipe.marginal_damages","text":"<pre><code>marginal_damages()\n</code></pre> <p>Marginal damages due to additional pulse</p> Source code in <code>src/dscim/menu/main_recipe.py</code> <pre><code>@cachedproperty\n@save(name=\"marginal_damages\")\ndef marginal_damages(self):\n    \"\"\"Marginal damages due to additional pulse\"\"\"\n\n    marginal_damages = []\n\n    for agg in [i for i in self.fair_aggregation if i != \"median\"]:\n        if agg == \"ce\":\n            md = self.ce_fair_no_pulse - self.ce_fair_pulse\n        elif agg in [\"mean\", \"gwr_mean\"]:\n            md = self.global_consumption_no_pulse.mean(\n                self.fair_dims\n            ) - self.global_consumption_pulse.mean(self.fair_dims)\n        elif agg == \"median_params\":\n            md = self.median_params_marginal_damages\n        else:\n            raise NotImplementedError(\n                f\"{agg} is not available. Enter list including\"\n                '[\"ce\", \"fair\", \"median\", \"median_params\"]'\n            )\n\n        md = md.assign_coords({\"fair_aggregation\": agg}).expand_dims(\n            \"fair_aggregation\"\n        )\n\n        # convert to the marginal damages from a single tonne\n        md = md * self.climate.conversion\n        marginal_damages.append(md)\n\n    return xr.concat(marginal_damages, dim=\"fair_aggregation\")\n</code></pre>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.order_plate","title":"dscim.menu.main_recipe.MainRecipe.order_plate","text":"<pre><code>order_plate(course)\n</code></pre> <p>Execute menu option section and save results</p> <p>This method is a entry point to the class and allows the user to calculate different elements of a specific menu option. These elements will automatically be saved in the path defined in <code>save_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>course</code> <p>Output to be calculated. Options are:     - <code>damage_function</code>: Return and save all damage function       elements including damage function points, coefficients, and       fitted values.     - <code>scc</code>: Return Social Cost of Carbon calculation. All elements     from <code>damage_function</code> are saved and returned.</p> required <p>Returns:</p> Type Description <code>None. Saved all elements to `save_path`</code> Source code in <code>src/dscim/menu/main_recipe.py</code> <pre><code>def order_plate(self, course):\n    \"\"\"\n    Execute menu option section and save results\n\n    This method is a entry point to the class and allows the user to\n    calculate different elements of a specific menu option. These elements\n    will automatically be saved in the path defined in `save_path`.\n\n    Parameters\n    ----------\n    course str\n        Output to be calculated. Options are:\n            - `damage_function`: Return and save all damage function\n              elements including damage function points, coefficients, and\n              fitted values.\n            - `scc`: Return Social Cost of Carbon calculation. All elements\n            from `damage_function` are saved and returned.\n\n    Returns\n    -------\n    None. Saved all elements to `save_path`\n\n    \"\"\"\n\n    self.logger.info(f\"\\n Executing {self.__repr__()}\")\n\n    def damage_function():\n        self.logger.info(\"Processing damage functions ...\")\n        if self.damage_function_path is None:\n            self.logger.info(\n                \"Existing damage functions not found. Damage points will be loaded.\"\n            )\n            self.damage_function_points\n        self.damage_function_coefficients\n        try:\n            self.damage_function_fit\n        except FileNotFoundError:\n            pass\n\n    def scc():\n        damage_function()\n        self.global_consumption\n        self.global_consumption_no_pulse\n        self.logger.info(\"Processing SCC calculation ...\")\n        if self.fit_type == \"quantreg\":\n            self.full_uncertainty_iqr\n            # stat_uncertainty_iqr function expects collapsed SCCs, so a fair aggregation is required\n            if len(self.fair_aggregation) &gt; 0:\n                self.calculate_scc\n                self.stat_uncertainty_iqr\n        else:\n            if len(self.fair_aggregation) &gt; 0:\n                self.stream_discount_factors\n                self.calculate_scc\n            self.uncollapsed_sccs\n            self.uncollapsed_marginal_damages\n            self.uncollapsed_discount_factors\n\n    course_dict = {\"damage_function\": damage_function, \"scc\": scc}\n\n    try:\n        course_dict[course]()\n        self.logger.info(f\"Results available: {self.save_path}\")\n    except KeyError as e:\n        self.logger.error(f\"{course} is not a valid option: {e}\")\n        raise e\n    except Exception as e:\n        self.logger.error(\"Error detected.\")\n        raise e\n\n    return None\n</code></pre>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.order_scc","title":"dscim.menu.main_recipe.MainRecipe.order_scc","text":"<pre><code>order_scc()\n</code></pre> <p>Execute menu option section and save results</p> <p>This method is a wrapper to <code>order_plate</code> that calls the \"scc\" course, which is the Social Cost of Carbon calculation. Elements involved in the calculation (<code>fair</code> and <code>damage_function</code>) will automatically be saved in the path defined in <code>save_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>None</code> required <p>Returns:</p> Type Description <code>xr.Dataset of SCCs</code> Source code in <code>src/dscim/menu/main_recipe.py</code> <pre><code>def order_scc(self):\n    \"\"\"\n    Execute menu option section and save results\n\n    This method is a wrapper to `order_plate` that calls the \"scc\" course,\n    which is the Social Cost of Carbon calculation. Elements involved in the calculation\n    (`fair` and `damage_function`) will automatically be saved in the path\n    defined in `save_path`.\n\n    Parameters\n    ----------\n    None\n\n    Returns\n    -------\n    xr.Dataset of SCCs\n\n    \"\"\"\n\n    self.logger.info(f\"\\n Executing {self.__repr__()}\")\n\n    try:\n        sccds = self.calculate_scc\n        self.logger.info(f\"Results available: {self.save_path}\")\n    except Exception as e:\n        self.logger.error(\"Error detected.\")\n        raise e\n\n    if (\"rcp45\" in sccds.rcp) or (\"rcp85\" in sccds.rcp):\n        # leave the dataset alone if there are already rcp scenario names\n        pass\n    else:\n        # rename the CMIP6 scenario names that start with \"ssp*\"\n        sccds = sccds.sortby(sccds.rcp)\n\n        rcpdt = {\n            \"ssp126\": \"RCP2.6\",\n            \"ssp245\": \"RCP4.5\",\n            \"ssp370\": \"RCP7.0\",\n            \"ssp460\": \"RCP6.0\",\n            \"ssp585\": \"RCP8.5\",\n        }\n        rlst = []\n        for rcp in sccds.rcp.values:\n            rlst.append(rcpdt[rcp])\n        sccds.coords[\"rcp\"] = rlst\n        sccds = sccds.sortby(sccds.rcp)\n\n    return sccds.squeeze(drop=True)\n</code></pre>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.stat_uncertainty_iqr","title":"dscim.menu.main_recipe.MainRecipe.stat_uncertainty_iqr","text":"<pre><code>stat_uncertainty_iqr()\n</code></pre> <p>Calculate the distribution of quantile-weighted SCCs produced from quantile regressions that have already been collapsed across other dimensions to give statistical-only uncertainty.</p> Source code in <code>src/dscim/menu/main_recipe.py</code> <pre><code>@cachedproperty\n@save(name=\"stat_uncertainty_iqr\")\ndef stat_uncertainty_iqr(self):\n    \"\"\"Calculate the distribution of quantile-weighted SCCs produced from\n    quantile regressions that have already been collapsed across other dimensions to give statistical-only uncertainty.\n    \"\"\"\n    return quantile_weight_quantilereg(\n        self.calculate_scc,\n        fair_dims=self.fair_dims,\n        quantiles=self.full_uncertainty_quantiles,\n    )\n</code></pre>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.uncollapsed_sccs","title":"dscim.menu.main_recipe.MainRecipe.uncollapsed_sccs","text":"<pre><code>uncollapsed_sccs()\n</code></pre> <p>Calculate full distribution of SCCs without FAIR aggregation</p> Source code in <code>src/dscim/menu/main_recipe.py</code> <pre><code>@cachedproperty\n@save(name=\"uncollapsed_sccs\")\ndef uncollapsed_sccs(self):\n    \"\"\"Calculate full distribution of SCCs without FAIR aggregation\"\"\"\n\n    md = (\n        self.global_consumption_no_pulse - self.global_consumption_pulse\n    )  # this is for full uncertainty\n\n    # convert to the marginal damages from a single pulse\n    md = md * self.climate.conversion\n\n    md = md.expand_dims({\"fair_aggregation\": [\"uncollapsed\"]})\n\n    sccs = self.discounted_damages(\n        damages=md,\n        discrate=self.discounting_type,\n    ).sum(dim=\"year\")\n\n    return sccs\n</code></pre>"},{"location":"menu/main_recipe/#dscim.menu.main_recipe.MainRecipe.weitzman_min","title":"dscim.menu.main_recipe.MainRecipe.weitzman_min","text":"<pre><code>weitzman_min(no_cc_consumption, cc_consumption, parameter)\n</code></pre> <p>Implements bottom coding that fixes marginal utility below a threshold to the marginal utility at that threshold. The threshold is the Weitzman parameter.</p> <p>Parameters:</p> Name Type Description Default <code>no_cc_consumption</code> <p>Consumption array of which the share will be used to calculate the absolute Weitzman parameter, only if parameter &lt;= 1.</p> required <code>consumption</code> <p>Consumption array to be bottom-coded.</p> required <code>parameter</code> <p>A positive number representing the Weitzman parameter, below which marginal utility will be top coded; ie., 0.01 implies marginal utility is top coded to the value of marginal utility at 1% of no-climate change global consumption. If parameter &gt; 1, it is assumed to be an absolute value. If parameter &lt;= 1, it is assumed to be a share of future global consumption (without climate change).</p> required <p>Returns:</p> Type Description <code>    xr.Dataset</code> Source code in <code>src/dscim/menu/main_recipe.py</code> <pre><code>def weitzman_min(self, no_cc_consumption, cc_consumption, parameter):\n    \"\"\"\n    Implements bottom coding that fixes marginal utility below a threshold\n    to the marginal utility at that threshold. The threshold is the Weitzman\n    parameter.\n\n    Parameters\n    ----------\n    no_cc_consumption: xr.DataArray\n        Consumption array of which the share will be used to calculate\n        the absolute Weitzman parameter, only if parameter &lt;= 1.\n    consumption: xr.DataArray\n        Consumption array to be bottom-coded.\n    parameter: float\n        A positive number representing the Weitzman parameter, below which marginal utility will be\n        top coded; ie., 0.01 implies marginal utility is top coded to the\n        value of marginal utility at 1% of no-climate change global consumption.\n        If parameter &gt; 1, it is assumed to be an absolute value.\n        If parameter &lt;= 1, it is assumed to be a share of future global consumption\n        (without climate change).\n\n    Returns\n    -------\n        xr.Dataset\n\n    \"\"\"\n    # if parameter is share of consumption,\n    # multiply by no-climate-change consumption\n    if parameter &lt;= 1:\n        parameter = parameter * no_cc_consumption\n\n    if self.eta == 1:\n        w_utility = np.log(parameter)\n        bottom_utility = np.power(parameter, -1) * (parameter - cc_consumption)\n        bottom_coded_cons = np.exp(w_utility - bottom_utility)\n\n        clipped_cons = xr.where(\n            cc_consumption &gt; parameter, cc_consumption, bottom_coded_cons\n        )\n    else:\n        w_utility = np.power(parameter, (1 - self.eta)) / (1 - self.eta)\n        bottom_utility = np.power(parameter, -self.eta) * (\n            parameter - cc_consumption\n        )\n        bottom_coded_cons = power(\n            ((1 - self.eta) * (w_utility - bottom_utility)), (1 / (1 - self.eta))\n        )\n\n        clipped_cons = xr.where(\n            cc_consumption &gt; parameter, cc_consumption, bottom_coded_cons\n        )\n\n    return clipped_cons\n</code></pre>"},{"location":"menu/risk_aversion/","title":"risk_aversion","text":"<p>Classes:</p> Name Description <code>RiskAversionRecipe</code> <p>Risk aversion option</p>"},{"location":"menu/risk_aversion/#dscim.menu.risk_aversion.RiskAversionRecipe","title":"dscim.menu.risk_aversion.RiskAversionRecipe","text":"<p>               Bases: <code>MainRecipe</code></p> <p>Risk aversion option</p> <p>Methods:</p> Name Description <code>ce_cc_calculation</code> <p>Calculate certainty equivalent over consumption with climate change</p> <code>ce_no_cc_calculation</code> <p>Calculate certainty equivalent consumption without climate change</p> <code>global_consumption_calculation</code> <p>Calculate global consumption</p> <code>global_damages_calculation</code> <p>Aggregate damages to global level</p> <p>Attributes:</p> Name Type Description <code>calculated_damages</code> <code>DataArray</code> <p>Calculate damages (difference between CEs)</p> Source code in <code>src/dscim/menu/risk_aversion.py</code> <pre><code>class RiskAversionRecipe(MainRecipe):\n    \"\"\"Risk aversion option\"\"\"\n\n    NAME = \"risk_aversion\"\n    __doc__ = MainRecipe.__doc__\n\n    def ce_cc_calculation(self) -&gt; xr.DataArray:\n        \"\"\"Calculate certainty equivalent over consumption with climate change\n\n        Returns\n        -------\n             xr.DataArray\n        \"\"\"\n        ce_array = self.risk_aversion_damages(\"cc\").cc\n\n        # for GWR options, take the CE over growth models\n        if \"gwr\" in self.discounting_type:\n            ce_array = self.ce(ce_array, dims=[\"ssp\", \"model\"])\n\n        return ce_array\n\n    def ce_no_cc_calculation(self) -&gt; xr.DataArray:\n        \"\"\"Calculate certainty equivalent consumption without climate change\n\n        Returns\n        -------\n            xr.DataArray\n        \"\"\"\n        ce_array = self.risk_aversion_damages(\"no_cc\").no_cc\n\n        if \"gwr\" in self.discounting_type:\n            ce_array = self.ce(ce_array, dims=[\"ssp\", \"model\"])\n\n        return ce_array\n\n    @property\n    def calculated_damages(self) -&gt; xr.DataArray:\n        \"\"\"Calculate damages (difference between CEs)\"\"\"\n        return self.ce_no_cc - self.ce_cc\n\n    def global_damages_calculation(self) -&gt; pd.DataFrame:\n        \"\"\"Aggregate damages to global level\n\n        Returns\n        --------\n            pd.DataFrame\n        \"\"\"\n\n        dams_collapse = (self.calculated_damages * self.collapsed_pop).sum(dim=\"region\")\n        df = dams_collapse.to_dataframe(\"damages\").reset_index()\n\n        if \"gwr\" in self.discounting_type:\n            df = df.assign(\n                ssp=str(list(self.gdp.ssp.values)),\n                model=str(list(self.gdp.model.values)),\n            )\n\n        return df\n\n    def global_consumption_calculation(self, disc_type):\n        \"\"\"Calculate global consumption\n\n        Returns\n        -------\n            xr.DataArray\n        \"\"\"\n\n        if (disc_type == \"constant\") or (\"ramsey\" in disc_type):\n            global_cons_no_cc = self.gdp.sum(dim=[\"region\"])\n\n        elif disc_type == \"constant_model_collapsed\":\n            global_cons_no_cc = self.gdp.sum(dim=[\"region\"]).mean(dim=[\"model\"])\n\n        elif \"gwr\" in disc_type:\n            ce_cons = self.ce(self.gdppc, dims=[\"ssp\", \"model\"])\n            global_cons_no_cc = (ce_cons * self.collapsed_pop).sum(dim=[\"region\"])\n\n        # Convert to array in case xarray becames temperamental. This is a hack\n        # that need to be changed\n        if isinstance(global_cons_no_cc, xr.Dataset):\n            global_cons_no_cc = global_cons_no_cc.to_array()\n\n        global_cons_no_cc.name = f\"global_cons_{disc_type}\"\n\n        return global_cons_no_cc\n</code></pre>"},{"location":"menu/risk_aversion/#dscim.menu.risk_aversion.RiskAversionRecipe.calculated_damages","title":"dscim.menu.risk_aversion.RiskAversionRecipe.calculated_damages  <code>property</code>","text":"<pre><code>calculated_damages\n</code></pre> <p>Calculate damages (difference between CEs)</p>"},{"location":"menu/risk_aversion/#dscim.menu.risk_aversion.RiskAversionRecipe.ce_cc_calculation","title":"dscim.menu.risk_aversion.RiskAversionRecipe.ce_cc_calculation","text":"<pre><code>ce_cc_calculation()\n</code></pre> <p>Calculate certainty equivalent over consumption with climate change</p> <p>Returns:</p> Type Description <code>     xr.DataArray</code> Source code in <code>src/dscim/menu/risk_aversion.py</code> <pre><code>def ce_cc_calculation(self) -&gt; xr.DataArray:\n    \"\"\"Calculate certainty equivalent over consumption with climate change\n\n    Returns\n    -------\n         xr.DataArray\n    \"\"\"\n    ce_array = self.risk_aversion_damages(\"cc\").cc\n\n    # for GWR options, take the CE over growth models\n    if \"gwr\" in self.discounting_type:\n        ce_array = self.ce(ce_array, dims=[\"ssp\", \"model\"])\n\n    return ce_array\n</code></pre>"},{"location":"menu/risk_aversion/#dscim.menu.risk_aversion.RiskAversionRecipe.ce_no_cc_calculation","title":"dscim.menu.risk_aversion.RiskAversionRecipe.ce_no_cc_calculation","text":"<pre><code>ce_no_cc_calculation()\n</code></pre> <p>Calculate certainty equivalent consumption without climate change</p> <p>Returns:</p> Type Description <code>    xr.DataArray</code> Source code in <code>src/dscim/menu/risk_aversion.py</code> <pre><code>def ce_no_cc_calculation(self) -&gt; xr.DataArray:\n    \"\"\"Calculate certainty equivalent consumption without climate change\n\n    Returns\n    -------\n        xr.DataArray\n    \"\"\"\n    ce_array = self.risk_aversion_damages(\"no_cc\").no_cc\n\n    if \"gwr\" in self.discounting_type:\n        ce_array = self.ce(ce_array, dims=[\"ssp\", \"model\"])\n\n    return ce_array\n</code></pre>"},{"location":"menu/risk_aversion/#dscim.menu.risk_aversion.RiskAversionRecipe.global_consumption_calculation","title":"dscim.menu.risk_aversion.RiskAversionRecipe.global_consumption_calculation","text":"<pre><code>global_consumption_calculation(disc_type)\n</code></pre> <p>Calculate global consumption</p> <p>Returns:</p> Type Description <code>    xr.DataArray</code> Source code in <code>src/dscim/menu/risk_aversion.py</code> <pre><code>def global_consumption_calculation(self, disc_type):\n    \"\"\"Calculate global consumption\n\n    Returns\n    -------\n        xr.DataArray\n    \"\"\"\n\n    if (disc_type == \"constant\") or (\"ramsey\" in disc_type):\n        global_cons_no_cc = self.gdp.sum(dim=[\"region\"])\n\n    elif disc_type == \"constant_model_collapsed\":\n        global_cons_no_cc = self.gdp.sum(dim=[\"region\"]).mean(dim=[\"model\"])\n\n    elif \"gwr\" in disc_type:\n        ce_cons = self.ce(self.gdppc, dims=[\"ssp\", \"model\"])\n        global_cons_no_cc = (ce_cons * self.collapsed_pop).sum(dim=[\"region\"])\n\n    # Convert to array in case xarray becames temperamental. This is a hack\n    # that need to be changed\n    if isinstance(global_cons_no_cc, xr.Dataset):\n        global_cons_no_cc = global_cons_no_cc.to_array()\n\n    global_cons_no_cc.name = f\"global_cons_{disc_type}\"\n\n    return global_cons_no_cc\n</code></pre>"},{"location":"menu/risk_aversion/#dscim.menu.risk_aversion.RiskAversionRecipe.global_damages_calculation","title":"dscim.menu.risk_aversion.RiskAversionRecipe.global_damages_calculation","text":"<pre><code>global_damages_calculation()\n</code></pre> <p>Aggregate damages to global level</p> <p>Returns:</p> Type Description <code>    pd.DataFrame</code> Source code in <code>src/dscim/menu/risk_aversion.py</code> <pre><code>def global_damages_calculation(self) -&gt; pd.DataFrame:\n    \"\"\"Aggregate damages to global level\n\n    Returns\n    --------\n        pd.DataFrame\n    \"\"\"\n\n    dams_collapse = (self.calculated_damages * self.collapsed_pop).sum(dim=\"region\")\n    df = dams_collapse.to_dataframe(\"damages\").reset_index()\n\n    if \"gwr\" in self.discounting_type:\n        df = df.assign(\n            ssp=str(list(self.gdp.ssp.values)),\n            model=str(list(self.gdp.model.values)),\n        )\n\n    return df\n</code></pre>"},{"location":"menu/simple_storage/","title":"simple_storage","text":"<p>Classes:</p> Name Description <code>Climate</code> <p>This class wraps all climate data used in DSCIM.</p> <code>EconVars</code> <p>This class wraps all socioeconomic data used in DSCIM.</p> <code>StackedDamages</code> <p>This class wraps all damages data used in DSCIM.</p>"},{"location":"menu/simple_storage/#dscim.menu.simple_storage.Climate","title":"dscim.menu.simple_storage.Climate","text":"<p>This class wraps all climate data used in DSCIM.</p> <p>Parameters:</p> Name Type Description Default <code>gmst_path</code> <code>str</code> <p>Path to GMST anomalies for damage function step.</p> required <code>gmsl_path</code> <code>str</code> <p>Path to GMSL anomalies for damage function step.</p> required <code>gmst_fair_path</code> <code>str</code> <p>Path to GMST anomalies data for FAIR step.</p> required <code>gmsl_fair_path</code> <code>str</code> <p>Path to GMSL anomalies data for FAIR step.</p> <code>None</code> <code>pulse_year</code> <code>int</code> <p>Year of the greenhouse gas pulse.</p> required <code>damages_pulse_conversion_path</code> <p>Path to file containing conversion factors for each greenhouse gas to turn the pulse units into the appropriate units for an SCC calculation.</p> required <code>ecs_mask_path</code> <code>str or None</code> <p>Path to a boolean NetCDF4 dataset sharing the same coordinates as self.anomalies, indicating which simulations should be included or excluded.</p> <code>None</code> <code>ecs_mask_name</code> <code>str or None</code> <p>Name of mask to be called from within <code>ecs_mask_path</code> NetCDF file.</p> <code>None</code> <code>base_period</code> <p>Period for rebasing FAIR temperature anomalies. This should match the CIL projection system's base period.</p> <code>(2001, 2010)</code> <code>emission_scenarios</code> <p>List of emission scenarios for which SCC will be calculated. Default is (), which gets set to [\"ssp119\", \"ssp126\", \"ssp245\", \"ssp460\", \"ssp370\", \"ssp585\"]. Use <code>None</code> when RCP emission scenarios are not the climate projections, such as with RFF-SP projections.</p> <code>()</code> <code>gases</code> <p>List of greenhouse gases for which SCC will be calculated. Default is [\"CO2_Fossil\", \"CH4\", \"N2O\"].</p> <code>None</code> <p>Methods:</p> Name Description <code>anomalies</code> <p>This function combines and subsets the projected GMST and GMSL anomalies by</p> <code>anomaly_vars</code> <p>Anomaly variables to include</p> <p>Attributes:</p> Name Type Description <code>conversion</code> <p>Conversion factors to turn the pulse units</p> <code>fair_control</code> <p>Anomalies without a pulse</p> <code>fair_median_params_control</code> <p>FAIR median parameters anomaly without a pulse</p> <code>fair_median_params_pulse</code> <p>FAIR median parameters anomaly with a pulse</p> <code>fair_pulse</code> <p>Anomalies with a pulse</p> <code>gmsl</code> <p>Cached GMSL anomalies</p> <code>gmsl_anomalies</code> <p>This function takes coastal sector's GMSL relative to 1991-2009.</p> <code>gmst</code> <p>Cached GMST anomalies</p> <code>gmst_anomalies</code> <p>This function takes FAIR GMST relative to 1765.</p> Source code in <code>src/dscim/menu/simple_storage.py</code> <pre><code>class Climate:\n    \"\"\"\n    This class wraps all climate data used in DSCIM.\n\n    Parameters\n    ---------\n    gmst_path : str\n        Path to GMST anomalies for damage function step.\n    gmsl_path : str\n        Path to GMSL anomalies for damage function step.\n    gmst_fair_path : str\n        Path to GMST anomalies data for FAIR step.\n    gmsl_fair_path : str\n        Path to GMSL anomalies data for FAIR step.\n    pulse_year : int\n        Year of the greenhouse gas pulse.\n    damages_pulse_conversion_path: str\n        Path to file containing conversion factors for each greenhouse gas\n        to turn the pulse units into the appropriate units for an SCC calculation.\n    ecs_mask_path : str or None, optional\n        Path to a boolean NetCDF4 dataset sharing the same coordinates as self.anomalies,\n        indicating which simulations should be included or excluded.\n    ecs_mask_name : str or None, optional\n        Name of mask to be called from within ``ecs_mask_path`` NetCDF file.\n    base_period: tuple, optional\n        Period for rebasing FAIR temperature anomalies. This should match the CIL projection system's base period.\n    emission_scenarios: list or None, optional\n        List of emission scenarios for which SCC will be calculated. Default\n        is (), which gets set to [\"ssp119\", \"ssp126\", \"ssp245\", \"ssp460\", \"ssp370\", \"ssp585\"].\n        Use `None` when RCP emission scenarios are not the climate projections,\n        such as with RFF-SP projections.\n    gases: list or None, optional\n        List of greenhouse gases for which SCC will be calculated. Default is\n        [\"CO2_Fossil\", \"CH4\", \"N2O\"].\n    \"\"\"\n\n    def __init__(\n        self,\n        gmst_path,\n        gmsl_path,\n        gmst_fair_path,\n        damages_pulse_conversion_path,\n        pulse_year,\n        gmsl_fair_path=None,\n        ecs_mask_path=None,\n        ecs_mask_name=None,\n        base_period=(2001, 2010),\n        emission_scenarios=(),\n        gases=None,\n    ):\n        if emission_scenarios == ():\n            emission_scenarios = [\n                \"ssp119\",\n                \"ssp126\",\n                \"ssp245\",\n                \"ssp460\",\n                \"ssp370\",\n                \"ssp585\",\n            ]\n        if gases is None:\n            gases = [\"CO2_Fossil\", \"CH4\", \"N2O\"]\n\n        self.gmst_path = gmst_path\n        self.gmsl_path = gmsl_path\n        self.gmst_fair_path = gmst_fair_path\n        self.damages_pulse_conversion_path = damages_pulse_conversion_path\n        self.gmsl_fair_path = gmsl_fair_path\n        self.pulse_year = pulse_year\n        self.emission_scenarios = emission_scenarios\n        self.gases = gases\n        self.base_period = base_period\n        self.ecs_mask_path = ecs_mask_path\n        self.ecs_mask_name = ecs_mask_name\n        self.logger = logging.getLogger(__name__)\n\n    @property\n    def gmst(self):\n        \"\"\"Cached GMST anomalies\"\"\"\n        gmst = pd.read_csv(self.gmst_path)\n\n        if \"temp\" in gmst.columns:\n            gmst = gmst.rename(columns={\"temp\": \"anomaly\"})\n\n        return gmst\n\n    @property\n    def gmsl(self):\n        \"\"\"Cached GMSL anomalies\"\"\"\n        gmsl = xr.open_zarr(self.gmsl_path).gmsl.to_dataframe().reset_index()\n\n        return gmsl\n\n    @property\n    def gmst_anomalies(self):\n        \"\"\"This function takes FAIR GMST relative to 1765.\n        It rebases it to self.base_period.\n        \"\"\"\n        # open FAIR GMST\n        temps = xr.open_dataset(\n            self.gmst_fair_path,\n            chunks={\n                \"year\": 11,\n            },\n        )\n\n        # calculate base period average\n        base_period = temps.sel(\n            year=slice(self.base_period[0], self.base_period[1])\n        ).mean(dim=\"year\")\n\n        # subset relevant years to save compute time\n        temps = temps.sel(year=slice(self.pulse_year, 2300))\n\n        # calculate anomalies\n        anomaly = temps - base_period\n\n        return anomaly\n\n    @property\n    def gmsl_anomalies(self):\n        \"\"\"This function takes coastal sector's GMSL relative to 1991-2009.\n        No rebasing occurs, as coastal damages are rebased to the same period.\n        \"\"\"\n        anomaly = xr.open_dataset(self.gmsl_fair_path)\n        anomaly = anomaly.chunk(anomaly.dims)\n\n        return anomaly\n\n    @cachedproperty\n    def anomaly_vars(self):\n        \"\"\"Anomaly variables to include\"\"\"\n        return (\n            [\"temperature\", \"gmsl\"]\n            if self.gmsl_fair_path is not None\n            else [\"temperature\"]\n        )\n\n    @cachedproperty\n    def anomalies(self):\n        \"\"\"\n        This function combines and subsets the projected GMST and GMSL anomalies by\n        pulse year and emissions scenario. If applicable, it\n        masks the data according to the mask passed to self.ecs_mask_path.\n        \"\"\"\n        if self.gmsl_fair_path is not None:\n            anomaly = xr.combine_by_coords(\n                [self.gmst_anomalies, self.gmsl_anomalies], combine_attrs=\"drop\"\n            )\n        else:\n            anomaly = self.gmst_anomalies\n\n        # subset by relevant coordinates\n        anomaly = anomaly.sel(\n            year=slice(self.pulse_year, 2300),\n            gas=self.gases,\n        )\n\n        if self.emission_scenarios is not None:\n            anomaly = anomaly.sel(rcp=self.emission_scenarios)\n\n        if \"pulse_year\" in anomaly.dims:\n            anomaly = anomaly.sel(pulse_year=self.pulse_year, drop=True)\n\n        # Apply ECS mask\n        if (self.ecs_mask_name is not None) and (self.ecs_mask_path is not None):\n            self.logger.info(f\"Masking anomalies with {self.ecs_mask_name}.\")\n\n            # load mask\n            mask = xr.open_dataset(self.ecs_mask_path)[self.ecs_mask_name]\n\n            # median variables can't be masked because they don't have a simulation dimension\n            vars_no_mask = [v for v in anomaly.keys() if \"median\" in v]\n            vars_to_mask = [v for v in anomaly.keys() if v not in vars_no_mask]\n\n            # mask and put back together\n            anomaly = anomaly[vars_no_mask].update(\n                anomaly[vars_to_mask].where(mask, drop=True)\n            )\n\n        return anomaly\n\n    @property\n    def fair_control(self):\n        \"\"\"Anomalies without a pulse\"\"\"\n        ds = self.anomalies[[f\"control_{var}\" for var in self.anomaly_vars]]\n        return ds.rename({f\"control_{var}\": var for var in self.anomaly_vars})\n\n    @property\n    def fair_pulse(self):\n        \"\"\"Anomalies with a pulse\"\"\"\n        ds = self.anomalies[[f\"pulse_{var}\" for var in self.anomaly_vars]]\n        return ds.rename({f\"pulse_{var}\": var for var in self.anomaly_vars})\n\n    @property\n    def fair_median_params_control(self):\n        \"\"\"FAIR median parameters anomaly without a pulse\"\"\"\n        ds = self.anomalies[\n            [f\"medianparams_control_{var}\" for var in self.anomaly_vars]\n        ]\n        return ds.rename(\n            {f\"medianparams_control_{var}\": var for var in self.anomaly_vars}\n        )\n\n    @property\n    def fair_median_params_pulse(self):\n        \"\"\"FAIR median parameters anomaly with a pulse\"\"\"\n        ds = self.anomalies[[f\"medianparams_pulse_{var}\" for var in self.anomaly_vars]]\n        return ds.rename(\n            {f\"medianparams_pulse_{var}\": var for var in self.anomaly_vars}\n        )\n\n    @property\n    def conversion(self):\n        \"\"\"Conversion factors to turn the pulse units\n        into the appropriate units for an SCC calculation\"\"\"\n\n        conversion = (\n            xr.open_dataset(self.damages_pulse_conversion_path)\n            .sel(gas=self.gases)\n            .to_array()\n            .isel(variable=0, drop=True)\n        )\n        return conversion\n</code></pre>"},{"location":"menu/simple_storage/#dscim.menu.simple_storage.Climate.conversion","title":"dscim.menu.simple_storage.Climate.conversion  <code>property</code>","text":"<pre><code>conversion\n</code></pre> <p>Conversion factors to turn the pulse units into the appropriate units for an SCC calculation</p>"},{"location":"menu/simple_storage/#dscim.menu.simple_storage.Climate.fair_control","title":"dscim.menu.simple_storage.Climate.fair_control  <code>property</code>","text":"<pre><code>fair_control\n</code></pre> <p>Anomalies without a pulse</p>"},{"location":"menu/simple_storage/#dscim.menu.simple_storage.Climate.fair_median_params_control","title":"dscim.menu.simple_storage.Climate.fair_median_params_control  <code>property</code>","text":"<pre><code>fair_median_params_control\n</code></pre> <p>FAIR median parameters anomaly without a pulse</p>"},{"location":"menu/simple_storage/#dscim.menu.simple_storage.Climate.fair_median_params_pulse","title":"dscim.menu.simple_storage.Climate.fair_median_params_pulse  <code>property</code>","text":"<pre><code>fair_median_params_pulse\n</code></pre> <p>FAIR median parameters anomaly with a pulse</p>"},{"location":"menu/simple_storage/#dscim.menu.simple_storage.Climate.fair_pulse","title":"dscim.menu.simple_storage.Climate.fair_pulse  <code>property</code>","text":"<pre><code>fair_pulse\n</code></pre> <p>Anomalies with a pulse</p>"},{"location":"menu/simple_storage/#dscim.menu.simple_storage.Climate.gmsl","title":"dscim.menu.simple_storage.Climate.gmsl  <code>property</code>","text":"<pre><code>gmsl\n</code></pre> <p>Cached GMSL anomalies</p>"},{"location":"menu/simple_storage/#dscim.menu.simple_storage.Climate.gmsl_anomalies","title":"dscim.menu.simple_storage.Climate.gmsl_anomalies  <code>property</code>","text":"<pre><code>gmsl_anomalies\n</code></pre> <p>This function takes coastal sector's GMSL relative to 1991-2009. No rebasing occurs, as coastal damages are rebased to the same period.</p>"},{"location":"menu/simple_storage/#dscim.menu.simple_storage.Climate.gmst","title":"dscim.menu.simple_storage.Climate.gmst  <code>property</code>","text":"<pre><code>gmst\n</code></pre> <p>Cached GMST anomalies</p>"},{"location":"menu/simple_storage/#dscim.menu.simple_storage.Climate.gmst_anomalies","title":"dscim.menu.simple_storage.Climate.gmst_anomalies  <code>property</code>","text":"<pre><code>gmst_anomalies\n</code></pre> <p>This function takes FAIR GMST relative to 1765. It rebases it to self.base_period.</p>"},{"location":"menu/simple_storage/#dscim.menu.simple_storage.Climate.anomalies","title":"dscim.menu.simple_storage.Climate.anomalies","text":"<pre><code>anomalies()\n</code></pre> <p>This function combines and subsets the projected GMST and GMSL anomalies by pulse year and emissions scenario. If applicable, it masks the data according to the mask passed to self.ecs_mask_path.</p> Source code in <code>src/dscim/menu/simple_storage.py</code> <pre><code>@cachedproperty\ndef anomalies(self):\n    \"\"\"\n    This function combines and subsets the projected GMST and GMSL anomalies by\n    pulse year and emissions scenario. If applicable, it\n    masks the data according to the mask passed to self.ecs_mask_path.\n    \"\"\"\n    if self.gmsl_fair_path is not None:\n        anomaly = xr.combine_by_coords(\n            [self.gmst_anomalies, self.gmsl_anomalies], combine_attrs=\"drop\"\n        )\n    else:\n        anomaly = self.gmst_anomalies\n\n    # subset by relevant coordinates\n    anomaly = anomaly.sel(\n        year=slice(self.pulse_year, 2300),\n        gas=self.gases,\n    )\n\n    if self.emission_scenarios is not None:\n        anomaly = anomaly.sel(rcp=self.emission_scenarios)\n\n    if \"pulse_year\" in anomaly.dims:\n        anomaly = anomaly.sel(pulse_year=self.pulse_year, drop=True)\n\n    # Apply ECS mask\n    if (self.ecs_mask_name is not None) and (self.ecs_mask_path is not None):\n        self.logger.info(f\"Masking anomalies with {self.ecs_mask_name}.\")\n\n        # load mask\n        mask = xr.open_dataset(self.ecs_mask_path)[self.ecs_mask_name]\n\n        # median variables can't be masked because they don't have a simulation dimension\n        vars_no_mask = [v for v in anomaly.keys() if \"median\" in v]\n        vars_to_mask = [v for v in anomaly.keys() if v not in vars_no_mask]\n\n        # mask and put back together\n        anomaly = anomaly[vars_no_mask].update(\n            anomaly[vars_to_mask].where(mask, drop=True)\n        )\n\n    return anomaly\n</code></pre>"},{"location":"menu/simple_storage/#dscim.menu.simple_storage.Climate.anomaly_vars","title":"dscim.menu.simple_storage.Climate.anomaly_vars","text":"<pre><code>anomaly_vars()\n</code></pre> <p>Anomaly variables to include</p> Source code in <code>src/dscim/menu/simple_storage.py</code> <pre><code>@cachedproperty\ndef anomaly_vars(self):\n    \"\"\"Anomaly variables to include\"\"\"\n    return (\n        [\"temperature\", \"gmsl\"]\n        if self.gmsl_fair_path is not None\n        else [\"temperature\"]\n    )\n</code></pre>"},{"location":"menu/simple_storage/#dscim.menu.simple_storage.EconVars","title":"dscim.menu.simple_storage.EconVars","text":"<p>This class wraps all socioeconomic data used in DSCIM.</p> <p>Parameters:</p> Name Type Description Default <code>path_econ</code> <code>str</code> <p>Path to economic data in NetCDF format.</p> required Notes <p>Note that the input data must have population and GDP data at the IR level with the desired dimensions: SSP, IAM (or <code>model</code>), IR, and year.</p> <p>Attributes:</p> Name Type Description <code>econ_vars</code> <p>Economic variables</p> Source code in <code>src/dscim/menu/simple_storage.py</code> <pre><code>class EconVars:\n    \"\"\"\n    This class wraps all socioeconomic data used in DSCIM.\n\n    Parameters\n    ----------\n    path_econ : str\n        Path to economic data in NetCDF format.\n\n    Notes\n    ------\n    Note that the input data must have population and GDP data at the IR level\n    with the desired dimensions: SSP, IAM (or ``model``), IR, and year.\n    \"\"\"\n\n    def __init__(self, path_econ):\n        self.path = path_econ\n        self.logger = logging.getLogger(__name__)\n\n    @property\n    def econ_vars(self):\n        \"\"\"Economic variables\"\"\"\n        if self.path[-3:] == \"arr\":\n            raw = xr.open_zarr(self.path, consolidated=True)\n        else:\n            raw = xr.open_dataset(self.path)\n        return raw[[\"gdp\", \"pop\"]]\n</code></pre>"},{"location":"menu/simple_storage/#dscim.menu.simple_storage.EconVars.econ_vars","title":"dscim.menu.simple_storage.EconVars.econ_vars  <code>property</code>","text":"<pre><code>econ_vars\n</code></pre> <p>Economic variables</p>"},{"location":"menu/simple_storage/#dscim.menu.simple_storage.StackedDamages","title":"dscim.menu.simple_storage.StackedDamages","text":"<p>This class wraps all damages data used in DSCIM.</p> <p>Parameters:</p> Name Type Description Default <code>sector_path</code> <code>str</code> <p>Path to input damages.</p> required <code>delta</code> <code>str</code> <p>Climate change damages variable.</p> <code>None</code> <code>histclim</code> <code>str</code> <p>No climate change damages variable.</p> <code>None</code> <code>econ_vars</code> <code>EconVars</code> required <code>climate_vars</code> <code>Climate</code> required <code>subset_dict</code> <code>dict</code> <p>A dictionary with coordinate values to filter data.</p> <code>None</code> <code>eta</code> <code>int</code> <p>Curvature parameter of the CRRA utility function.</p> required <code>gdppc_bottom_code</code> <code>int or float</code> <p>Minimum values allowed for per-capita GDP in <code>self.gdppc</code>.</p> required <code>ce_path</code> <code>str</code> <p>Path to directory containing certainty equivalent reduced damages and risk aversion data. This directory can contain <code>adding_up_cc.zarr</code> and <code>adding_up_no_cc.zarr</code> which have reduced damages due to climate in dollars (by impact region, year, etc.) for the <code>adding_up</code> recipe with climate change (cc) and without climate change (no cc). This directory should also contain <code>risk_aversion_{ce_type}_eta{eta}.zarr</code> as used for risk aversion calculations.</p> <code>None</code> <p>Methods:</p> Name Description <code>cut</code> <p>Subset array to self.subset_dict.</p> <code>risk_aversion_damages</code> <p>This function calls pre-calculated risk-aversion IR-level 'CE' over batches.</p> <p>Attributes:</p> Name Type Description <code>adding_up_damages</code> <p>This property calls pre-calculated adding-up IR-level 'mean' over batches.</p> <code>cut_econ_vars</code> <p>Economic variables from SSP object</p> Source code in <code>src/dscim/menu/simple_storage.py</code> <pre><code>class StackedDamages:\n    \"\"\"\n    This class wraps all damages data used in DSCIM.\n\n    Parameters\n    ----------\n    sector_path : str\n        Path to input damages.\n    delta : str\n        Climate change damages variable.\n    histclim : str\n        No climate change damages variable.\n    econ_vars : dscim.simple_storage.EconVars\n    climate_vars : dscim.simple_storage.Climate\n    subset_dict : dict\n        A dictionary with coordinate values to filter data.\n    eta : int\n        Curvature parameter of the CRRA utility function.\n    gdppc_bottom_code : int or float\n        Minimum values allowed for per-capita GDP in ``self.gdppc``.\n    ce_path : str, optional\n        Path to directory containing certainty equivalent reduced damages and\n        risk aversion data. This directory can contain `adding_up_cc.zarr` and\n        `adding_up_no_cc.zarr` which have reduced damages due to climate in\n        dollars (by impact region, year, etc.) for the `adding_up` recipe with\n        climate change (cc) and without climate change (no cc).\n        This directory should also contain `risk_aversion_{ce_type}_eta{eta}.zarr`\n        as used for risk aversion calculations.\n    \"\"\"\n\n    NAME = \"\"\n\n    def __init__(\n        self,\n        sector_path,\n        save_path,\n        econ_vars,\n        climate_vars,\n        eta,\n        gdppc_bottom_code,\n        delta=None,\n        histclim=None,\n        ce_path=None,\n        subset_dict=None,\n    ):\n        self.sector_path = sector_path\n        self.save_path = save_path\n        self.gdppc_bottom_code = gdppc_bottom_code\n        self.subset_dict = subset_dict\n        self.econ_vars = econ_vars\n        self.climate = climate_vars\n        self.delta = delta\n        self.histclim = histclim\n        self.ce_path = ce_path\n        self.eta = eta\n\n        self.logger = logging.getLogger(__name__)\n\n    def cut(self, xr_array, end_year=2099):\n        \"\"\"Subset array to self.subset_dict.\n\n        Parameters\n        ----------\n        xr_array :  xr.Dataset or xr.Dataarray\n            An xarray object\n\n        end_year : int\n            Which year should be last in the dataset (all further data is dropped)\n\n        Returns\n        -------\n        xr.Dataset or xr.Dataarray\n            ``xarray`` object filtered using the dict defined in the class:\n            ``self.subset_dict``\n        \"\"\"\n\n        valid_keys = {\n            key: self.subset_dict[key]\n            for key in self.subset_dict\n            if key in xr_array.coords\n        }\n\n        self.logger.debug(f\"Subsetting on {valid_keys} keys.\")\n\n        xr_data = xr_array.sel(valid_keys).sel(\n            year=slice(self.climate.pulse_year - 2, end_year)\n        )\n\n        return xr_data\n\n    @property\n    def cut_econ_vars(self):\n        \"\"\"Economic variables from SSP object\"\"\"\n        if 2300 in self.econ_vars.econ_vars.year:\n            # because RFF data runs to 2300, these menu runs don't need to sliced and extrapolated\n            raw = self.cut(self.econ_vars.econ_vars, end_year=2300)\n        else:\n            # 2100 should be dropped from SSP data since CIL damages only extend to 2099\n            raw = self.cut(self.econ_vars.econ_vars, end_year=2099)\n\n        if raw is None:\n            raise ValueError(\n                \"Economic data is not loaded. Check your config or input settings.\"\n            )\n        return raw\n\n    @cachedproperty\n    def gdp(self):\n        return self.cut_econ_vars.gdp\n\n    @cachedproperty\n    def pop(self):\n        return self.cut_econ_vars.pop\n\n    @cachedproperty\n    def gdppc(self):\n        return np.maximum(self.gdp / self.pop, self.gdppc_bottom_code)\n\n    @property\n    def adding_up_damages(self):\n        \"\"\"This property calls pre-calculated adding-up IR-level 'mean' over batches.\"\"\"\n\n        mean_cc = f\"{self.ce_path}/adding_up_cc.zarr\"\n        mean_no_cc = f\"{self.ce_path}/adding_up_no_cc.zarr\"\n\n        if os.path.exists(mean_cc) and os.path.exists(mean_no_cc):\n            self.logger.info(\n                f\"Adding up aggregated damages found at {mean_cc}, {mean_no_cc}. These are being loaded...\"\n            )\n            damages = (\n                (xr.open_zarr(mean_no_cc).no_cc - xr.open_zarr(mean_cc).cc) * self.pop\n            ).sum(\"region\")\n        else:\n            raise NotImplementedError(\n                f\"Adding up reduced damages not found: {mean_no_cc}, {mean_cc}. Please reduce damages for for `adding_up`.\"\n            )\n        return self.cut(damages)\n\n    def risk_aversion_damages(self, ce_type):\n        \"\"\"This function calls pre-calculated risk-aversion IR-level 'CE' over batches.\n\n        Parameters\n        ----------\n        ce_type : either `no_cc` or `cc`\n\n        Returns\n        -------\n        xr.DataArray\n        \"\"\"\n        file = f\"{self.ce_path}/risk_aversion_{ce_type}_eta{self.eta}.zarr\"\n\n        if os.path.exists(file):\n            self.logger.info(\n                f\"Risk-aversion CEs found at {file}. These are being loaded...\"\n            )\n        else:\n            raise NotImplementedError(\n                \"Risk-aversion CEs not found. Please run CE_calculation.ipynb for `risk_aversion`.\"\n            )\n        return self.cut(xr.open_zarr(file))\n</code></pre>"},{"location":"menu/simple_storage/#dscim.menu.simple_storage.StackedDamages.adding_up_damages","title":"dscim.menu.simple_storage.StackedDamages.adding_up_damages  <code>property</code>","text":"<pre><code>adding_up_damages\n</code></pre> <p>This property calls pre-calculated adding-up IR-level 'mean' over batches.</p>"},{"location":"menu/simple_storage/#dscim.menu.simple_storage.StackedDamages.cut_econ_vars","title":"dscim.menu.simple_storage.StackedDamages.cut_econ_vars  <code>property</code>","text":"<pre><code>cut_econ_vars\n</code></pre> <p>Economic variables from SSP object</p>"},{"location":"menu/simple_storage/#dscim.menu.simple_storage.StackedDamages.cut","title":"dscim.menu.simple_storage.StackedDamages.cut","text":"<pre><code>cut(xr_array, end_year=2099)\n</code></pre> <p>Subset array to self.subset_dict.</p> <p>Parameters:</p> Name Type Description Default <code>xr_array</code> <code> xr.Dataset or xr.Dataarray</code> <p>An xarray object</p> required <code>end_year</code> <code>int</code> <p>Which year should be last in the dataset (all further data is dropped)</p> <code>2099</code> <p>Returns:</p> Type Description <code>Dataset or Dataarray</code> <p><code>xarray</code> object filtered using the dict defined in the class: <code>self.subset_dict</code></p> Source code in <code>src/dscim/menu/simple_storage.py</code> <pre><code>def cut(self, xr_array, end_year=2099):\n    \"\"\"Subset array to self.subset_dict.\n\n    Parameters\n    ----------\n    xr_array :  xr.Dataset or xr.Dataarray\n        An xarray object\n\n    end_year : int\n        Which year should be last in the dataset (all further data is dropped)\n\n    Returns\n    -------\n    xr.Dataset or xr.Dataarray\n        ``xarray`` object filtered using the dict defined in the class:\n        ``self.subset_dict``\n    \"\"\"\n\n    valid_keys = {\n        key: self.subset_dict[key]\n        for key in self.subset_dict\n        if key in xr_array.coords\n    }\n\n    self.logger.debug(f\"Subsetting on {valid_keys} keys.\")\n\n    xr_data = xr_array.sel(valid_keys).sel(\n        year=slice(self.climate.pulse_year - 2, end_year)\n    )\n\n    return xr_data\n</code></pre>"},{"location":"menu/simple_storage/#dscim.menu.simple_storage.StackedDamages.risk_aversion_damages","title":"dscim.menu.simple_storage.StackedDamages.risk_aversion_damages","text":"<pre><code>risk_aversion_damages(ce_type)\n</code></pre> <p>This function calls pre-calculated risk-aversion IR-level 'CE' over batches.</p> <p>Parameters:</p> Name Type Description Default <code>ce_type</code> <code>either `no_cc` or `cc`</code> required <p>Returns:</p> Type Description <code>DataArray</code> Source code in <code>src/dscim/menu/simple_storage.py</code> <pre><code>def risk_aversion_damages(self, ce_type):\n    \"\"\"This function calls pre-calculated risk-aversion IR-level 'CE' over batches.\n\n    Parameters\n    ----------\n    ce_type : either `no_cc` or `cc`\n\n    Returns\n    -------\n    xr.DataArray\n    \"\"\"\n    file = f\"{self.ce_path}/risk_aversion_{ce_type}_eta{self.eta}.zarr\"\n\n    if os.path.exists(file):\n        self.logger.info(\n            f\"Risk-aversion CEs found at {file}. These are being loaded...\"\n        )\n    else:\n        raise NotImplementedError(\n            \"Risk-aversion CEs not found. Please run CE_calculation.ipynb for `risk_aversion`.\"\n        )\n    return self.cut(xr.open_zarr(file))\n</code></pre>"},{"location":"preprocessing/input_damages/","title":"input_damages","text":"<p>Calculate damages from the projection system using VSL</p> <p>Functions:</p> Name Description <code>calculate_energy_impacts</code> <p>Calculate impacts for labor results for individual modeling unit.</p> <code>calculate_labor_impacts</code> <p>Calculate impacts for labor results.</p> <code>compute_ag_damages</code> <p>Reshapes ag estimate runs for use in integration system,</p> <code>concatenate_damage_output</code> <p>Concatenate labor/energy damage output across batches.</p> <code>concatenate_energy_damages</code> <p>Concatenate damages across batches and create a lazy array for future</p> <code>concatenate_labor_damages</code> <p>Concatenate damages across batches.</p> <code>read_energy_files</code> <p>Read energy CSV files and trasnform them to Xarray objects</p> <code>read_energy_files_parallel</code> <p>Concatenate energy results from CSV to NetCDF by batches using</p>"},{"location":"preprocessing/input_damages/#dscim.preprocessing.input_damages.calculate_energy_impacts","title":"dscim.preprocessing.input_damages.calculate_energy_impacts","text":"<pre><code>calculate_energy_impacts(input_path, file_prefix, variable)\n</code></pre> <p>Calculate impacts for labor results for individual modeling unit.</p> <p>Read in individual damages files from the labor projection system output and re-index to add region dimension. This is needed to adjust the projection file outcomes that do not have a region dimension</p> Paramemters <p>input_path str     Path to model/gcm/iam/rcp/ folder, usually from the     <code>_parse_projection_filesys</code> function. file_prefix str     Prefix of the MC output filenames variable str     Variable to use within <code>xr.Dataset</code></p> <p>Returns:</p> Type Description <code>    xr.Dataset object with per-capita monetary damages</code> Source code in <code>src/dscim/preprocessing/input_damages.py</code> <pre><code>def calculate_energy_impacts(input_path, file_prefix, variable):\n    \"\"\"Calculate impacts for labor results for individual modeling unit.\n\n    Read in individual damages files from the labor projection system output\n    and re-index to add region dimension. This is needed to adjust the\n    projection file outcomes that do not have a region dimension\n\n    Paramemters\n    ----------\n    input_path str\n        Path to model/gcm/iam/rcp/ folder, usually from the\n        `_parse_projection_filesys` function.\n    file_prefix str\n        Prefix of the MC output filenames\n    variable str\n        Variable to use within `xr.Dataset`\n\n    Returns\n    -------\n        xr.Dataset object with per-capita monetary damages\n    \"\"\"\n\n    damages_delta = xr.open_dataset(f\"{input_path}/{file_prefix}_delta.nc4\").sel(\n        year=slice(2010, 2099)\n    )\n    damages_histclim = xr.open_dataset(f\"{input_path}/{file_prefix}_histclim.nc4\").sel(\n        year=slice(2010, 2099)\n    )\n\n    # generate the histclim variable\n    impacts = damages_histclim[variable].to_dataset(name=f\"histclim_{variable}\")\n    # generate the delta variable\n    impacts[f\"delta_{variable}\"] = damages_delta[variable]\n\n    return impacts\n</code></pre>"},{"location":"preprocessing/input_damages/#dscim.preprocessing.input_damages.calculate_labor_impacts","title":"dscim.preprocessing.input_damages.calculate_labor_impacts","text":"<pre><code>calculate_labor_impacts(input_path, file_prefix, variable, val_type)\n</code></pre> <p>Calculate impacts for labor results.</p> Paramemters <p>input_path str     Path to model/gcm/iam/rcp/ folder, usually from the     <code>_parse_projection_filesys</code> function. file_prefix str     Prefix of the MC output filenames variable str     Variable to use within <code>xr.Dataset</code> val_type str     Valuation type.</p> <p>Returns:</p> Type Description <code>    xr.Dataset object with per-capita monetary damages</code> Source code in <code>src/dscim/preprocessing/input_damages.py</code> <pre><code>def calculate_labor_impacts(input_path, file_prefix, variable, val_type):\n    \"\"\"Calculate impacts for labor results.\n\n    Paramemters\n    ----------\n    input_path str\n        Path to model/gcm/iam/rcp/ folder, usually from the\n        `_parse_projection_filesys` function.\n    file_prefix str\n        Prefix of the MC output filenames\n    variable str\n        Variable to use within `xr.Dataset`\n    val_type str\n        Valuation type.\n\n    Returns\n    -------\n        xr.Dataset object with per-capita monetary damages\n    \"\"\"\n\n    damages_val = xr.open_dataset(f\"{input_path}/{file_prefix}-{val_type}.nc4\").sel(\n        year=slice(2010, 2099)\n    )\n    damages_histclim = xr.open_dataset(\n        f\"{input_path}/{file_prefix}-histclim-{val_type}.nc4\"\n    ).sel(year=slice(2010, 2099))\n\n    # labour needs indexing assigned\n    damages_val_index = damages_val.assign_coords({\"region\": damages_val.regions})\n\n    damages_histclim_index = damages_histclim.assign_coords(\n        {\"region\": damages_histclim.regions}\n    )\n\n    # calculate the delta output\n    impacts_delta = damages_val_index[variable] - damages_histclim_index[variable]\n\n    # generate the histclim variable\n    impacts = damages_histclim_index[variable].to_dataset(name=f\"histclim_{variable}\")\n    # generate the delta variable\n    impacts[f\"delta_{variable}\"] = impacts_delta\n\n    return impacts\n</code></pre>"},{"location":"preprocessing/input_damages/#dscim.preprocessing.input_damages.compute_ag_damages","title":"dscim.preprocessing.input_damages.compute_ag_damages","text":"<pre><code>compute_ag_damages(input_path, save_path, pop, varname, query='exists==True', topcode=None, scalar=None, integration=False, batches=range(0, 15), num_cpus=15, file='/disaggregated_damages.nc4', vars=None, min_year=2010, max_year=2099)\n</code></pre> <p>Reshapes ag estimate runs for use in integration system, then converts to negative per capita damages.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <p>Path to NetCDF4 files to be reshaped.</p> required <code>file</code> <p>Name of files to be globbed.</p> <code>'/disaggregated_damages.nc4'</code> <code>integration</code> <p>If True, will format files to be integrated with other sectors.</p> <code>False</code> <code>pop</code> <p>Population data to convert ag damages into per capita terms.</p> required <code>save_path</code> <p>Path where files should be saved</p> required Source code in <code>src/dscim/preprocessing/input_damages.py</code> <pre><code>def compute_ag_damages(\n    input_path,\n    save_path,\n    pop,\n    varname,\n    query=\"exists==True\",\n    topcode=None,\n    scalar=None,\n    integration=False,\n    batches=range(0, 15),\n    num_cpus=15,\n    file=\"/disaggregated_damages.nc4\",\n    vars=None,\n    min_year=2010,\n    max_year=2099,\n):\n    \"\"\"\n    Reshapes ag estimate runs for use in integration system,\n    then converts to negative per capita damages.\n\n    Parameters\n    ----------\n    input_path str or list\n        Path to NetCDF4 files to be reshaped.\n    file str\n        Name of files to be globbed.\n    integration bool\n        If True, will format files to be integrated with other sectors.\n    pop xr.DataArray\n        Population data to convert ag damages into per capita terms.\n    save_path str\n        Path where files should be saved\n    \"\"\"\n    if vars is None:\n        vars = [\"wc_no_reallocation\"]\n\n    if integration:\n        assert (\n            topcode is not None\n        ), \"Data is being processed for integration. Please pass a topcode.\"\n\n    if isinstance(input_path, str):\n        input_path = [input_path]\n\n    paths = []\n    for f in input_path:\n        p = _parse_projection_filesys(input_path=f, query=query)\n        paths.append(p)\n\n    paths = pd.concat(paths)\n    paths[\"path\"] = paths.path + file\n\n    def prep(ds):\n        ds.coords[\"model\"] = ds.model.str.replace(\"high\", \"OECD Env-Growth\")\n        ds.coords[\"model\"] = ds.model.str.replace(\"low\", \"IIASA GDP\")\n        return ds\n\n    def process_batch(g):\n        i = g.batch.values[0]\n        if i in [f\"batch{j}\" for j in batches]:\n            print(f\"Processing damages in {i}\")\n            ds = xr.open_mfdataset(g.path, preprocess=prep, parallel=True)[vars]\n            attrs = ds.attrs\n            ds = ds.sel({\"year\": slice(min_year, max_year)})\n            # ag has missing 2099 damages so we fill these in with the 2098 damages\n            ds = ds.reindex(year=range(min_year, max_year + 1), method=\"ffill\")\n\n            # squeeze ag-specific dimensions out so that\n            # it can be stacked with other sectors\n            if integration:\n                ds = ds.sel(demand_topcode=topcode)\n                print(f\"Selecting topcode {topcode}.\")\n                for var in [\n                    \"continent\",\n                    \"iam\",\n                    \"Es_Ed\",\n                    \"market_level\",\n                    \"demand_topcode\",\n                ]:\n                    if var in ds.coords:\n                        attrs[var] = ds[var].values\n                        ds = ds.drop(var)\n\n            # get in per capita 2019 PPP-adjusted USD damages\n            ds = (ds / pop.load()) * -1 * 1.273526\n\n            # replace infinite values with missings\n            for var in ds.keys():\n                ds[var] = xr.where(np.isinf(ds[var]), np.nan, ds[var])\n\n            if scalar is not None:\n                print(\"Scaling for reallocation.\")\n                ds[\"wc_reallocation\"] = ds[\"wc_no_reallocation\"] * scalar\n\n            ds.attrs = attrs\n            return ds\n        else:\n            print(f\"Skipped {i}.\")\n\n    batches = p_map(\n        process_batch, [g for i, g in paths.groupby(\"batch\")], num_cpus=num_cpus\n    )\n    batches = [ds for ds in batches if ds is not None]\n    chunkies = {\n        \"rcp\": 1,\n        \"region\": -1,\n        \"gcm\": 1,\n        \"year\": 10,\n        \"model\": 1,\n        \"ssp\": 1,\n        \"batch\": 15,\n    }\n    batches = (\n        xr.concat(batches, \"batch\", combine_attrs=\"override\")\n        .chunk(chunkies)\n        .drop(\"variable\")\n        .squeeze()\n    )\n    batches = xr.where(np.isinf(batches), np.nan, batches)\n    batches = batches.astype(np.float32)\n\n    batches.rename({\"wc_reallocation\": varname})[varname].to_dataset().to_zarr(\n        store=save_path, mode=\"a\", consolidated=True\n    )\n</code></pre>"},{"location":"preprocessing/input_damages/#dscim.preprocessing.input_damages.concatenate_damage_output","title":"dscim.preprocessing.input_damages.concatenate_damage_output","text":"<pre><code>concatenate_damage_output(damage_dir, basename, save_path)\n</code></pre> <p>Concatenate labor/energy damage output across batches.</p> <p>Parameters:</p> Name Type Description Default <code>damage_dir</code> <p>Directory containing separate labor/energy damage output files by batches.</p> required <code>basename</code> <p>Prefix of the damage output filenames (ex. {basename}_batch0.zarr)</p> required <code>save_path</code> <p>Path to save concatenated file in .zarr format</p> required Source code in <code>src/dscim/preprocessing/input_damages.py</code> <pre><code>def concatenate_damage_output(damage_dir, basename, save_path):\n    \"\"\"Concatenate labor/energy damage output across batches.\n\n    Parameters\n    ----------\n    damage_dir str\n        Directory containing separate labor/energy damage output files by batches.\n    basename str\n        Prefix of the damage output filenames (ex. {basename}_batch0.zarr)\n    save_path str\n        Path to save concatenated file in .zarr format\n    \"\"\"\n    paths = [\n        f\"{damage_dir}/{basename}_{b}.zarr\"\n        for b in [\"batch\" + str(i) for i in range(0, 15)]\n    ]\n    data = xr.open_mfdataset(paths=paths, engine=\"zarr\")\n\n    for v in data:\n        del data[v].encoding[\"chunks\"]\n\n    chunkies = {\n        \"batch\": 15,\n        \"rcp\": 1,\n        \"gcm\": 1,\n        \"model\": 1,\n        \"ssp\": 1,\n        \"region\": -1,\n        \"year\": 10,\n    }\n\n    data = data.chunk(chunkies)\n\n    for v in list(data.coords.keys()):\n        if data.coords[v].dtype == object:\n            data.coords[v] = data.coords[v].astype(\"unicode\")\n    data.coords[\"batch\"] = data.coords[\"batch\"].astype(\"unicode\")\n    for v in list(data.variables.keys()):\n        if data[v].dtype == object:\n            data[v] = data[v].astype(\"unicode\")\n\n    data.to_zarr(save_path, mode=\"w\")\n</code></pre>"},{"location":"preprocessing/input_damages/#dscim.preprocessing.input_damages.concatenate_energy_damages","title":"dscim.preprocessing.input_damages.concatenate_energy_damages","text":"<pre><code>concatenate_energy_damages(input_path, save_path, ec_cls, file_prefix='TINV_clim_integration_total_energy', variable='rebased', format_file='netcdf', **kwargs)\n</code></pre> <p>Concatenate damages across batches and create a lazy array for future calculations.</p> <p>Using the <code>value_mortality_damages</code> function this function lazily loads all damages for SCC calculations and scale damage to per-capital damages and also scale labor inpacts to indicate increasing damages as positive, and gains from warming as negative.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <p>Directory containing all raw projection output.</p> required <code>ec_cls</code> <p>EconVars class with population and GDP data to rescale damages</p> required <code>save_path</code> <p>Path to save concatenated file in .zarr or .nc4 format</p> required <code>file_prefix</code> <p>Prefix of the MC output filenames</p> <code>'TINV_clim_integration_total_energy'</code> <code>variables</code> <p>Variable names to extract from calculated damages</p> required <code>format_file</code> <p>Format to save file. Options are 'netcdf' or 'zarr'</p> <code>'netcdf'</code> <code>**kwargs</code> <p>Other options passed to the <code>value_mortality_damages</code> function and the <code>_parse_projection_filesys</code>: query=\"rcp=='rcp45'\"</p> <code>{}</code> Source code in <code>src/dscim/preprocessing/input_damages.py</code> <pre><code>def concatenate_energy_damages(\n    input_path,\n    save_path,\n    ec_cls,\n    file_prefix=\"TINV_clim_integration_total_energy\",\n    variable=\"rebased\",\n    format_file=\"netcdf\",\n    **kwargs,\n):\n    \"\"\"Concatenate damages across batches and create a lazy array for future\n    calculations.\n\n    Using the `value_mortality_damages` function this function lazily loads all\n    damages for SCC calculations and scale damage to per-capital damages and\n    also scale labor inpacts to indicate increasing damages as positive, and\n    gains from warming as negative.\n\n    Parameters\n    ----------\n    input_path str\n        Directory containing all raw projection output.\n    ec_cls dscim.simple_storage.EconVars\n        EconVars class with population and GDP data to rescale damages\n    save_path str\n        Path to save concatenated file in .zarr or .nc4 format\n    file_prefix str\n        Prefix of the MC output filenames\n    variables list\n        Variable names to extract from calculated damages\n    format_file str\n        Format to save file. Options are 'netcdf' or 'zarr'\n    **kwargs\n        Other options passed to the `value_mortality_damages` function\n        and the `_parse_projection_filesys`: query=\"rcp=='rcp45'\"\n    \"\"\"\n\n    # Load df with paths\n    df = _parse_projection_filesys(\n        input_path=input_path, query=kwargs.get(\"query\", \"exists==True\")\n    )\n\n    # Process files by batches and save as .zarr files\n    for i, g in df.groupby(\"batch\"):\n        logger.info(f\"Processing damages in batch: {i}\")\n        list_damages_batch = []\n        for idx, row in g.iterrows():\n            try:\n                ds = calculate_energy_impacts(\n                    input_path=row.path,\n                    file_prefix=file_prefix,\n                    variable=variable,\n                )\n                list_damages_batch.append(ds)\n\n            except Exception as e:\n                logger.error(f\"Error in batch{i}: {e}\")\n                pass\n\n        # Concatenate file within batch\n        conversion_value = 1.273526\n        concat_ds = xr.combine_by_coords(list_damages_batch)\n        for v in [f\"histclim_{variable}\", f\"delta_{variable}\"]:\n            concat_ds[v] = (\n                concat_ds[v] / ec_cls.econ_vars.pop.load()\n            ) * conversion_value\n\n        # Save file\n        file_name = f\"{variable}_{i}\"\n        path_to_file = os.path.join(save_path, file_name)\n\n        # convert to float32\n        concat_ds = concat_ds.astype(np.float32)\n        logger.info(f\"Concatenating and processing {i}\")\n\n        if format_file == \"zarr\":\n            to_store = concat_ds.copy()\n            for var in to_store.variables:\n                to_store[var].encoding.clear()\n\n            to_store.to_zarr(f\"{path_to_file}.zarr\", mode=\"w\", consolidated=True)\n        elif format_file == \"netcdf\":\n            to_store = concat_ds.copy()\n            for var in to_store.variables:\n                to_store[var].encoding.clear()\n\n            to_store.to_netcdf(f\"{path_to_file}.nc4\")\n\n    return concat_ds\n</code></pre>"},{"location":"preprocessing/input_damages/#dscim.preprocessing.input_damages.concatenate_labor_damages","title":"dscim.preprocessing.input_damages.concatenate_labor_damages","text":"<pre><code>concatenate_labor_damages(input_path, save_path, ec_cls, file_prefix='uninteracted_main_model', variable='rebased', val_type='wage-levels', format_file='netcdf', **kwargs)\n</code></pre> <p>Concatenate damages across batches.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <p>Directory containing all raw projection output.</p> required <code>ec_cls</code> <p>EconVars class with population and GDP data to rescale damages</p> required <code>save_path</code> <p>Path to save concatenated file in .zarr or .nc4 format</p> required <code>file_prefix</code> <p>Prefix of the MC output filenames</p> <code>'uninteracted_main_model'</code> <code>variables</code> <p>Variable names to extract from calculated damages</p> required <code>format_file</code> <p>Format to save file. Options are 'netcdf' or 'zarr'</p> <code>'netcdf'</code> <code>**kwargs</code> <p>Other options passed to the <code>_parse_projection_filesys</code>: query=\"rcp=='rcp45'\"</p> <code>{}</code> Source code in <code>src/dscim/preprocessing/input_damages.py</code> <pre><code>def concatenate_labor_damages(\n    input_path,\n    save_path,\n    ec_cls,\n    file_prefix=\"uninteracted_main_model\",\n    variable=\"rebased\",\n    val_type=\"wage-levels\",\n    format_file=\"netcdf\",\n    **kwargs,\n):\n    \"\"\"Concatenate damages across batches.\n\n    Parameters\n    ----------\n    input_path str\n        Directory containing all raw projection output.\n    ec_cls dscim.simple_storage.EconVars\n        EconVars class with population and GDP data to rescale damages\n    save_path str\n        Path to save concatenated file in .zarr or .nc4 format\n    file_prefix str\n        Prefix of the MC output filenames\n    variables list\n        Variable names to extract from calculated damages\n    format_file str\n        Format to save file. Options are 'netcdf' or 'zarr'\n    **kwargs\n        Other options passed to the `_parse_projection_filesys`: query=\"rcp=='rcp45'\"\n    \"\"\"\n\n    # Load df with paths\n    df = _parse_projection_filesys(\n        input_path=input_path, query=kwargs.get(\"query\", \"exists==True\")\n    )\n\n    # Process files by batches and save as .zarr files\n    for i, g in df.groupby(\"batch\"):\n        logger.info(f\"Processing damages in batch: {i}\")\n        list_damages_batch = []\n        for idx, row in g.iterrows():\n            try:\n                ds = calculate_labor_impacts(\n                    input_path=row.path,\n                    file_prefix=file_prefix,\n                    variable=variable,\n                    val_type=val_type,\n                )\n                ds = ds.assign_coords(\n                    {\n                        \"ssp\": row.ssp,\n                        \"rcp\": row.rcp,\n                        \"gcm\": row.gcm,\n                        \"model\": row.iam,\n                        \"batch\": row.batch,\n                    }\n                )\n\n                ds_exp = ds.expand_dims([\"ssp\", \"rcp\", \"model\", \"gcm\", \"batch\"])\n                list_damages_batch.append(ds_exp)\n\n            except Exception as e:\n                logger.error(f\"Error in batch{i}: {e}\")\n                pass\n\n        # Concatenate file within batch\n        conversion_value = 1.273526\n        concat_ds = xr.combine_by_coords(list_damages_batch)\n        for v in [f\"histclim_{variable}\", f\"delta_{variable}\"]:\n            concat_ds[v] = (\n                (concat_ds[v] / ec_cls.econ_vars.pop.load()) * -1 * conversion_value\n            )\n\n        # Save file\n        file_name = f\"{variable}_{val_type}_{i}\"\n        path_to_file = os.path.join(save_path, file_name)\n\n        # convert to float32\n        concat_ds = concat_ds.astype(np.float32)\n        logger.info(f\"Concatenating and processing {i}\")\n\n        # save out\n        if format_file == \"zarr\":\n            to_store = concat_ds.copy()\n            for var in to_store.variables:\n                to_store[var].encoding.clear()\n\n            to_store.to_zarr(f\"{path_to_file}.zarr\", mode=\"w\", consolidated=True)\n        elif format_file == \"netcdf\":\n            concat_ds.to_netcdf(f\"{path_to_file}.nc4\")\n\n    return concat_ds\n</code></pre>"},{"location":"preprocessing/input_damages/#dscim.preprocessing.input_damages.read_energy_files","title":"dscim.preprocessing.input_damages.read_energy_files","text":"<pre><code>read_energy_files(df, seed='TINV_clim_price014_total_energy_fulladapt-histclim')\n</code></pre> <p>Read energy CSV files and trasnform them to Xarray objects</p> <p>This function reads a dataframe with the filesystem metadata (from <code>_parse_projection_filesys</code>) to read all CSV files in it with the desired <code>seed</code> and transform to xarray object adding the directory metadata as new dimensions, this will be helpful for data concatenation.</p> <p>This function is parallelized by <code>read_energy_files_parallel</code></p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with projection system metadata by batch/RCP/IAM/GCM</p> required <p>Returns:</p> Type Description <code>None</code> <p>Saved data array with expanded damages from original CSV</p> Source code in <code>src/dscim/preprocessing/input_damages.py</code> <pre><code>def read_energy_files(df, seed=\"TINV_clim_price014_total_energy_fulladapt-histclim\"):\n    \"\"\"Read energy CSV files and trasnform them to Xarray objects\n\n    This function reads a dataframe with the filesystem metadata (from\n    ``_parse_projection_filesys``) to read all CSV files in it with the desired\n    ``seed`` and transform to xarray object adding the directory metadata as\n    new dimensions, this will be helpful for data concatenation.\n\n    This function is parallelized by ``read_energy_files_parallel``\n\n    Parameters\n    ---------\n    df : pd.DataFrame\n        DataFrame with projection system metadata by batch/RCP/IAM/GCM\n\n    Returns\n    -------\n    None\n        Saved data array with expanded damages from original CSV\n    \"\"\"\n\n    for idx, row in df.iterrows():\n        path = os.path.join(row.path, f\"{seed}.csv\")\n        try:\n            damages = pd.read_csv(path)\n            damages = damages[damages.year &gt;= 2010]\n            damages_arr = damages.set_index([\"region\", \"year\"]).to_xarray()\n\n            # Add dims to array\n            damages_arr = damages_arr.expand_dims(\n                {\n                    \"batch\": [row.batch],\n                    \"rcp\": [row.rcp],\n                    \"gcm\": [row.gcm],\n                    \"model\": [row.iam],\n                    \"ssp\": [row.ssp],\n                }\n            )\n\n        except Exception as e:\n            logger.error(f\"Error in file {row.path}: {e}\")\n            pass\n\n        damages_arr.to_netcdf(os.path.join(row.path, f\"{seed}.nc4\"))\n\n    return None\n</code></pre>"},{"location":"preprocessing/input_damages/#dscim.preprocessing.input_damages.read_energy_files_parallel","title":"dscim.preprocessing.input_damages.read_energy_files_parallel","text":"<pre><code>read_energy_files_parallel(input_path, **kwargs)\n</code></pre> <p>Concatenate energy results from CSV to NetCDF by batches using multiprocessing</p> <p>This function takes all CSV files per batch and maps the <code>read_energy_files</code> function to all the files within a batch. The files will be saved in the same path as the CSV files but in NetCDF format.</p> <p>Once saved, the files will be read again, using a Dask <code>Client</code> and chunked to be finally saved as files per batch. Before saving, the function will calculate damages per capita using SSP populations for each scenario with a <code>EconVars</code> class and then calculate 2019 USD damages</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to root folder organized by batch containing all projection system files</p> required <code>**kwargs</code> <p>Other elements too the <code>read_energy_files</code> damages</p> <code>{}</code> <p>Returns:</p> Type Description <code>    None</code> Source code in <code>src/dscim/preprocessing/input_damages.py</code> <pre><code>def read_energy_files_parallel(input_path, **kwargs):\n    \"\"\"Concatenate energy results from CSV to NetCDF by batches using\n    multiprocessing\n\n    This function takes all CSV files per batch and maps the\n    ``read_energy_files`` function to all the files within a batch. The files\n    will be saved in the same path as the CSV files but in NetCDF format.\n\n    Once saved, the files will be read again, using a Dask ``Client`` and\n    chunked to be finally saved as files per batch. Before saving, the function\n    will calculate damages per capita using SSP populations for each scenario\n    with a ``EconVars`` class and then calculate 2019 USD damages\n\n    Parameters\n    ----------\n    input_path : str\n        Path to root folder organized by batch containing all projection system\n        files\n    **kwargs\n        Other elements too the ``read_energy_files`` damages\n\n    Returns\n    ------\n        None\n    \"\"\"\n\n    # Start reading and save to NetCDF\n    for i in range(0, 15):\n        logger.info(f\"Processing damages in batch: {i}\")\n        # Read files available\n        df = _parse_projection_filesys(\n            input_path=input_path, query=f\"exists==True&amp;batch=='batch{i}'\"\n        )\n        # Calculate the chunk size as an integer\n        num_processes = multiprocessing.cpu_count()\n        chunk_size = int(df.shape[0] / num_processes)\n        logger.info(\n            f\"Starting multiprocessing in {num_processes} cores with {chunk_size} rows each\"\n        )\n        chunks = [\n            df.iloc[i : i + chunk_size, :] for i in range(0, df.shape[0], chunk_size)\n        ]\n\n        with multiprocessing.Pool(processes=num_processes) as p:\n            result = p.map(partial(read_energy_files, **kwargs), chunks)\n\n    return result\n</code></pre>"},{"location":"preprocessing/midprocessing/","title":"midprocessing","text":""},{"location":"preprocessing/preprocessing/","title":"preprocessing","text":"<p>Functions:</p> Name Description <code>clip_damages</code> <p>This function is no longer in use.</p>"},{"location":"preprocessing/preprocessing/#dscim.preprocessing.preprocessing.clip_damages","title":"dscim.preprocessing.preprocessing.clip_damages","text":"<pre><code>clip_damages(config, sector, econ_path='/shares/gcp/integration/float32/dscim_input_data/econvars/zarrs/integration-econ-bc39.zarr')\n</code></pre> <p>This function is no longer in use. To operationalize, make sure to get a Dask client running with the following code:</p>"},{"location":"preprocessing/preprocessing/#dscim.preprocessing.preprocessing.clip_damages--set-up-dask","title":"set up dask","text":"<p>dask.config.set(     {         \"distributed.worker.memory.target\": 0.7,         \"distributed.worker.memory.spill\": 0.8,         \"distributed.worker.memory.pause\": 0.9,     } )</p> <p>client = Client(n_workers=40, memory_limit=\"9G\", threads_per_worker=1)</p> Source code in <code>src/dscim/preprocessing/preprocessing.py</code> <pre><code>def clip_damages(\n    config,\n    sector,\n    econ_path=\"/shares/gcp/integration/float32/dscim_input_data/econvars/zarrs/integration-econ-bc39.zarr\",\n):\n    \"\"\"This function is no longer in use.\n    To operationalize, make sure to get a Dask client running with the following code:\n\n    # set up dask\n    dask.config.set(\n        {\n            \"distributed.worker.memory.target\": 0.7,\n            \"distributed.worker.memory.spill\": 0.8,\n            \"distributed.worker.memory.pause\": 0.9,\n        }\n    )\n\n    client = Client(n_workers=40, memory_limit=\"9G\", threads_per_worker=1)\n    \"\"\"\n\n    # load config\n    with open(config) as stream:\n        loaded_config = yaml.safe_load(stream)\n        params = loaded_config[\"sectors\"][sector]\n\n    # get sector paths and variable names\n    sector_path = Path(params[\"sector_path\"])\n    histclim = params[\"histclim\"]\n    delta = params[\"delta\"]\n\n    with xr.open_zarr(sector_path, chunks=None)[delta] as ds:\n        with xr.open_zarr(econ_path, chunks=None) as gdppc:\n            ce_batch_dims = [i for i in ds.dims]\n            ce_batch_coords = {c: ds[c].values for c in ce_batch_dims}\n            ce_batch_coords[\"region\"] = [\n                i for i in ds.region.values if i in gdppc.region.values\n            ]\n            ce_shapes = [len(ce_batch_coords[c]) for c in ce_batch_dims]\n            ce_chunks = [xr.open_zarr(sector_path).chunks[c][0] for c in ce_batch_dims]\n            print(ce_chunks)\n\n    template = xr.DataArray(\n        da.empty(ce_shapes, chunks=ce_chunks),\n        dims=ce_batch_dims,\n        coords=ce_batch_coords,\n    )\n\n    def chunk_func(\n        damages,\n    ):\n        year = damages.year.values\n        ssp = damages.ssp.values\n        model = damages.model.values\n        region = damages.region.values\n\n        gdppc = (\n            xr.open_zarr(econ_path, chunks=None)\n            .sel(year=year, ssp=ssp, model=model, region=region, drop=True)\n            .gdppc\n        )\n\n        # get damages as % of GDPpc\n        shares = damages / gdppc\n\n        # find the 1st/99th percentile of damage share\n        # across batches and regions\n        quantile = shares.quantile([0.01, 0.99], [\"batch\", \"region\"])\n\n        # find the equivalent damages\n        # if damage share is capped to 1st/99th percentile\n        quantdams = quantile * gdppc\n\n        # keep damages that are within cutoff,\n        # otherwise replace with capped damages\n        damages = xr.where(\n            (shares &lt;= quantile.sel(quantile=0.99, drop=True)),\n            damages,\n            quantdams.sel(quantile=0.99, drop=True),\n        )\n\n        damages = xr.where(\n            (shares &gt;= quantile.sel(quantile=0.01, drop=True)),\n            damages,\n            quantdams.sel(quantile=0.01, drop=True),\n        )\n\n        return damages\n\n    data = xr.open_zarr(sector_path)\n\n    for var in [delta, histclim]:\n        out = (\n            data[var].map_blocks(chunk_func, template=template).rename(var).to_dataset()\n        )\n\n        parent, name = sector_path.parent, sector_path.name\n        clipped_name = name.replace(\".zarr\", \"_clipped.zarr\")\n        outpath = Path(parent).joinpath(clipped_name)\n\n        out.to_zarr(outpath, mode=\"a\", consolidated=True)\n</code></pre>"},{"location":"utils/functions/","title":"functions","text":"<p>Functions:</p> Name Description <code>ce_func</code> <p>Calculate CRRA function</p> <code>mean_func</code> <p>Calculate a mean</p>"},{"location":"utils/functions/#dscim.utils.functions.ce_func","title":"dscim.utils.functions.ce_func","text":"<pre><code>ce_func(consumption, dims, eta)\n</code></pre> <p>Calculate CRRA function</p> Source code in <code>src/dscim/utils/functions.py</code> <pre><code>def ce_func(consumption, dims, eta):\n    \"\"\"Calculate CRRA function\"\"\"\n    # use log utility when eta is 1\n    if eta == 1:\n        return np.exp(np.log(consumption).mean(dims))\n    # CRRA utility otherwise\n    else:\n        return power(\n            (power(consumption, (1 - eta)) / (1 - eta)).mean(dims) * (1 - eta),\n            (1 / (1 - eta)),\n        )\n</code></pre>"},{"location":"utils/functions/#dscim.utils.functions.mean_func","title":"dscim.utils.functions.mean_func","text":"<pre><code>mean_func(consumption, dims)\n</code></pre> <p>Calculate a mean</p> Source code in <code>src/dscim/utils/functions.py</code> <pre><code>def mean_func(consumption, dims):\n    \"\"\"Calculate a mean\"\"\"\n    return consumption.mean(dims)\n</code></pre>"},{"location":"utils/menu_runs/","title":"menu_runs","text":""},{"location":"utils/rff/","title":"rff","text":"<p>Functions:</p> Name Description <code>aggregate_rff_weights</code> <p>Wrapper function for <code>clean_simulation()</code> and <code>clean_error()</code>.</p> <code>clean_error</code> <p>Clean the weights csvs generated by the RFF emulator.</p> <code>clean_simulation</code> <p>Clean the weights csvs generated by the RFF emulator.</p> <code>prep_rff_socioeconomics</code> <p>Generate the global or domestic RFF socioeconomics file for use with the <code>dscim</code> MainRecipe.</p> <code>process_rff_sample</code> <p>Clean raw socioeconomic projections from a single RFF-SP simulation run,</p> <code>process_ssp_sample</code> <p>Clean SSP per capita GDP projections</p> <code>rff_damage_functions</code> <p>Wrapper function for <code>weight_df()</code>.</p> <code>solve_optimization</code> <p>Generate weights based on which to derive the weighted average of damage function coefficents</p> <code>weight_df</code> <p>Weight, fractionalize, and combine SSP damage functions,</p>"},{"location":"utils/rff/#dscim.utils.rff.aggregate_rff_weights","title":"dscim.utils.rff.aggregate_rff_weights","text":"<pre><code>aggregate_rff_weights(root, output)\n</code></pre> <p>Wrapper function for <code>clean_simulation()</code> and <code>clean_error()</code>. Generates an aggregated file of RFF emulator weights.</p> Source code in <code>src/dscim/utils/rff.py</code> <pre><code>def aggregate_rff_weights(\n    root,\n    output,\n):\n    \"\"\"Wrapper function for `clean_simulation()` and `clean_error()`.\n    Generates an aggregated file of RFF emulator weights.\n    \"\"\"\n\n    # clean simulation files\n    datasets = p_map(partial(clean_simulation, root=root), range(1, 10001, 1))\n\n    # concatenate and interpolate\n    concatenated = xr.concat(datasets, \"rff_sp\").interp(\n        {\"year\": range(2010, 2101, 1)}, method=\"linear\"\n    )\n\n    # reweight\n    reweighted = concatenated / concatenated.sum([\"model\", \"ssp\"])\n\n    # make sure weights sum to 1\n    assert_allclose(reweighted.sum([\"model\", \"ssp\"]).values, 1)\n\n    # describe and save file\n    reweighted = reweighted.to_dataset()\n    reweighted.attrs[\"version\"] = 3\n    reweighted.attrs[\"description\"] = \"\"\"\n    This set of emulator weights is generated using this script:\n    dscim/dscim/utils/rff.py -&gt; aggregate_rff_weights\n    It cleans and aggregates the emulator weights csvs, linearly interpolates them between 5 year intervals, reweights them to sum to 1, and converts to ncdf4 format.\n    \"\"\"\n    os.makedirs(output, exist_ok=True)\n    reweighted.to_netcdf(f\"{output}/damage_function_weights.nc4\")\n\n    # save out error files\n\n    error_datasets = p_map(partial(clean_error, root=root), range(1, 10001, 1))\n    error_concatenated = xr.concat(error_datasets, \"rff_sp\")\n\n    # describe and save file\n    error_concatenated = error_concatenated.to_dataset()\n    error_concatenated.attrs[\"version\"] = 3\n    error_concatenated.attrs[\"description\"] = \"\"\"\n    This set of emulator weight errors is generated using this script:\n    dscim/dscim/preprocessing/rff/aggregate_rff_weights.py\n    It cleans and aggregates the emulator weights csvs for error rows only, and converts to ncdf4 format.\n    \"\"\"\n\n    error_concatenated.to_netcdf(f\"{output}/weights_errors.nc4\")\n</code></pre>"},{"location":"utils/rff/#dscim.utils.rff.clean_error","title":"dscim.utils.rff.clean_error","text":"<pre><code>clean_error(draw, root)\n</code></pre> <p>Clean the weights csvs generated by the RFF emulator. This produces a file of the errors (for diagnostics).</p> <p>Parameters:</p> Name Type Description Default <code>draw</code> <code>int, weight draw</code> required <code>root</code> <code>str, root directory</code> required Source code in <code>src/dscim/utils/rff.py</code> <pre><code>def clean_error(\n    draw,\n    root,\n):\n    \"\"\"\n    Clean the weights csvs generated by the RFF emulator.\n    This produces a file of the errors (for diagnostics).\n\n    Parameters\n    ----------\n    draw : int, weight draw\n    root : str, root directory\n    \"\"\"\n\n    ds = pd.read_csv(\n        f\"{root}/emulate-fivebean-{draw}.csv\",\n        skiprows=9,\n    )\n\n    # cleaning columns\n    for i, name in enumerate([\"iso\", \"year\"]):\n        ds[name] = ds[\"name\"].str.split(\":\", expand=True)[i]\n\n    # dropping weights rows and keeping only error rows\n    ds = ds.loc[ds.param == \"error\"]\n\n    ds[\"rff_sp\"] = draw\n\n    ds[\"year\"] = ds.year.astype(int)\n\n    ds = ds.set_index([\"iso\", \"year\", \"rff_sp\"]).value.to_xarray()\n\n    return ds\n</code></pre>"},{"location":"utils/rff/#dscim.utils.rff.clean_simulation","title":"dscim.utils.rff.clean_simulation","text":"<pre><code>clean_simulation(draw, root)\n</code></pre> <p>Clean the weights csvs generated by the RFF emulator. This produces a file of the weights.</p> <p>Parameters:</p> Name Type Description Default <code>draw</code> <code>int, weight draw</code> required <code>root</code> <code>str, root directory</code> required Source code in <code>src/dscim/utils/rff.py</code> <pre><code>def clean_simulation(\n    draw,\n    root,\n):\n    \"\"\"\n    Clean the weights csvs generated by the RFF emulator.\n    This produces a file of the weights.\n\n    Parameters\n    ----------\n    draw : int, weight draw\n    root : str, root directory\n    \"\"\"\n\n    ds = pd.read_csv(\n        f\"{root}/emulate-fivebean-{draw}.csv\",\n        skiprows=9,\n    )\n\n    # cleaning columns\n    for i, name in enumerate([\"model\", \"ssp\"]):\n        ds[name] = ds[\"name\"].str.split(\"/\", expand=True)[i]\n    for i, name in enumerate([\"year\", \"model\"]):\n        ds[name] = ds[\"model\"].str.split(\":\", expand=True)[i]\n\n    # dropping error rows and keeping only weights rows\n    ds = ds.loc[ds.param != \"error\"]\n\n    ds[\"model\"] = ds.model.replace({\"low\": \"IIASA GDP\", \"high\": \"OECD Env-Growth\"})\n\n    ds[\"rff_sp\"] = draw\n\n    ds[\"year\"] = ds.year.astype(int)\n\n    ds = ds.set_index([\"model\", \"ssp\", \"rff_sp\", \"year\"]).to_xarray()[\"value\"]\n\n    return ds\n</code></pre>"},{"location":"utils/rff/#dscim.utils.rff.prep_rff_socioeconomics","title":"dscim.utils.rff.prep_rff_socioeconomics","text":"<pre><code>prep_rff_socioeconomics(inflation_path, rff_path, runid_path, out_path, USA)\n</code></pre> <p>Generate the global or domestic RFF socioeconomics file for use with the <code>dscim</code> MainRecipe.</p> Source code in <code>src/dscim/utils/rff.py</code> <pre><code>def prep_rff_socioeconomics(\n    inflation_path,\n    rff_path,\n    runid_path,\n    out_path,\n    USA,\n):\n    \"\"\"Generate the global or domestic RFF socioeconomics file for use with the `dscim` MainRecipe.\"\"\"\n\n    # Load Fed GDP deflator\n    fed_gdpdef = pd.read_csv(inflation_path).set_index(\"year\")[\"gdpdef\"].to_dict()\n\n    # transform 2011 USD to 2019 USD\n    inflation_adj = fed_gdpdef[2019] / fed_gdpdef[2011]\n\n    # read in RFF data\n    socioec = xr.open_dataset(rff_path)\n\n    if not USA:\n        print(\"Summing to globe.\")\n        socioec = socioec.sum(\"Country\")\n    else:\n        print(\"USA output.\")\n        socioec = socioec.sel(Country=\"USA\", drop=True)\n\n    # interpolate with log -&gt; linear interpolation -&gt; exponentiate\n    socioec = np.exp(\n        np.log(socioec).interp({\"Year\": range(2020, 2301, 1)}, method=\"linear\")\n    ).rename({\"runid\": \"rff_sp\", \"Year\": \"year\", \"Pop\": \"pop\", \"GDP\": \"gdp\"})\n\n    socioec[\"pop\"] = socioec[\"pop\"] * 1000\n    socioec[\"gdp\"] = socioec[\"gdp\"] * 1e6 * inflation_adj\n\n    # read in RFF runids and update coordinates with them\n    run_id = xr.open_dataset(runid_path)\n    socioec = socioec.sel(rff_sp=run_id.rff_sp, drop=True)\n\n    if not USA:\n        socioec.expand_dims({\"region\": [\"world\"]}).to_netcdf(\n            f\"{out_path}/rff_global_socioeconomics.nc4\"\n        )\n    else:\n        socioec.expand_dims({\"region\": [\"USA\"]}).to_netcdf(\n            f\"{out_path}/rff_USA_socioeconomics.nc4\"\n        )\n</code></pre>"},{"location":"utils/rff/#dscim.utils.rff.process_rff_sample","title":"dscim.utils.rff.process_rff_sample","text":"<pre><code>process_rff_sample(i, rffpath, ssp_df, outdir, HEADER, **storage_options)\n</code></pre> <p>Clean raw socioeconomic projections from a single RFF-SP simulation run, pass the cleaned dataset to the <code>solve_optimization</code> function, and save outputs This produces a csv file of RFF emulator weights and country-level errors in 5-year increments for a single RFF-SP</p> Source code in <code>src/dscim/utils/rff.py</code> <pre><code>def process_rff_sample(i, rffpath, ssp_df, outdir, HEADER, **storage_options):\n    \"\"\"Clean raw socioeconomic projections from a single RFF-SP simulation run,\n    pass the cleaned dataset to the `solve_optimization` function, and save outputs\n    This produces a csv file of RFF emulator weights and country-level errors in 5-year\n    increments for a single RFF-SP\n    \"\"\"\n\n    read_feather = os.path.join(rffpath, f\"run_{i:d}.feather\")\n    rff_raw = pd.read_feather(read_feather)\n    rff_raw.rename(columns={\"Year\": \"year\", \"Country\": \"iso\"}, inplace=True)\n\n    # Fill missing data with mean across SSP scenarios of the same years\n    rff_df = pd.DataFrame()\n    for iso, group in rff_raw.groupby(\"iso\"):\n        minyear = min(group.year)\n        before_all = ssp_df[(ssp_df.year &lt; minyear) &amp; (ssp_df.iso == iso)][\n            [\"iso\", \"year\", \"value\"]\n        ]\n        before = before_all.groupby([\"iso\", \"year\"]).mean().reset_index()\n        after = pd.DataFrame(\n            dict(\n                iso=iso,\n                year=group.year,\n                value=(88.58 / 98.71) * (group.GDP * 1e6) / (group.Pop * 1000),\n            )\n        )  # Get in per capita 2005 PPP-adjusted USD rff GDP\n        all_year_df = pd.concat((before, after))\n        rff_df = pd.concat((rff_df, all_year_df))\n\n    rff_df[\"loginc\"] = np.log(rff_df.value)\n    rff_df[\"isoyear\"] = rff_df.apply(lambda row: f\"{row.iso}:{row.year:d}\", axis=1)\n\n    rff_df = pd.merge(rff_df, rff_raw, on=[\"year\", \"iso\"], how=\"left\")\n\n    rff_df[\"weight\"] = (\n        88.58 / 98.71\n    ) * rff_df.GDP  # Adjust weight measurement from 2011 tp 2005 PPP USD\n\n    # print(rff_df.iso[np.isnan(rff_df.weight)])\n    rff_df[\"weight\"] = rff_df[\"weight\"].fillna(\n        np.exp(np.nanmean(np.log(rff_df.weight)))\n    )  # Fill missing value weights with sample mean\n\n    out_df = solve_optimization(ssp_df, rff_df)\n\n    write_file = os.path.join(outdir, \"emulate-%d.csv\") % i\n\n    protocol = write_file.split(\"://\")[0] if \"://\" in write_file else \"\"\n    write_options = storage_options if protocol != \"\" else {}\n    fs = fsspec.filesystem(protocol, **write_options)\n\n    with fs.open(write_file, \"w\") as outf:\n        outf.write(HEADER.strip() + \"\\n\")\n        out_df.to_csv(outf, index=False)\n</code></pre>"},{"location":"utils/rff/#dscim.utils.rff.process_ssp_sample","title":"dscim.utils.rff.process_ssp_sample","text":"<pre><code>process_ssp_sample(ssppath)\n</code></pre> <p>Clean SSP per capita GDP projections</p> Source code in <code>src/dscim/utils/rff.py</code> <pre><code>def process_ssp_sample(ssppath):\n    \"\"\"Clean SSP per capita GDP projections\"\"\"\n    ssp_df = pd.read_csv(ssppath, skiprows=11)\n    ssp_df = ssp_df[ssp_df.year &gt;= 2010]\n    ssp_df[\"loginc\"] = np.log(ssp_df.value)\n    ssp_df[\"isoyear\"] = ssp_df.apply(lambda row: f\"{row.iso}:{row.year:d}\", axis=1)\n    ssp_df[\"yearscen\"] = ssp_df.apply(\n        lambda row: f\"{row.year:d}:{row.model}/{row.scenario}\", axis=1\n    )\n\n    return ssp_df\n</code></pre>"},{"location":"utils/rff/#dscim.utils.rff.rff_damage_functions","title":"dscim.utils.rff.rff_damage_functions","text":"<pre><code>rff_damage_functions(sectors, eta_rhos, USA, ssp_gdp, rff_gdp, recipes_discs, in_library, out_library, runid_path, weights_path, pulse_year, mask)\n</code></pre> <p>Wrapper function for <code>weight_df()</code>.</p> Source code in <code>src/dscim/utils/rff.py</code> <pre><code>def rff_damage_functions(\n    sectors,\n    eta_rhos,\n    USA,\n    ssp_gdp,\n    rff_gdp,\n    recipes_discs,\n    in_library,\n    out_library,\n    runid_path,\n    weights_path,\n    pulse_year,\n    mask,\n):\n    \"\"\"Wrapper function for `weight_df()`.\"\"\"\n\n    # ssp GDP for fractionalizing damage functions\n    ssp_gdp = xr.open_zarr(ssp_gdp, consolidated=True).sum(\"region\").gdp\n\n    # get RFF data\n    region = \"USA\" if USA else \"world\"\n    rff_gdp = xr.open_dataset(rff_gdp).sel(region=region, drop=True).gdp\n\n    # get global consumption factors to extrapolate damage function\n    factors = rff_gdp.sel(year=slice(2100, 2300)) / rff_gdp.sel(year=2099)\n\n    # get RFF emulator weights\n    run_id = xr.open_dataset(runid_path)\n    weights = (\n        xr.open_dataset(f\"{weights_path}/damage_function_weights.nc4\")\n        .sel(rff_sp=run_id.rff_sp, drop=True)\n        .value\n    )\n\n    for recipe_disc, sector, eta_rho in product(recipes_discs, sectors, eta_rhos):\n        print(f\"{datetime.now()} : {recipe_disc} {sector} {eta_rho}\")\n\n        weight_df(\n            sector=sector,\n            eta_rho=eta_rho,\n            recipe=recipe_disc[0],\n            disc=recipe_disc[1],\n            file=\"damage_function_coefficients\",\n            in_library=in_library,\n            out_library=out_library,\n            rff_gdp=rff_gdp,\n            ssp_gdp=ssp_gdp,\n            weights=weights,\n            factors=factors,\n            pulse_year=pulse_year,\n            mask=mask,\n        )\n</code></pre>"},{"location":"utils/rff/#dscim.utils.rff.solve_optimization","title":"dscim.utils.rff.solve_optimization","text":"<pre><code>solve_optimization(ssp_df, rff_df)\n</code></pre> <p>Generate weights based on which to derive the weighted average of damage function coefficents across six SSP-growth models for a single RFF-SP This function applies an emulation scheme to calculate a set of weights, constrained to sum to unity, that, when used to take a weighted average of global GDP across SSP-growth models (3 SSPs X 2 IAMs), most closely recovers the global GDP in the RFF-SP simulation run that wish to emulate. The emulation scheme is estimated and applied separately for each 5-year period, of a single RFF-SP. Within each period, the scheme aims to interpolate between the SSP-growth models in order to match the country-level GDPs designated by the given RFF-SP. Empirically, it solves an optimization problem to minimize a weighted sum of country-level errors, taking country-level RFF-SP GDPs as weights</p> <p>Parameters:</p> Name Type Description Default <code>ssp_df</code> <code>DataFrame</code> <p>Dataset with country-level log per capita GDPs by SSP-growth models in 5-year increments, post- processed by the <code>process_ssp_sample</code> function</p> required <code>rff_df</code> <code>DataFrame</code> <p>Dateset with country-level GDPs and log per capita GDPs for a single RFF-SP simulation run</p> required <p>Returns:</p> Type Description <code>    Dataset with a set of SSP-growth model weights and country-level errors in 5-year increments</code> <p>for a single RFF-SP</p> Source code in <code>src/dscim/utils/rff.py</code> <pre><code>def solve_optimization(ssp_df, rff_df):\n    \"\"\"Generate weights based on which to derive the weighted average of damage function coefficents\n    across six SSP-growth models for a single RFF-SP\n    This function applies an emulation scheme to calculate a set of weights, constrained to\n    sum to unity, that, when used to take a weighted average of global GDP across SSP-growth models\n    (3 SSPs X 2 IAMs), most closely recovers the global GDP in the RFF-SP simulation run that\n    wish to emulate. The emulation scheme is estimated and applied separately for each 5-year period,\n    of a single RFF-SP. Within each period, the scheme aims to interpolate between the SSP-growth models\n    in order to match the country-level GDPs designated by the given RFF-SP. Empirically, it solves\n    an optimization problem to minimize a weighted sum of country-level errors, taking country-level\n    RFF-SP GDPs as weights\n    Parameters\n    ----------\n    ssp_df : pd.DataFrame\n        Dataset with country-level log per capita GDPs by SSP-growth models in 5-year increments, post-\n        processed by the `process_ssp_sample` function\n    rff_df : pd.DataFrame\n        Dateset with country-level GDPs and log per capita GDPs for a single RFF-SP simulation run\n    Returns\n    ------\n        Dataset with a set of SSP-growth model weights and country-level errors in 5-year increments\n        for a single RFF-SP\n    \"\"\"\n    # Import gurobipy here so only required for this function rather than entire module.\n    import gurobipy as gp\n\n    ssp_df = ssp_df[(ssp_df.scenario != \"SSP1\") &amp; (ssp_df.scenario != \"SSP5\")]\n\n    output = []\n\n    header = [\"year\", \"param\", \"name\", \"value\"]\n\n    years = pd.unique(ssp_df.year)\n    for year in years:\n        sspidf = ssp_df[ssp_df.year == year]\n        rffidf = rff_df[rff_df.year == year]\n\n        if rffidf.shape[0] == 0:\n            continue\n\n        isoyears = pd.unique(sspidf.isoyear)\n\n        # Create parameters list\n        alphaparams = pd.unique(sspidf.yearscen)\n\n        # Drop the first entry for each subgroup\n        alphaparams_butfirst = np.delete(alphaparams, 0)\n        params = np.concatenate((isoyears, alphaparams_butfirst))\n        paramindex_alpha0 = len(isoyears)\n\n        # Construct objective function\n        if \"weight\" in rff_df.columns:\n            weights = [\n                rffidf.weight[(rffidf.isoyear == isoyear)].values[0]\n                for isoyear in isoyears\n            ]\n        else:\n            weights = np.ones(len(isoyears))\n        objfunc = np.concatenate((weights, np.zeros(len(alphaparams_butfirst))))\n\n        # Contruct constraints, all in the form of A x &lt; b\n        AA_rows = []\n        AA_cols = []\n        AA_data = []\n        bb = []\n\n        def add_AA_cell(row, col, value):\n            AA_rows.append(row)\n            AA_cols.append(col)\n            AA_data.append(value)\n\n        ## Rows defining absolute values\n\n        # d_it &gt; y_it - (sum_s&gt;1 alpha_is y_sit + (1 - sum_s&gt;1 alpha_is) y_1it)\n        # -d_it - (sum_s&gt;1 alpha_is y_sit - (sum_s&gt;1 alpha_is) y_1it) &lt; -y_it + y_1it\n\n        # d_it &gt; -(y_it - (sum_s&gt;1 alpha_is y_sit + (1 - sum_s&gt;1 alpha_is) y_1it))\n        # -d_it + (sum_s&gt;1 alpha_is y_sit - (sum_s&gt;1 alpha_is) y_1it) &lt; y_it - y_1it\n\n        for ii, isoyear in enumerate(isoyears):\n            subdf = sspidf[sspidf.isoyear == isoyear]\n            if subdf.shape[0] == 0 or not np.any(rffidf.isoyear == isoyear):\n                continue\n\n            try:\n                y1it = subdf.loginc[subdf.yearscen == alphaparams[0]].values[0]\n            except Exception as ex:\n                print(ii, isoyear, ex)\n                print(\"Exception! Keep going..\")  # KM added\n                continue\n\n            add_AA_cell(len(bb), ii, -1)\n            add_AA_cell(len(bb) + 1, ii, -1)\n\n            for jj, alphaparam in enumerate(alphaparams_butfirst):\n                ysit = subdf.loginc[subdf.yearscen == alphaparam].values[0]\n\n                add_AA_cell(len(bb), paramindex_alpha0 + jj, -ysit + y1it)\n                add_AA_cell(len(bb) + 1, paramindex_alpha0 + jj, ysit - y1it)\n\n            bb.append(-rffidf.loginc[(rffidf.isoyear == isoyear)].values[0] + y1it)\n            bb.append(rffidf.loginc[(rffidf.isoyear == isoyear)].values[0] - y1it)\n\n        constrindex_alphag10 = len(bb)\n\n        # sum alpha_is &lt; 1\n        for jj in range(paramindex_alpha0, len(params)):\n            add_AA_cell(constrindex_alphag10, jj, 1)\n        bb.append(1)\n\n        constrindex_end = constrindex_alphag10 + 1\n\n        AA = coo_matrix(\n            (AA_data, (AA_rows, AA_cols)), shape=(constrindex_end, len(params))\n        )\n\n        # Also constrained so that d_it &gt; 0 and alpha_is &gt; 0\n\n        env = gp.Env(empty=True)\n        env.setParam(\"OutputFlag\", 0)\n        env.start()\n        mod = gp.Model(\"gourmet\", env=env)\n        xx = mod.addMVar(shape=len(objfunc), vtype=gp.CONTINUOUS, name=\"xx\")\n        mod.setObjective(objfunc @ xx, gp.MINIMIZE)\n        bb = np.array(bb)\n        mod.addConstr(AA @ xx &lt;= bb)\n        mod.optimize()\n        errors = np.around(xx.X[:paramindex_alpha0], 6)\n        alphas = np.around(xx.X[paramindex_alpha0:], 6)\n\n        for ii, error in enumerate(errors):\n            output.append([year, \"error\", isoyears[ii], error])\n\n        output.append([year, \"alpha\", alphaparams[0], max(0, 1 - sum(alphas))])\n        for ii, alpha in enumerate(alphas):\n            output.append([year, \"alpha\", alphaparams_butfirst[ii], alpha])\n\n    out_df = pd.DataFrame(output, columns=header)\n\n    return out_df\n</code></pre>"},{"location":"utils/rff/#dscim.utils.rff.weight_df","title":"dscim.utils.rff.weight_df","text":"<pre><code>weight_df(sector, eta_rho, recipe, disc, file, in_library, out_library, rff_gdp, ssp_gdp, weights, factors, pulse_year, fractional=False, mask='unmasked')\n</code></pre> <p>Weight, fractionalize, and combine SSP damage functions, then multiply by RFF GDP to return RFF damage functions.</p> Source code in <code>src/dscim/utils/rff.py</code> <pre><code>def weight_df(\n    sector,\n    eta_rho,\n    recipe,\n    disc,\n    file,\n    in_library,\n    out_library,\n    rff_gdp,\n    ssp_gdp,\n    weights,\n    factors,\n    pulse_year,\n    fractional=False,\n    mask=\"unmasked\",\n):\n    \"\"\"Weight, fractionalize, and combine SSP damage functions,\n    then multiply by RFF GDP to return RFF damage functions.\n    \"\"\"\n\n    # get damage function as share of global GDP\n    df = (\n        xr.open_dataset(\n            f\"{in_library}/{sector}/{pulse_year}/{mask}/{recipe}_{disc}_eta{eta_rho[0]}_rho{eta_rho[1]}_{file}.nc4\"\n        )\n        / ssp_gdp\n    )\n\n    # pre-2100 weighted fractional damage functions\n    rff = (df * weights).sum([\"ssp\", \"model\"])\n\n    # save fractional damage function\n    if fractional:\n        rff.sel(year=slice(2020, 2099)).to_netcdf(\n            f\"{out_library}/{sector}/{pulse_year}/{mask}/{recipe}_{disc}_eta{eta_rho[0]}_rho{eta_rho[1]}_fractional_{file}.nc4\"\n        )\n\n    # recover damage function as dollars instead of fraction\n    rff = (rff * rff_gdp).sel(year=slice(2020, 2099))\n\n    # post-2100 weighted damage functions\n    post_2100 = rff.sel(year=2099) * factors\n\n    dfs = xr.combine_by_coords([rff, post_2100])\n\n    os.makedirs(f\"{out_library}/{sector}/{pulse_year}/{mask}\", exist_ok=True)\n    dfs.to_netcdf(\n        f\"{out_library}/{sector}/{pulse_year}/{mask}/{recipe}_{disc}_eta{eta_rho[0]}_rho{eta_rho[1]}_{file}.nc4\"\n    )\n</code></pre>"},{"location":"utils/utils/","title":"utils","text":"<p>Functions:</p> Name Description <code>c_equivalence</code> <p>Calculate certainty equivalent assuming a CRRA utility</p> <code>compute_damages</code> <p>Calculate damages using FAIR anomalies (either control or pulse).</p> <code>extrapolate</code> <p>Extrapolate values from xarray object indexed in time</p> <code>get_weights</code> <p>Compute the weights of each quantile regression.</p> <code>model_outputs</code> <p>Estimate damage function coefficients and predictions using</p> <code>modeler</code> <p>Wrapper function for statsmodels functions (OLS and quantiles)</p> <code>power</code> <p>Power function that doesn't return NaN values for negative fractional exponents</p> <code>quantile_weight_quantilereg</code> <p>Produce quantile weights of the quantile regression damages.</p>"},{"location":"utils/utils/#dscim.utils.utils.c_equivalence","title":"dscim.utils.utils.c_equivalence","text":"<pre><code>c_equivalence(array, dims, eta, weights=None, func_args=None, func=None)\n</code></pre> <p>Calculate certainty equivalent assuming a CRRA utility function.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code> xr.Dataset or xr.DataArray</code> <p>A <code>xarray</code> object containing non-negative consumption numbers.</p> required <code>dims</code> <code>list</code> <p>List of dimensions to use in aggregation.</p> required <code>eta</code> <code>int or float</code> <p>the parameter determining the curvature of the CRRA utility function. Must be different from 1.</p> required <code>func</code> <code>function</code> <p>If user desired a different utility function to the CRRA, this argument can take a <code>lambda</code> function where <code>array</code> is the main variable.</p> <code>None</code> <code>weights</code> <code> xr.DataArray</code> <p>Array with weights for aggregation calculation. <code>None</code> by default.</p> <code>None</code> <code>func_args</code> <code>dict</code> <p>Other arguments passed to <code>func</code></p> <code>None</code> <p>Returns:</p> Type Description <code>    xarray.Dataset with CEs over the desired dims</code> Source code in <code>src/dscim/utils/utils.py</code> <pre><code>def c_equivalence(array, dims, eta, weights=None, func_args=None, func=None):\n    \"\"\"Calculate certainty equivalent assuming a CRRA utility\n    function.\n\n    Parameters\n    ----------\n    array :  xr.Dataset or xr.DataArray\n        A `xarray` object containing non-negative consumption numbers.\n    dims : list\n        List of dimensions to use in aggregation.\n    eta : int or float\n        the parameter determining the curvature of the CRRA utility function. Must be different from 1.\n    func : function\n        If user desired a different utility function to the CRRA, this\n        argument can take a `lambda` function where `array` is the main\n        variable.\n    weights :  xr.DataArray\n        Array with weights for aggregation calculation. `None` by default.\n    func_args : dict\n        Other arguments passed to `func`\n\n    Returns\n    -------\n        xarray.Dataset with CEs over the desired dims\n    \"\"\"\n\n    if isinstance(array, xr.DataArray):\n        negative_values = array.where(lambda x: (x &lt; 0).compute(), drop=True).size\n    elif isinstance(array, xr.Dataset):\n        negative_values = (\n            array[list(array.variables)[0]]\n            .where(lambda x: (x &lt; 0).compute(), drop=True)\n            .size\n        )\n    else:\n        raise TypeError(\"array should be either an xarray.DataArray or xarray.Dataset\")\n\n    if negative_values != 0:\n        raise ValueError(\n            \"Negative values were passed to the certainty equivalence function\"\n        )\n\n    if func is None:\n        if eta == 1:\n            utility = np.log(array)\n        else:\n            exp = 1 / (1 - eta)\n            utility = np.true_divide(array ** (1 - eta), (1 - eta))\n\n        if weights is None:\n            exp_utility = utility.mean(dim=dims)\n        else:\n            exp_utility = utility.weighted(weights).mean(dim=dims)\n\n        if eta == 1:\n            ce_array = np.exp(exp_utility)\n        else:\n            ce_array = (exp_utility * (1 - eta)) ** exp\n\n    else:\n        try:\n            ce_array = array.map_partitions(func, **func_args)\n        except Exception as exception:\n            raise ValueError(f\"Custom function not conforming with array: {exception}\")\n\n    return ce_array\n</code></pre>"},{"location":"utils/utils/#dscim.utils.utils.compute_damages","title":"dscim.utils.utils.compute_damages","text":"<pre><code>compute_damages(anomaly, betas, formula)\n</code></pre> <p>Calculate damages using FAIR anomalies (either control or pulse).</p> <p>Parameters:</p> Name Type Description Default <code>anomaly</code> <p><code>xarray</code> object containing FAIR temperature anomaly. These can be either from control or pulse runs.</p> required <code>betas</code> <p>An <code>xarray</code> object containing the fit coefficients from the yearly damage functions</p> required <code>formula</code> <code>str</code> <p>The formula used to calculate damages</p> required <p>Returns:</p> Type Description <code>    xr.DataArray with damages</code> Source code in <code>src/dscim/utils/utils.py</code> <pre><code>def compute_damages(anomaly, betas, formula):\n    \"\"\"Calculate damages using FAIR anomalies (either control or pulse).\n\n    Parameters\n    ----------\n    anomaly: xr.DataArray or xr.Dataset\n        `xarray` object containing FAIR temperature anomaly. These can be\n        either from control or pulse runs.\n    betas: xr.DataSet\n        An `xarray` object containing the fit coefficients from the yearly damage\n        functions\n    formula : str\n        The formula used to calculate damages\n\n    Returns\n    -------\n        xr.DataArray with damages\n    \"\"\"\n\n    # Parse formula to calculate damages\n    if (\n        formula\n        == \"damages ~ -1 + anomaly * gmsl + anomaly * np.power(gmsl, 2) + gmsl * np.power(anomaly, 2) + np.power(anomaly, 2) * np.power(gmsl, 2)\"\n    ):\n        betas, anomaly = xr.broadcast(betas, anomaly)\n        betas = betas.transpose(*sorted(betas.dims)).sortby(list(betas.dims))\n        anomaly = anomaly.transpose(*sorted(anomaly.dims)).sortby(list(anomaly.dims))\n        damages_fair = (\n            betas[\"anomaly\"] * anomaly.temperature\n            + betas[\"np.power(anomaly, 2)\"] * np.power(anomaly.temperature, 2)\n            + betas[\"gmsl\"] * anomaly.gmsl\n            + betas[\"np.power(gmsl, 2)\"] * np.power(anomaly.gmsl, 2)\n            + betas[\"anomaly:gmsl\"] * anomaly.temperature * anomaly.gmsl\n            + betas[\"anomaly:np.power(gmsl, 2)\"]\n            * anomaly.temperature\n            * np.power(anomaly.gmsl, 2)\n            + betas[\"gmsl:np.power(anomaly, 2)\"]\n            * anomaly.gmsl\n            * np.power(anomaly.temperature, 2)\n            + betas[\"np.power(anomaly, 2):np.power(gmsl, 2)\"]\n            * np.power(anomaly.temperature, 2)\n            * np.power(anomaly.gmsl, 2)\n        )\n\n    elif (\n        formula\n        == \"damages ~ -1 + anomaly:gmsl + anomaly:np.power(gmsl, 2) + gmsl:np.power(anomaly, 2) + np.power(anomaly, 2):np.power(gmsl, 2)\"\n    ):\n        damages_fair = (\n            betas[\"anomaly:gmsl\"] * anomaly.temperature * anomaly.gmsl\n            + betas[\"anomaly:np.power(gmsl, 2)\"]\n            * anomaly.temperature\n            * np.power(anomaly.gmsl, 2)\n            + betas[\"gmsl:np.power(anomaly, 2)\"]\n            * anomaly.gmsl\n            * np.power(anomaly.temperature, 2)\n            + betas[\"np.power(anomaly, 2):np.power(gmsl, 2)\"]\n            * np.power(anomaly.temperature, 2)\n            * np.power(anomaly.gmsl, 2)\n        )\n    elif formula == \"damages ~ -1 + gmsl:anomaly + gmsl:np.power(anomaly, 2)\":\n        damages_fair = betas[\n            \"gmsl:anomaly\"\n        ] * anomaly.temperature * anomaly.gmsl + betas[\n            \"gmsl:np.power(anomaly, 2)\"\n        ] * anomaly.gmsl * np.power(anomaly.temperature, 2)\n    elif (\n        formula\n        == \"damages ~ -1 + anomaly + np.power(anomaly, 2) + gmsl + np.power(gmsl, 2)\"\n    ):\n        damages_fair = (\n            betas[\"anomaly\"] * anomaly.temperature\n            + betas[\"np.power(anomaly, 2)\"] * np.power(anomaly.temperature, 2)\n            + betas[\"gmsl\"] * anomaly.gmsl\n            + betas[\"np.power(gmsl, 2)\"] * np.power(anomaly.gmsl, 2)\n        )\n    elif (\n        formula == \"damages ~ anomaly + np.power(anomaly, 2) + gmsl + np.power(gmsl, 2)\"\n    ):\n        damages_fair = (\n            betas[\"Intercept\"]\n            + betas[\"anomaly\"] * anomaly.temperature\n            + betas[\"np.power(anomaly, 2)\"] * np.power(anomaly.temperature, 2)\n            + betas[\"gmsl\"] * anomaly.gmsl\n            + betas[\"np.power(gmsl, 2)\"] * np.power(anomaly.gmsl, 2)\n        )\n    elif formula == \"damages ~ -1 + gmsl + anomaly + np.power(anomaly, 2)\":\n        damages_fair = (\n            betas[\"anomaly\"] * anomaly.temperature\n            + betas[\"np.power(anomaly, 2)\"] * np.power(anomaly.temperature, 2)\n            + betas[\"gmsl\"] * anomaly.gmsl\n        )\n\n    elif formula == \"damages ~ -1 + anomaly + np.power(anomaly, 2)\":\n        damages_fair = betas[\"anomaly\"] * anomaly.temperature + betas[\n            \"np.power(anomaly, 2)\"\n        ] * np.power(anomaly.temperature, 2)\n\n    elif formula == \"damages ~ anomaly + np.power(anomaly, 2)\":\n        damages_fair = (\n            betas[\"Intercept\"]\n            + betas[\"anomaly\"] * anomaly.temperature\n            + betas[\"np.power(anomaly, 2)\"] * np.power(anomaly.temperature, 2)\n        )\n    elif formula == \"damages ~ gmsl + np.power(gmsl, 2)\":\n        damages_fair = (\n            betas[\"Intercept\"]\n            + betas[\"gmsl\"] * anomaly.gmsl\n            + betas[\"np.power(gmsl, 2)\"] * np.power(anomaly.gmsl, 2)\n        )\n    elif formula == \"damages ~ -1 + gmsl + np.power(gmsl, 2)\":\n        damages_fair = betas[\"gmsl\"] * anomaly.gmsl + betas[\n            \"np.power(gmsl, 2)\"\n        ] * np.power(anomaly.gmsl, 2)\n    elif formula == \"damages ~ -1 + gmsl\":\n        damages_fair = betas[\"gmsl\"] * anomaly.gmsl\n\n    elif formula == \"damages ~ -1 + np.power(anomaly, 2)\":\n        damages_fair = betas[\"np.power(anomaly, 2)\"] * np.power(anomaly.temperature, 2)\n\n    return damages_fair\n</code></pre>"},{"location":"utils/utils/#dscim.utils.utils.extrapolate","title":"dscim.utils.utils.extrapolate","text":"<pre><code>extrapolate(xr_array, start_year, end_year, interp_year, method, var=None, cap=None)\n</code></pre> <p>Extrapolate values from xarray object indexed in time</p> <p>Extrapolate values (of a specific type, see <code>xr_array</code>) in xr.DataArray. This function uses the <code>interp</code> option in xarray objects. The extrapolation can either be linear or constant by using the range of years between <code>start_year</code> and <code>end_year</code>.</p> Parameters: <p>xr_array : xr.Dataset or xr.DataArray     Array with a <code>'year'</code> dimension. There can't be negative values, nor in the initial,     neither in the extrapolated array. start_year : int     Extrapolation subsample start time.     TODO : It is currently used to subsample the data before extrapolation happens. Should be removed and the last two data points be kept in all cases. end_year : int     Extrapolation subsample end time interp_year : int     End of extrapolation. The function will calculate values from the     range given by the <code>end_year</code> and the <code>interp_year</code>. var : str or None     If str, it is required that <code>xr_array</code> is a dataset, and <code>var</code> defines the variable within     the dataset. Default value is <code>None</code>. method : str     Interpolation method. Valid values are \"linear\", \"linear_log\", \"elog\", \"squared\", \"growth_constant\", otherwise a     ValueError is raised. <code>xr_array</code> should be of type <code>xr.Array</code> if <code>method</code> is equal to \"growth_constant\". cap: xr.DataArray     Used only if <code>method</code> is equal to \"growth_constant\". An Array with identical dimensions to <code>xr_array</code>, which will     serve as a cap for end of century growth rates of <code>xr_array</code>.     If <code>None</code> (the default value) is passed, no cap will be imposed.</p> <p>Returns:</p> Type Description <code>    An interpolated ``xarray`` object with additional years.</code> Source code in <code>src/dscim/utils/utils.py</code> <pre><code>def extrapolate(\n    xr_array, start_year, end_year, interp_year, method, var=None, cap=None\n):\n    \"\"\"Extrapolate values from xarray object indexed in time\n\n    Extrapolate values (of a specific type, see ``xr_array``) in xr.DataArray. This function uses the ``interp``\n    option in xarray objects. The extrapolation can either be linear or\n    constant by using the range of years between ``start_year`` and\n    ``end_year``.\n\n    Parameters:\n    ----------\n    xr_array : xr.Dataset or xr.DataArray\n        Array with a ``'year'`` dimension. There can't be negative values, nor in the initial,\n        neither in the extrapolated array.\n    start_year : int\n        Extrapolation subsample start time.\n        TODO : It is currently used to subsample the data before extrapolation happens. Should be removed and the last two data points be kept in all cases.\n    end_year : int\n        Extrapolation subsample end time\n    interp_year : int\n        End of extrapolation. The function will calculate values from the\n        range given by the ``end_year`` and the ``interp_year``.\n    var : str or None\n        If str, it is required that ``xr_array`` is a dataset, and ``var`` defines the variable within\n        the dataset. Default value is ``None``.\n    method : str\n        Interpolation method. Valid values are \"linear\", \"linear_log\", \"elog\", \"squared\", \"growth_constant\", otherwise a\n        ValueError is raised. ``xr_array`` should be of type ``xr.Array`` if ``method`` is equal to \"growth_constant\".\n    cap: xr.DataArray\n        Used only if ``method`` is equal to \"growth_constant\". An Array with identical dimensions to ``xr_array``, which will\n        serve as a cap for end of century growth rates of ``xr_array``.\n        If ``None`` (the default value) is passed, no cap will be imposed.\n\n    Returns\n    -------\n        An interpolated ``xarray`` object with additional years.\n    \"\"\"\n\n    if isinstance(xr_array, xr.Dataset) and var is None:\n        raise ValueError(\"Need to pass a var value to subset the xr.Dataset\")\n\n    if method == \"linear\":\n        if var is not None:\n            return (\n                xr_array[var]\n                .sel({\"year\": slice(start_year, end_year)})\n                .interp(\n                    year=np.arange(end_year + 1, interp_year + 1),\n                    method=\"linear\",\n                    kwargs={\"fill_value\": \"extrapolate\"},\n                )\n            )\n        else:\n            return xr_array.sel(year=slice(start_year, end_year)).interp(\n                year=np.arange(end_year + 1, interp_year + 1),\n                method=\"linear\",\n                kwargs={\"fill_value\": \"extrapolate\"},\n            )\n\n    elif method == \"linear_log\":\n        log_array = np.log(xr_array).sel(year=slice(start_year, end_year))\n        log_interp = log_array.interp(\n            year=np.arange(end_year + 1, interp_year + 1),\n            method=\"linear\",\n            kwargs={\"fill_value\": \"extrapolate\"},\n        )\n        return np.exp(log_interp)\n\n    elif method == \"elog\":\n        factor = 1e14\n        exp_array = np.exp(xr_array / factor).sel(year=slice(start_year, end_year))\n        exp_interp = exp_array.interp(\n            year=np.arange(end_year + 1, interp_year + 1),\n            method=\"linear\",\n            kwargs={\"fill_value\": \"extrapolate\"},\n        )\n        return np.log(exp_interp) * factor\n\n    elif method == \"squared\":\n        sqr_array = (xr_array**2).sel(year=slice(start_year, end_year))\n        sqr_interp = sqr_array.interp(\n            year=np.arange(end_year + 1, interp_year + 1),\n            method=\"linear\",\n            kwargs={\"fill_value\": \"extrapolate\"},\n        )\n        return np.sqrt(sqr_interp)\n\n    elif method == \"growth_constant\":\n        # Calculate growth rates\n        growth_rates = xr_array.diff(\"year\") / xr_array.shift(year=1)\n\n        # cap growth in final year\n        if cap is not None:\n            growth_rates = xr.where(\n                (growth_rates.year == end_year) &amp; (growth_rates &gt; cap),\n                cap,\n                growth_rates,\n            )\n        else:\n            print(\"End-of-century growth rates are not capped.\")\n\n        # hold last year constant for extrapolated array\n        growth_rates_ext = growth_rates.reindex(\n            year=range(end_year + 1, interp_year + 1), method=\"ffill\"\n        )\n\n        # Start growth rates from zero from the first year\n        # Calculate new array using fixed growth rate\n        growth_rates_cum = np.multiply.accumulate(\n            (growth_rates_ext.values + 1), growth_rates_ext.dims.index(\"year\")\n        )\n        growth_rates_cum_xarr = xr.DataArray(\n            growth_rates_cum, coords=growth_rates_ext.coords\n        )\n\n        return growth_rates_cum_xarr * xr_array.sel(year=end_year)\n\n    else:\n        raise ValueError(f\"{method} is not a valid interpolation method.\")\n</code></pre>"},{"location":"utils/utils/#dscim.utils.utils.get_weights","title":"dscim.utils.utils.get_weights","text":"<pre><code>get_weights(quantiles)\n</code></pre> <p>Compute the weights of each quantile regression.</p> Parameters : <p>quantiles : array_like     representing a sequence of quantile values, must be between 0-1, otherwise throws as RunTimeError.</p> Returns : <p>vector of weights represented by an np.ndarray</p> Source code in <code>src/dscim/utils/utils.py</code> <pre><code>def get_weights(quantiles):\n    \"\"\"\n    Compute the weights of each quantile regression.\n\n    Parameters :\n    ------------\n    quantiles : array_like\n        representing a sequence of quantile values, must be between 0-1, otherwise throws as RunTimeError.\n\n    Returns :\n    --------\n    vector of weights represented by an np.ndarray\n    \"\"\"\n    quantiles = np.array(quantiles)\n    # find midpoints between quantiles\n    if np.any(np.logical_or(quantiles &gt; 1, quantiles &lt; 0)):\n        raise RuntimeError(\"quantiles must be between 0-1\")\n    bounds = np.array([0] + ((quantiles[:-1] + quantiles[1:]) / 2).tolist() + [1])\n    weights = np.diff(bounds)\n    return weights\n</code></pre>"},{"location":"utils/utils/#dscim.utils.utils.model_outputs","title":"dscim.utils.utils.model_outputs","text":"<pre><code>model_outputs(damage_function, extrapolation_type, formula, year_range, year_start_pred, quantiles, global_c=None, min_anomaly=0, max_anomaly=20, step_anomaly=0.2, min_gmsl=0, max_gmsl=300, step_gmsl=3, type_estimation='ols')\n</code></pre> <p>Estimate damage function coefficients and predictions using passed formula.</p> <p>This model is estimated for each year in our timeframe (2010 to 2099) and using a rolling window across time (5-yr by default).</p> <p>Damage functions are extrapolated for each year between 2099 and 2300.</p> <p>Parameters:</p> Name Type Description Default <code>damage_function</code> <p>A global damage function.</p> required <code>type_estimation</code> <p>Type of model use for damage function fitting: <code>ols</code>, <code>quantreg</code></p> <code>'ols'</code> <code>extrapolation_type</code> <code>str</code> <p>Type of extrapolation: <code>global_c_ratio</code></p> required <code>global_c</code> <code>DataArray</code> <p>Array with global consumption extrapolated to 2300. This is only used when <code>extrapolation_type</code> is <code>global_c_ratio</code>.</p> <code>None</code> <code>year_start_pred</code> <p>Start of extrapolation</p> required <code>year_range</code> <p>Range of years to estimate over. Default is 2010 to 2100</p> required <p>Returns:</p> Type Description <code>dict</code> <p>dict with two keys, <code>params</code> and <code>preds</code>. Each value is a Pandas DataFrame with yearly damage functions (coefficients and predictions respectively).</p> Source code in <code>src/dscim/utils/utils.py</code> <pre><code>def model_outputs(\n    damage_function,\n    extrapolation_type,\n    formula,\n    year_range,\n    year_start_pred,\n    quantiles,\n    global_c=None,\n    min_anomaly=0,\n    max_anomaly=20,\n    step_anomaly=0.2,\n    min_gmsl=0,\n    max_gmsl=300,\n    step_gmsl=3,\n    type_estimation=\"ols\",\n):\n    \"\"\"Estimate damage function coefficients and predictions using\n    passed formula.\n\n    This model is estimated for each year in our timeframe (2010 to 2099) and\n    using a rolling window across time (5-yr by default).\n\n    Damage functions are extrapolated for each year between 2099 and 2300.\n\n    Parameters\n    ----------\n    damage_function: pandas.DataFrame\n        A global damage function.\n    type_estimation: str\n        Type of model use for damage function fitting: `ols`, `quantreg`\n    extrapolation_type : str\n        Type of extrapolation: `global_c_ratio`\n    global_c : xr.DataArray\n        Array with global consumption extrapolated to 2300. This is only used\n        when ``extrapolation_type`` is ``global_c_ratio``.\n    year_start_pred: int\n        Start of extrapolation\n    year_range: sequence, lst, tuple, range\n        Range of years to estimate over. Default is 2010 to 2100\n\n    Returns\n    ------\n\n    dict\n        dict with two keys, `params` and `preds`. Each value is a Pandas\n        DataFrame with yearly damage functions (coefficients and predictions\n        respectively).\n    \"\"\"\n\n    # set year of prediction for global C extrapolation\n    fix_global_c = year_start_pred - 1\n\n    # set exogenous variables for predictions\n    gmsl = np.arange(min_gmsl, max_gmsl, step_gmsl)\n    temps = np.arange(min_anomaly, max_anomaly, step_anomaly)\n\n    if (\"anomaly\" in damage_function.columns) and (\"gmsl\" in damage_function.columns):\n        exog_X = pd.DataFrame(product(temps, gmsl))\n        exog = dict(anomaly=exog_X.values[:, 0], gmsl=exog_X.values[:, 1])\n    elif \"anomaly\" in damage_function.columns:\n        exog = dict(anomaly=temps)\n    elif \"gmsl\" in damage_function.columns:\n        exog = dict(gmsl=gmsl)\n    else:\n        print(\"Independent variables not found.\")\n\n    # Rolling window estimation (5-yr)\n    list_params, list_y_hats = [], []\n\n    for year in year_range:\n        time_window = range(year - 2, year + 3)\n        df = damage_function[damage_function.year.isin(time_window)]\n        params, y_hat = modeler(\n            df=df,\n            formula=formula,\n            type_estimation=type_estimation,\n            exog=exog,\n            quantiles=quantiles,\n        )\n\n        params, y_hat = params.assign(year=year), y_hat.assign(year=year)\n        list_params.append(params)\n        list_y_hats.append(y_hat)\n\n    # Concatenate results\n    param_df = pd.concat(list_params)\n    y_hat_df = pd.concat(list_y_hats)\n\n    if extrapolation_type == \"global_c_ratio\":\n        # convert to xarray immediately\n        index = [\"year\", \"q\"] if type_estimation == \"quantreg\" else [\"year\"]\n        y_hat_df = y_hat_df.set_index(\n            [i for i in y_hat_df.columns if \"y_hat\" not in i]\n        ).to_xarray()\n        param_df = param_df.set_index(index).to_xarray()\n\n        # Calculate global consumption ratios to fixed year\n        global_c_factors = (\n            global_c.sel(year=slice(fix_global_c + 1, None))\n            / global_c.sel(year=fix_global_c)\n        ).squeeze()\n\n        # Extrapolate by multiplying fixed params by ratios\n        extrap_preds = y_hat_df.sel(year=fix_global_c) * global_c_factors\n        extrap_params = param_df.sel(year=fix_global_c) * global_c_factors\n\n        # concatenate extrapolation and pre-2100\n        preds = xr.concat([y_hat_df, extrap_preds], dim=\"year\")\n        parameters = xr.concat([param_df, extrap_params], dim=\"year\")\n\n        # For the local case we don't care about the time dimension, the\n        # extrapolation will index on the last year available of damages (2099)\n        # by design (changed using the fix_global_c parameter) and will use\n        # those damages to extrapolate to the last year of interest (2300)\n        # which comes from the global_c object.\n        if global_c_factors.year.dims == ():\n            raise NotImplementedError(\"WATCH OUT! Local is scrapped.\")\n\n    # Return all results\n    res = {\n        \"parameters\": parameters,\n        \"preds\": preds,\n    }\n\n    return res\n</code></pre>"},{"location":"utils/utils/#dscim.utils.utils.modeler","title":"dscim.utils.utils.modeler","text":"<pre><code>modeler(df, formula, type_estimation, exog, quantiles=None)\n</code></pre> <p>Wrapper function for statsmodels functions (OLS and quantiles)</p> <p>For the quantiles, the function will fit for all quantiles between 0 and 1 with a 0.25 step.</p> Parameters: <p>df: pd.DataFrame     A dataframe of prepared damage function. Since formulas are hard to     get, the function expects standarized column names: <code>damages</code> must     contain all damages coming from the damage function, and <code>temp</code> must     be the name of temperature anomalies from the models. formula: str     R-like formula to start fitting. (i.e. damages ~ temp +     np.power(temp,2)) defines a quadratic relationship between temperature     and damages. type_estimation: str     Model to use . 'ols' and 'quantreg' are the one availables. Any other     passed option will yield a <code>NonImplementedError</code> exog: pd.DataFrame     predictors used to generate predicted damage function fit, the name of the variable should be the same     as that used in the model formula. quantiles: list of floats between 0 and 1     List of quantile regressions to be run.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>pd.DataFrame with coefficients by year pd.DataFrame with predictions</p> Source code in <code>src/dscim/utils/utils.py</code> <pre><code>def modeler(df, formula, type_estimation, exog, quantiles=None):\n    \"\"\"Wrapper function for statsmodels functions (OLS and quantiles)\n\n    For the quantiles, the function will fit for all quantiles between 0 and 1\n    with a 0.25 step.\n\n    Parameters:\n    ---------\n    df: pd.DataFrame\n        A dataframe of prepared damage function. Since formulas are hard to\n        get, the function expects standarized column names: `damages` must\n        contain all damages coming from the damage function, and `temp` must\n        be the name of temperature anomalies from the models.\n    formula: str\n        R-like formula to start fitting. (i.e. damages ~ temp +\n        np.power(temp,2)) defines a quadratic relationship between temperature\n        and damages.\n    type_estimation: str\n        Model to use . 'ols' and 'quantreg' are the one availables. Any other\n        passed option will yield a `NonImplementedError`\n    exog: pd.DataFrame\n        predictors used to generate predicted damage function fit, the name of the variable should be the same\n        as that used in the model formula.\n    quantiles: list of floats between 0 and 1\n        List of quantile regressions to be run.\n\n    Returns\n    ------\n    tuple\n        pd.DataFrame with coefficients by year\n        pd.DataFrame with predictions\n    \"\"\"\n\n    if type_estimation == \"ols\":\n        mod = smf.ols(formula=formula, data=df).fit()\n        params = pd.DataFrame(mod.params).T\n        y_hat = pd.DataFrame(dict(exog, y_hat=mod.predict(exog=exog)))\n\n    elif type_estimation == \"quantreg\":\n        # silence the 'max recursions' warning\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\")\n\n            # set up the model\n            mod = smf.quantreg(formula=formula, data=df)\n            q_params, q_y_hat = [], []\n\n            # calculate each quantile regression\n            # save parameters and predictions\n            for quant in quantiles:\n                quant_mod = mod.fit(q=quant)\n                params_quantile = pd.DataFrame(quant_mod.params).T\n                params_quantile = params_quantile.assign(q=quant)\n                q_params.append(params_quantile)\n\n                fitted = pd.DataFrame(dict(exog, y_hat=quant_mod.predict(exog=exog)))\n                q_y_hat.append(fitted.assign(q=quant))\n\n            params = pd.concat(q_params)\n            y_hat = pd.DataFrame(pd.concat(q_y_hat))\n\n    else:\n        raise NotImplementedError(f\"{type_estimation} is not valid option!\")\n\n    return params, y_hat\n</code></pre>"},{"location":"utils/utils/#dscim.utils.utils.power","title":"dscim.utils.utils.power","text":"<pre><code>power(a, b)\n</code></pre> <p>Power function that doesn't return NaN values for negative fractional exponents</p> Source code in <code>src/dscim/utils/utils.py</code> <pre><code>def power(a, b):\n    \"\"\"Power function that doesn't return NaN values for negative fractional exponents\"\"\"\n    return np.sign(a) * (np.abs(a)) ** b\n</code></pre>"},{"location":"utils/utils/#dscim.utils.utils.quantile_weight_quantilereg","title":"dscim.utils.utils.quantile_weight_quantilereg","text":"<pre><code>quantile_weight_quantilereg(array, fair_dims, quantiles=None)\n</code></pre> <p>Produce quantile weights of the quantile regression damages.</p> <p>Parameters:</p> Name Type Description Default <code>qr_quantiles</code> required <code>quantiles</code> <code>sequence or None</code> <p>Quantiles to compute. Default is (0.01, 0.05, 0.167, 0.25, 0.5, 0.75, 0.833, 0.95, 0.99).</p> <code>None</code> Source code in <code>src/dscim/utils/utils.py</code> <pre><code>def quantile_weight_quantilereg(array, fair_dims, quantiles=None):\n    \"\"\"Produce quantile weights of the quantile regression damages.\n\n    Parameters\n    ----------\n    qr_quantiles: the quantile regression quantiles for damages. (Must be 0-1!)\n    quantiles : sequence or None, optional\n        Quantiles to compute. Default is (0.01, 0.05, 0.167, 0.25, 0.5, 0.75,\n        0.833, 0.95, 0.99).\n    \"\"\"\n    if quantiles is None:\n        quantiles = [0.01, 0.05, 0.167, 0.25, 0.5, 0.75, 0.833, 0.95, 0.99]\n\n    qr_quantiles = array.q.values\n\n    to_stack = [\"q\"] + list(set(fair_dims).intersection(set(array.coords)))\n    weights = xr.DataArray(get_weights(qr_quantiles), dims=[\"q\"], coords=[qr_quantiles])\n\n    if len(to_stack) &gt; 1:\n        ds_stacked = array.stack(obs=to_stack)\n        weights_by_obs = weights.sel(q=ds_stacked.obs.q)\n    else:\n        ds_stacked = array.rename({to_stack[0]: \"obs\"})\n        weights_by_obs = weights.sel(q=ds_stacked.obs)\n\n    dim = \"obs\"\n\n    # these are quantiles of the statistical or full uncertainty, weighted by the quantile regression quantiles\n    ds_quantiles = impactlab_tools.utils.weighting.weighted_quantile_xr(\n        ds_stacked, quantiles, sample_weight=weights_by_obs.values, dim=dim\n    )\n\n    return ds_quantiles\n</code></pre>"}]}